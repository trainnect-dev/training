# ðŸ“˜ Microsoft Fabric for Healthcare: 2-Day Technical Training Manual {#main-title}

## Course Mindmap {#course-mindmap}
<figure>
  <img src="./images/diagram_course-mindmap.png" alt="Microsoft Fabric Healthcare Skilling Plan Mindmap" style="width:100%;" />
  <figcaption><em>Figure: Mindmap overview of the Microsoft Fabric Healthcare Skilling Plan.</em></figcaption>
</figure>

---

## ðŸ“‘ Training Manual Outline {#training-manual-outline}
1.  [**Introduction to Microsoft Fabric in Healthcare**](#section-1-introduction-to-microsoft-fabric-in-healthcare)
    *   Overview of Microsoft Fabric
    *   Importance in the healthcare industry
    *   Key features and benefits

2.  [**Setting Up the Environment**](#section-2-setting-up-the-environment)
    *   Prerequisites and system requirements
    *   Installation and configuration (step-by-step, with interactive visuals)
    *   Accessing Microsoft Fabric components

3.  [**Data Ingestion and Integration**](#section-3-data-ingestion-and-integration)
    *   Connecting to various data sources (guided walkthrough)
    *   Data ingestion techniques (interactive demo)
    *   Ensuring data quality and consistency

4.  [**Data Modeling and Transformation**](#section-4-data-modeling-and-transformation)
    *   Designing data models (drag-and-drop exercise)
    *   Data transformation processes
    *   Best practices for data modeling in healthcare

5.  [**Security, Compliance, and Governance**](#section-5-security-compliance-and-governance)
    *   Understanding HIPAA and HITRUST requirements
    *   Implementing security measures (interactive checklist)
    *   Ensuring compliance in data handling

6.  [**Analytics and Reporting**](#section-6-analytics-and-reporting)
    *   Utilizing analytics tools within Microsoft Fabric
    *   Creating reports and dashboards
    *   Interpreting analytical results

7.  [**Machine Learning and AI Integration**](#section-7-machine-learning-and-ai-integration)
    *   Introduction to machine learning concepts
    *   Integrating AI models
    *   Use cases in healthcare

8.  [**Performance Optimization**](#section-8-performance-optimization)
    *   Monitoring system performance
    *   Optimizing data processing
    *   Troubleshooting common issues

9.  [**Collaboration and Sharing**](#section-9-collaboration-and-sharing)
    *   Sharing data and insights
    *   Collaborating within teams
    *   Managing access and permissions

10. [**Advanced Features and Customization**](#section-10-advanced-features-and-customization)
    *   Exploring advanced functionalities
    *   Customizing Microsoft Fabric components
    *   Extending capabilities with add-ons
Section%2010%20-%20Advanced%20Features%20and%20Customization.html))
11. [**Case Studies and Real-world Applications**](#section-11-case-studies-and-real-world-applications)
    *   Examining successful implementations
    *   Lessons learned from real-world scenarios
    *   Applying knowledge to practical situations

12. [**Labs and Exercises**](#section-12-labs-and-exercises)
    *   Hands-on labs for each module
    *   Practice exercises with solutions
    *   Reinforcing learning through application

13. [**Assessment and Certification**](#section-13-assessment-and-certification)
    *   Comprehensive quiz covering all modules
    *   Evaluation criteria
    *   Certification of completion
    *   ([Access Interactive Element](./visualizations-html/Section%2013%20-%20Assessment%20and%20Certification.html))
    *   ([Student Assessment Test](./visualizations-html/Section%2013%20-%20Student%20Assessment%20Test.html))
    *   ([Student Assessment Test 2](./visualizations-html/Section%2013%20-%20Student%20Assessment%20Test%202.html))
    *   ([Microsoft Fabric Interactive Quiz](./visualizations-html/microsoft-fabric-interactive-quiz.html))

14. [**Job Aids and Cheat Sheets**](#section-14-job-aids-and-cheat-sheets)
    *   Quick reference guides
    *   Tips and tricks for efficient use
    *   Summarized procedures and workflows

15. [**References and Further Reading**](#section-15-final-notes-and-resources)
    *   Additional resources
    *   Suggested readings
    *   Links to official documentation
  

---

## ðŸ“˜ Section 1: Introduction to Microsoft Fabric in Healthcare {#section-1-introduction-to-microsoft-fabric-in-healthcare}

---

### 1.0 Overview
In the age of digital transformation, healthcare organizations face unprecedented challenges and opportunities. Data is both a burden and a blessing. While the volume of data continues to grow exponentiallyâ€”coming from electronic health records (EHRs), laboratory systems, wearable devices, imaging repositories, and moreâ€”the ability to derive real-time, actionable insights remains constrained by legacy architectures, siloed systems, and interoperability gaps.

Enter **Microsoft Fabric**, an integrated Software-as-a-Service (SaaS) data platform built on top of Azure that consolidates analytics, governance, AI, and data engineering in one seamless environment. For data engineers in healthcare, Fabric isn't just a toolâ€”it's a comprehensive platform that redefines how healthcare data is ingested, secured, modeled, analyzed, and operationalized.

This manual begins with a robust understanding of why Fabric matters to healthcare and what foundational competencies engineers must build to effectively implement it in clinical settings.

---

### 1.1 The Healthcare Data Dilemma
Before diving into technology, it's critical to grasp the environment in which it operates.

#### Fragmentation Across Systems
Healthcare systems are notoriously siloed. Data lives in:

*   **EHRs** (Epic, Cerner, Meditech, etc.)
*   **Laboratory Information Systems (LIS)**
*   **Radiology PACS**
*   **Pharmacy dispensing systems**
*   **Billing and revenue cycle software**
*   **IoT-enabled medical devices**
*   **Department-level Excel sheets or Access DBs**

Each repository holds vital data, but their lack of interconnectivity impairs holistic insights. Interoperability standards like HL7v2, FHIR, and DICOM attempt to bridge the divideâ€”but real-time, semantic-level interoperability remains elusive.

#### The Challenge of Data Velocity & Variety
*   **Structured data** (e.g., vitals, lab results)
*   **Unstructured data** (e.g., physician notes, PDF scans)
*   **Image data** (e.g., MRI, CT)
*   **Streaming data** (e.g., continuous glucose monitors, heart monitors)

Handling these streams in a secure, compliant, and analyzable fashion demands more than traditional data warehouses.

#### Compliance Pressures
Healthcare organizations are bound by rigorous regulations:

*   **HIPAA (U.S.):** Ensures privacy and security of protected health information (PHI).
*   **HITRUST:** A certifiable framework combining ISO, NIST, and HIPAA.
*   **State regulations:** Varying rules on data storage, breach notification, and access auditing.

Fabric addresses these with built-in governance, access controls, encryption, and integration with Microsoft Purview.

---

### 1.2 What is Microsoft Fabric?
Microsoft Fabric is a **SaaS-based unified data platform** designed to consolidate multiple Azure services into one streamlined experience:

*   **Azure Data Factory** (for data integration)
*   **Azure Synapse Analytics** (for data engineering)
*   **Power BI** (for business intelligence)
*   **Real-Time Analytics** (for event streaming and telemetry)
*   **Data Science tools** (with MLflow, notebooks)
*   **Microsoft Purview integration** (for data governance)

At its core is **OneLake**: a single, logical data lake for the entire organization where data resides in an open format (Delta Parquet).

#### Core Features
| Feature | Description |
| :--- | :--- |
| OneLake | Centralized data lake using open Delta format |
| Data Factory | Connect, ingest, and orchestrate data pipelines |
| Notebooks | Jupyter-style environments for PySpark, Python, R |
| Power BI Integration | Real-time visual analytics on unified data |
| AI/ML Integration | Train models directly within Fabric, leverage Azure Cognitive Services |
| Data Governance | RBAC, data classification, audit trails via Microsoft Purview |
| Copilot Integration | Natural language interface to generate code, queries, and dashboards |

---

### 1.3 Why Microsoft Fabric Matters to Healthcare
#### A. Unified Data Estate
Microsoft Fabric allows healthcare organizations to unify patient, clinical, financial, and operational data from multiple systems into **OneLake**. This enables:

*   Patient 360 views
*   Population health dashboards
*   Real-time alerts
*   Interdepartmental collaboration

#### B. AI-Powered Analytics
With integrated ML capabilities, data engineers can deploy models for:

*   Predicting readmission risk
*   ICU deterioration monitoring
*   Medication adherence
*   No-show prediction
*   Operational throughput

#### C. Compliance and Security
Fabric supports:

*   End-to-end encryption (at rest, in transit)
*   Role-based access control (RBAC)
*   Activity auditing
*   Support for customer-managed keys
*   De-identification workflows
*   Integration with Purview for lineage tracking

#### D. Developer Productivity
Via Copilot, engineers can:

*   Auto-generate transformation code
*   Get SQL/DAX suggestions
*   Receive inline documentation
*   Troubleshoot errors

---

### 1.4 Visualizing Data Architectures
#### 1.4.1 Traditional vs. Unified Data Flow in Microsoft Fabric
<figure>
  <img src="./images/1.4-diagram_Conceptual%20diagram%20illustrating%20traditional%20fragmented%20data.png" alt="Traditional fragmented data vs. Microsoft Fabric unified data approach" style="width:100%;" />
  <figcaption><em>Figure: Conceptual diagram illustrating traditional fragmented data approaches versus Microsoft Fabric's unified data approach.</em></figcaption>
</figure>

---

### 1.5 Lab: Understanding Healthcare Data Fragmentation
**Module Alignment:** Section 1: Introduction to Microsoft Fabric in Healthcare

**Objective:**
*   Analyze a fictional hospitalâ€™s data systems to identify fragmentation.
*   Understand the challenges posed by data fragmentation for analytics.
*   Conceptualize how Microsoft Fabric can address these challenges.

**Scenario:**
Valley General Hospital, a mid-sized healthcare provider, uses several distinct IT systems to manage its operations and patient care:
1.  **Electronic Health Record (EHR) System:** Contains patient demographics, clinical encounter details, prescribed medications, allergies, and problem lists.
2.  **Pharmacy System:** Manages medication dispensing, prescription fill history, and inventory. Linked to the EHR for prescriptions but operates as a separate database for dispensing records.
3.  **Laboratory Information System (LIS):** Stores results from all lab tests conducted, including blood work, pathology reports, and microbiology. Results are sent to the EHR but detailed metadata and raw outputs might remain in the LIS.
4.  **Outpatient Portal System:** Allows patients to schedule appointments, view summaries of their visits, and communicate with providers. Appointment data and patient-entered information are stored here.

**Prerequisites:**
*   Understanding of basic healthcare data types (demographics, encounters, medications, lab results).
*   Familiarity with the concept of data silos.

**Tools to be Used:**
*   This is a conceptual lab, primarily requiring analytical thinking and discussion. No specific Fabric tools are used for execution, but knowledge of Fabric's purpose is beneficial.

**Estimated Time:** 30 minutes

**Tasks:**

This lab is discussion-based. Consider the following questions based on the scenario:

**Questions & Detailed Answers:**

**1. List which key data elements are likely stored in each system at Valley General Hospital.**

*   **EHR System:**
    *   `Patient_ID`, `Patient_Name`, `Date_of_Birth`, `Gender`, `Address`, `Contact_Info`
    *   `Encounter_ID`, `Encounter_Date`, `Encounter_Type` (e.g., Inpatient, Outpatient, Emergency)
    *   `Provider_ID`, `Attending_Physician`
    *   `Diagnosis_Codes` (e.g., ICD-10), `Problem_List`
    *   `Medication_Orders` (prescribed medications, dosage, frequency)
    *   `Allergies_List`, `Adverse_Reactions`
    *   `Vital_Signs` (Height, Weight, Blood Pressure, Temperature)
    *   `Clinical_Notes` (Progress notes, consultation notes - often unstructured)
    *   `Immunization_Records`
    *   Pointers to lab results and radiology reports.

*   **Pharmacy System:**
    *   `Prescription_ID` (linked to EHR order)
    *   `Patient_ID`
    *   `Medication_NDC_Code` (National Drug Code)
    *   `Dispense_Date`, `Dispense_Quantity`, `Days_Supply`
    *   `Fill_Number`, `Refills_Remaining`
    *   `Pharmacist_ID`
    *   `Cost_Information`, `Insurance_Formulary_Status` (potentially)
    *   Inventory levels of medications.

*   **Laboratory Information System (LIS):**
    *   `Lab_Order_ID` (linked to EHR order)
    *   `Patient_ID`
    *   `Specimen_ID`, `Specimen_Type`, `Collection_Date_Time`
    *   `Test_Code` (e.g., LOINC), `Test_Name`
    *   `Result_Value` (quantitative or qualitative)
    *   `Reference_Range`, `Abnormal_Flags`
    *   `Performing_Lab_ID`, `Technician_ID`
    *   Pathology reports, microbiology culture details (can be extensive and semi-structured).

*   **Outpatient Portal System:**
    *   `Patient_ID`
    *   `Appointment_ID`, `Scheduled_Date_Time`, `Appointment_Status` (Scheduled, Confirmed, Cancelled, Completed)
    *   `Provider_ID`, `Clinic_Location`
    *   `Reason_for_Visit` (patient-stated)
    *   Secure messages exchanged between patient and provider.
    *   Patient-entered data (e.g., pre-visit questionnaires, symptom checkers).
    *   View logs (which patient viewed what information).

**2. What are the challenges if Valley General Hospital wants to develop a predictive model for patient readmission within 30 days of discharge?**

*   **Data Silos & Integration Complexity:**
    *   Crucial data for readmission prediction (e.g., discharge medications from EHR, actual dispensing from Pharmacy, post-discharge lab results from LIS, follow-up appointment adherence from Outpatient Portal) reside in separate systems.
    *   Combining this data requires complex, often manual, and potentially error-prone integration efforts. Each system might use different patient identifiers or data formats, requiring sophisticated mapping.

*   **Data Timeliness & Accessibility:**
    *   Real-time or near real-time data access is difficult. If the Pharmacy system updates dispensing records daily via batch, the model might not have the latest medication adherence information.
    *   Accessing data from multiple systems often involves different APIs, query languages, or export formats, increasing development time.

*   **Inconsistent Data Definitions & Standards:**
    *   The definition of "medication adherence" or "completed follow-up" might differ or not be explicitly captured across systems.
    *   While standards like HL7 or FHIR might be used for some interfaces, the internal storage and granularity can vary.

*   **Lack of a Unified Patient View (Patient 360):**
    *   Without a consolidated view, it's hard to see the complete patient journey leading up to a potential readmission. For example, did the patient pick up their discharge medications? Did they attend their follow-up appointment? Were there critical lab value changes post-discharge?

*   **Feature Engineering Difficulty:**
    *   Creating meaningful features for the model (e.g., number of prior admissions, specific medication classes, social determinants of health if captured) becomes challenging when data is fragmented. For instance, calculating the "number of hospitalizations in the last 6 months" requires querying and joining data that might span EHR and potentially older archived systems.

*   **Data Quality Issues:**
    *   Discrepancies between systems (e.g., a medication prescribed in EHR but never dispensed according to Pharmacy records) can lead to inaccurate model inputs.
    *   Missing data in one system that is critical for a feature can reduce the model's predictive power.

*   **Scalability & Performance:**
    *   Querying multiple disparate systems for large patient cohorts to train a model can be slow and resource-intensive.

**3. How can Microsoft Fabricâ€™s OneLake and Data Factory help address these challenges?**

*   **OneLake as a Unified Data Store:**
    *   **Centralization:** OneLake acts as a single, logical data lake for the entire organization. Data from the EHR, Pharmacy, LIS, and Outpatient Portal can be ingested and stored in OneLake, eliminating physical silos.
    *   **Single Source of Truth:** By processing and conforming data into Bronze, Silver, and Gold layers within OneLake, Valley General can create a unified, reliable source for analytics and model training. This enables a true Patient 360 view.
    *   **Open Data Formats:** Storing data in open formats like Delta Parquet within OneLake means it's accessible by various compute engines in Fabric (Spark, SQL, Power BI via DirectLake) without data movement or duplication.

*   **Data Factory for Ingestion and Orchestration:**
    *   **Connectors:** Data Factory provides a wide range of connectors to pull data from various sources, including SQL databases (for EHR, LIS, Pharmacy backends), APIs (potentially for the Outpatient Portal or modern EHRs using FHIR), and file systems.
    *   **Data Integration Pipelines:** Data Factory can be used to build robust pipelines to extract, transform (if needed at a basic level for ingestion), and load (ETL/ELT) data from these disparate systems into the Bronze layer of OneLake.
    *   **Scheduling & Automation:** These pipelines can be scheduled to run at regular intervals (e.g., hourly, daily), ensuring that OneLake is consistently updated with the latest information, improving data timeliness for the readmission model.
    *   **Data Transformation (with Dataflows Gen2 or Notebooks):** While Data Factory orchestrates, it integrates seamlessly with Dataflows Gen2 (for low-code transformations) or Notebooks (for complex transformations using Spark/Python) to cleanse, standardize, and conform the ingested data into the Silver and Gold layers. This is where data from different sources can be joined and harmonized.

*   **Addressing Specific Challenges with Fabric:**
    *   **Integration Complexity:** Fabric provides the tools (Data Factory, Notebooks) to build these integrations once and then automate them.
    *   **Data Timeliness:** Scheduled pipelines ensure fresher data. Real-Time Analytics in Fabric could even handle streaming data if some sources support it.
    *   **Unified Patient View:** The Gold layer in OneLake, built using Fabric tools, would house the comprehensive data needed for the readmission model, effectively creating that Patient 360.
    *   **Feature Engineering:** With all relevant data in OneLake, data scientists can use Fabric Notebooks (with Spark or Python) to easily access and process this data to engineer complex features for the model.
    *   **Scalability & Performance:** Fabric's compute engines (Spark for Notebooks, SQL engine for the Warehouse) are designed for performance and can scale to handle large datasets for model training and scoring.

**Expected Outcome / Deliverables:**
*   A clear understanding of how data fragmentation in healthcare impacts analytical capabilities.
*   An appreciation for the role of a unified data platform like Microsoft Fabric in overcoming these challenges.

---

### 1.6 Exercise: Your Organizationâ€™s Fragmentation Map
**Instructions:**

Draw a simple flow of your current healthcare data systems. Then answer:

1.  Which of these systems are connected today?
2.  Where are the gaps?
3.  Which departments most struggle with data access?
4.  Which datasets would benefit most from AI analysis?

---

### 1.7 Knowledge Review: Flash Cards
| Front | Back |
| :--- | :--- |
| What is Microsoft Fabric? | A SaaS analytics platform unifying data engineering, AI, BI in OneLake |
| What is OneLake? | A unified Delta Parquet-based data lake embedded in Fabric |
| Key Fabric Benefits for Healthcare | Unified views, AI capabilities, security compliance, productivity |
| How does Fabric support HIPAA? | RBAC, encryption, audit logs, Purview integration |

---

### 1.8 Summary
This introductory chapter sets the foundation for mastering Microsoft Fabric. You now understand:

*   The key pain points in healthcare data management
*   How Fabric addresses fragmentation, AI needs, and compliance
*   The essential components of the platform

In the next section, weâ€™ll begin configuring and preparing the Fabric environment.

---

### âœ… Certification Quiz
**1. Which format does OneLake store data in?**
a) CSV
b) JSON
c) Delta Parquet
**â†’ Answer: c**

**2. Which regulation applies to patient privacy in the U.S.?**
a) GDPR
b) HIPAA
c) PCI-DSS
**â†’ Answer: b**

**3. What role does Microsoft Purview play?**
a) Visualization
b) Governance
c) Backup
**â†’ Answer: b**

---

### ðŸ”– Cheat Sheet (Printable)
*   **OneLake** = Fabricâ€™s unified Delta-based data lake
*   **Data Factory** = Data pipeline and transformation tool
*   **Power BI** = Visualize data directly from OneLake
*   **Copilot** = Code/insight generator
*   **Fabric Compliance** = Encryption + RBAC + Audit + Purview

---

### Interactive Element for Section 1
[Interactive Info Card: Industry Impact](./visualizations-html/Section%201%20-%20Introduction%20to%20Microsoft%20Fabric%20in%20Healthcare.html)

---

## ðŸ“˜ Section 2: Setting Up the Environment {#section-2-setting-up-the-environment}

---

### 2.0 Overview
Before diving into ingestion pipelines, AI modeling, and governance layers, it's essential to configure a robust, compliant, and scalable **Microsoft Fabric environment**. This chapter will guide healthcare data engineers through the initial setup of the environmentâ€”including provisioning access, establishing workspace boundaries, configuring tools, and applying basic governance policiesâ€”all while adhering to HIPAA and HITRUST standards.

---

### 2.1 Prerequisites
#### âœ”ï¸ Organizational Requirements
To begin using Microsoft Fabric effectively, your organization must:

*   Have an active **Microsoft 365 tenant**
*   Possess a valid **Microsoft Fabric license** (details below)
*   Enable **Microsoft Fabric in the Microsoft 365 Admin Center**
*   Assign **Power BI Admin and Fabric Admin roles** for workspace provisioning
*   Possess **Azure Active Directory (Azure AD / Entra ID)** for identity and access management

#### âœ”ï¸ Role-Based Access Considerations
From the outset, itâ€™s crucial to define **personas** and align them with the proper **roles and access levels**:

| Role | Common Tasks | Fabric Role |
| :--- | :--- | :--- |
| Data Engineer | Pipeline design, transformation logic | Contributor / Admin |
| Data Scientist | Model training, notebooks | Contributor |
| Analyst | Report creation | Member / Viewer |
| Compliance Officer | Audit log access, governance | Viewer (Purview access) |

Use **RBAC** principles to minimize data exposure and restrict unnecessary permissions.

---

### 2.2 Licensing and Region Selection
#### Fabric Licensing
Microsoft Fabric is integrated into **Power BI Premium**, **Microsoft 365 E5**, or via **Fabric capacity SKUs**. Licensing influences:

*   Data capacity
*   Performance
*   Real-time analytics limits

> ðŸ’¡ **Tip:** For healthcare scenarios, a minimum of **F64 capacity** is recommended for large datasets and concurrent AI modeling.

#### Region Selection and HIPAA Compliance
When creating a Microsoft Fabric tenant:

*   Choose **U.S.-based regions** to ensure **HIPAA data residency**
*   Avoid cross-region data flows unless governed and encrypted
*   Consider **multi-geo** for national health systems

> ðŸ›¡ï¸ Microsoft signs a **Business Associate Agreement (BAA)** for HIPAA compliance with Azure customers using eligible services, including Fabric.

---

### 2.3 Accessing Microsoft Fabric
To begin using Fabric:

1.  **Sign into Microsoft 365 portal** as a global admin or Power BI admin
2.  Navigate to **Power BI Admin Portal**
3.  Select **â€œTenant Settingsâ€ â†’ Enable Microsoft Fabric**
4.  Assign **admin, contributor, member, or viewer roles** within Fabric workspaces

#### UI Walkthrough
Fabric is accessed via:

*   [https://app.fabric.microsoft.com](https://app.fabric.microsoft.com)
*   Once inside, users can switch between tools: **Data Factory**, **Power BI**, **Real-Time Analytics**, **Data Activator**, and **Notebooks**

> ðŸ“¸ Screenshot Note: Include labeled screenshot of Fabric UI with arrows pointing to Data Factory, Lakehouse, Notebooks, and Power BI.

---

### 2.4 Creating Your First Workspace
Workspaces serve as **project containers** in Fabric. Think of them as boundary-enforced collaboration zones.

#### Steps:
1.  In the Fabric portal, click **â€œNew Workspaceâ€**
2.  Name the workspace (e.g., *ClinicalAnalytics*)
3.  Choose the desired **capacity**
4.  Assign roles:
    *   Admin: Data Engineering Lead
    *   Member: Analysts
    *   Viewer: Compliance officer

#### Workspace Best Practices
*   Separate **development**, **test**, and **production** workspaces
*   Use **prefixes** like `DEV_`, `TEST_`, `PROD_` to identify environments
*   Document workspace purposes and access rules
*   Enable **sensitivity labels** via Microsoft Purview (more in Section 5)

---

### 2.5 Connecting Data Sources
Microsoft Fabric enables hundreds of connectors. In a healthcare context, the most common data sources are:

| Source Type | Example Systems | Connector |
| :--- | :--- | :--- |
| Database | SQL Server, Oracle, PostgreSQL | Native |
| Cloud Storage | Azure Blob, ADLS Gen2 | Native |
| APIs | FHIR Server, HL7 Gateway | REST/OData |
| Files | HL7 batch files, CSVs, DICOM metadata | File Connector |
| Streaming | Medical IoT (via Event Hub) | Eventstream |

> ðŸ§ª Use **Dataflows Gen2** or **Spark Notebooks** to begin loading data.

---

### 2.6 Setting Up OneLake
**OneLake** is Microsoft Fabricâ€™s **Delta Parquet-based centralized data lake**. It's auto-provisioned at the tenant level.

#### How to Use It:
*   Each Fabric workspace contains a **Lakehouse** (your logical interface into OneLake)
*   Data is stored in **Bronze (raw), Silver (conformed), and Gold (aggregated)** layers
*   You can create **shortcuts** to bring in existing data from:
    *   Azure Data Lake
    *   Amazon S3
    *   Google Cloud Storage

#### HIPAA Alignment
*   All data is **encrypted at rest**
*   RBAC can restrict access to specific folders
*   **Audit logs** track access to sensitive PHI

#### Conceptual Data Flow for Unified Healthcare Data Estate (Medallion Architecture)
<figure>
  <img src="./images/2.6-diagram%20illustrates%20the%20Medallion%20Architecture%20%28Bronze,%20Silver,%20Gold%20layers%29%20within%20OneLake.png" alt="Medallion Architecture (Bronze, Silver, Gold layers) in OneLake" style="width:100%;" />
  <figcaption><em>Figure: This diagram illustrates the Medallion Architecture (Bronze, Silver, Gold layers) within OneLake, a common best practice for building robust and scalable data estates.</em></figcaption>
</figure>

---

### 2.7 Toolchain Setup
#### âœ… Data Factory (Fabric-native)
*   Open a workspace
*   Click **New â†’ Data pipeline**
*   Drag and drop activities:
    *   Source (e.g., SQL Server)
    *   Dataflow Gen2 (transform)
    *   Sink (Lakehouse)

#### âœ… Power BI
*   Reports use **DirectLake** connection
*   Power BI pulls from OneLake without duplicating data
*   Use **row-level security (RLS)** for role-based access to dashboards

#### âœ… Notebooks
*   Launch Fabricâ€™s **Jupyter-style Notebooks**
*   Language support: **Python, Spark, SQL, R**
*   Access any Lakehouse table with:
```python
df = spark.read.format("delta").load("Tables/Gold_PatientRisk")
```

---

### 2.8 Setting Up Governance Basics
Fabric integrates with **Microsoft Purview** for compliance, governance, and lineage.

#### Key Actions:
*   Classify PHI using built-in classifiers (e.g., "patient name", "DOB")
*   Assign **sensitivity labels**
*   Create **lineage diagrams** from source to dashboard
*   Enable **access policies** to restrict export/sharing

> ðŸ›¡ï¸ Purview is essential for **HIPAAâ€™s â€œaudit controlâ€ and â€œintegrityâ€ safeguards**

---

### 2.9 Lab: Setting Up Your First Fabric Environment and Ingesting Sample Data
**Module Alignment:** Section 2: Setting Up the Environment

**Objective:**
*   Familiarize with the Microsoft Fabric portal and workspace creation.
*   Create a Lakehouse and ingest sample CSV data.
*   Explore the ingested data using a Fabric Notebook.
*   Understand basic role assignments within a workspace.

**Scenario:**
You are a data engineer at Valley General Hospital, newly onboarded to Microsoft Fabric. Your first task is to set up a development workspace for an upcoming cardiology analytics project and perform a test ingestion of sample patient data.

**Prerequisites:**
*   Access to a Microsoft Fabric enabled Microsoft 365 tenant.
*   Permissions to create workspaces in Fabric (typically Fabric Administrator or Power BI Administrator to enable Fabric for the tenant, then users with appropriate capacity permissions can create workspaces).
*   A sample CSV file with patient data. We will create one in the lab.

**Tools to be Used:**
*   Microsoft Fabric Portal
*   Fabric Workspace
*   Fabric Lakehouse
*   Fabric Notebook (PySpark)

**Estimated Time:** 45 minutes

**Tasks / Step-by-Step Instructions:**

**Part 1: Create a Workspace**

1.  **Navigate to Microsoft Fabric:** Open your browser and go to `app.fabric.microsoft.com`.
2.  **Create a New Workspace:**
    *   In the left navigation pane, click on **Workspaces**.
    *   Click the **+ New workspace** button in the top right.
    *   **Name:** Enter `DEV_CardiologyAnalytics`.
    *   **Description:** (Optional) Enter "Development workspace for cardiology analytics projects".
    *   **Domain:** (Optional) Assign to a relevant domain if your organization uses them.
    *   **Capacity:** Assign the workspace to a Fabric capacity (e.g., a trial capacity or a provisioned F-SKU). This is crucial for using Fabric features.
    *   Click **Apply**.

**Part 2: Create a Lakehouse**

1.  **Open Your Workspace:** From the Workspaces list, click on `DEV_CardiologyAnalytics` to open it.
2.  **Create a New Lakehouse:**
    *   Within the workspace, click the **+ New** button.
    *   Select **Lakehouse** from the options.
    *   **Name:** Enter `CardiologyLakehouse`.
    *   Click **Create**.
    *   The Lakehouse explorer view will open. You'll see sections for `Tables` and `Files`.

**Part 3: Prepare and Upload Sample CSV Data**

1.  **Create Sample CSV Data:**
    *   Open a plain text editor (like Notepad or VS Code) on your local machine.
    *   Copy and paste the following data into the editor:
        ```csv
        PatientID,FirstName,LastName,DateOfBirth,Gender,LastVisitDate,Diagnosis
        P001,John,Doe,1985-06-15,Male,2023-01-10,Hypertension
        P002,Jane,Smith,1992-03-22,Female,2023-02-20,Diabetes Type 2
        P003,Robert,Jones,1978-11-05,Male,2022-12-05,Asthma
        P004,Emily,Brown,2001-07-30,Female,2023-03-15,Migraine
        P005,Michael,Davis,1965-09-12,Male,2023-01-25,Coronary Artery Disease
        ```
    *   Save the file as `sample_patients.csv` on your local machine.

2.  **Upload Data to Lakehouse (Files section):**
    *   In the `CardiologyLakehouse` explorer view, under the `Files` section, click the three dots (**...**) next to `Files` (or an existing folder if you prefer).
    *   Select **Upload** -> **Upload files**.
    *   Browse to your locally saved `sample_patients.csv` and select it.
    *   Confirm the upload. You should see `sample_patients.csv` appear under the `Files` section.

3.  **Load CSV to a Delta Table (using UI):**
    *   In the Lakehouse explorer, find the `sample_patients.csv` file under `Files`.
    *   Click the three dots (**...**) next to `sample_patients.csv`.
    *   Select **Load to table** -> **New table**.
    *   **Table name:** Enter `bronze_patients`.
    *   Fabric will infer the schema. Review it and click **Load**.
    *   A notification will appear once the table is created. You should see `bronze_patients` under the `Tables` section.

**Part 4: Explore Data with a Notebook**

1.  **Create a New Notebook:**
    *   Go back to your `DEV_CardiologyAnalytics` workspace view.
    *   Click **+ New** -> **Notebook**.
    *   A new notebook will open.

2.  **Attach Lakehouse to Notebook:**
    *   In the notebook interface, on the left side, you should see an "Explorer" pane.
    *   Click on **Add Lakehouse**.
    *   Select your `CardiologyLakehouse` and click **Add**. This makes the tables and files in your Lakehouse accessible to the notebook.

3.  **Write and Run PySpark Code:**
    *   In the first cell of the notebook, ensure the language is set to **PySpark (Python)**.
    *   Enter the following code to load and display the `bronze_patients` table:

        ```python
        # Read the Delta table from the Lakehouse
        df_patients = spark.read.table("CardiologyLakehouse.bronze_patients") # Or just "bronze_patients" if default lakehouse is set

        # Display the DataFrame schema and some data
        df_patients.printSchema()
        display(df_patients.limit(5)) # display() is a Fabric-specific function for rich table rendering

        # Perform a simple count
        patient_count = df_patients.count()
        print(f"Total number of patients: {patient_count}")
        ```
    *   Click the **Run cell** button (or Shift+Enter) to execute the code.

**Part 5: Assign Roles in Workspace (Conceptual - Requires Admin to actually assign)**

1.  **Navigate to Workspace Access Management:**
    *   Go to your `DEV_CardiologyAnalytics` workspace.
    *   In the top right corner of the workspace view, click on **Manage access** (or an icon representing access).
2.  **Add Users and Assign Roles:**
    *   You would typically see a list of current members. Click **+ Add people or groups**.
    *   Enter the email address of a team member.
    *   Assign a role:
        *   **Admin:** Full control (e.g., Data Engineering Lead).
        *   **Member:** Can view, edit, and publish content (e.g., another Data Engineer or Power BI Developer).
        *   **Contributor:** Can create items and publish reports but can't manage access or workspace settings (e.g., Data Analyst).
        *   **Viewer:** Can only view content (e.g., Clinical stakeholder).
    *   Click **Add**.
    *   *(Note: For this lab, you might not have other users to add, but understand the process.)*

**Expected Outcome / Deliverables:**
*   A Fabric workspace named `DEV_CardiologyAnalytics` is created.
*   A Lakehouse named `CardiologyLakehouse` exists within the workspace.
*   The `sample_patients.csv` data is uploaded to the Lakehouse `Files` section and loaded into a Delta table named `bronze_patients`.
*   A Fabric Notebook successfully reads and displays data from the `bronze_patients` table.
*   Understanding of how to conceptually assign different roles within a Fabric workspace.

**Questions from Manual & Answers - LINK TO HTML???:**

*   **Q1: What level of access should a data analyst be granted if they primarily need to build reports and perform data exploration but not manage the workspace or core data engineering pipelines?**
    *   **A1:** A **Contributor** role is often suitable. They can create notebooks, dataflows, and Power BI reports using existing data in the Lakehouse. If they only need to consume existing reports, **Viewer** would be more appropriate. If they need to publish reports and manage datasets they create, **Member** might be considered, but Contributor is a good starting point for report building without full workspace admin rights.

*   **Q2: Which Fabric tool is best for building visual, low-code/no-code ETL pipelines for ingesting data from various sources into the Lakehouse?**
    *   **A2:** **Dataflows Gen2** is the primary tool in Fabric for visual, low-code/no-code ETL/ELT pipeline creation. Data Factory pipelines orchestrate activities, which can include running Dataflows Gen2.

*   **Q3: How does OneLake enhance collaboration between different roles (e.g., Data Engineers, Data Scientists, BI Analysts) working on the same data?**
    *   **A3:** OneLake provides a **single, unified, and logical copy of the data** (stored in Delta Parquet format).
        *   **No Data Duplication:** Different roles and their preferred tools (Spark for Data Scientists/Engineers, SQL endpoint for Analysts, DirectLake for Power BI) can access the same underlying data in OneLake without creating multiple copies. This ensures everyone is working from the same version of truth.
        *   **Interoperability:** Data written by a Spark notebook is immediately queryable via the SQL endpoint and accessible in Power BI via DirectLake.
        *   **Shortcuts:** Data can be virtualized from other storage accounts or even other Fabric domains/workspaces, further enhancing collaboration without physically moving data.
        *   **Simplified Governance:** Managing security and governance is easier on a single data store.

---

### 2.10 Quiz
**1. What format does OneLake store data in?**
a) CSV
b) JSON
c) Delta Parquet
**â†’ Answer: c**

**2. What is the default security model for workspaces?**
a) Public
b) Private
c) Role-based access control
**â†’ Answer: c**

**3. What is the purpose of Microsoft Purview?**
a) Data visualization
b) Governance and compliance
c) Query optimization
**â†’ Answer: b**

---

### 2.11 Cheat Sheet (Printable)
*   **OneLake** = Unified, tenant-wide data lake
*   **Workspaces** = Role-bound collaborative zones
*   **Data Factory** = Pipelines for ingestion and transformation
*   **Purview** = Data governance, classification, audit, and lineage
*   **RBAC** = Assign only needed permissions
*   **Encryption** = In transit and at rest by default
*   **HIPAA** = Requires BAA + audit + access + integrity controls

---

### Interactive Element for Section 2
[Hands-on Setup Checklist](./visualizations-html/Section%202%20-%20Setting%20Up%20the%20Environment.html)

---

## ðŸ“˜ Section 3: Data Ingestion and Integration {#section-3-data-ingestion-and-integration}

---

### 3.0 Overview
The cornerstone of any successful data platformâ€”especially in healthcareâ€”is robust, scalable, secure **data ingestion and integration**. Healthcare organizations must harmonize fragmented datasets from a variety of clinical, operational, and research systems.

This section introduces how **Microsoft Fabric** helps data engineers ingest and integrate diverse datasets from EHRs, HL7 messages, FHIR APIs, imaging systems, pharmacy systems, and streaming medical devices. You'll also learn how to construct **batch and real-time ingestion pipelines** while preserving data lineage and privacy, enabling AI/BI downstream.

---

### 3.1 Key Principles of Healthcare Data Ingestion
#### 1. Diversity of Data Sources
Healthcare data may originate from:

| System Type | Example | Format/Source Type |
| :--- | :--- | :--- |
| EHR | Epic, Cerner | SQL DB, FHIR API, HL7 |
| Lab Systems | LIS | CSV, HL7 |
| Radiology | PACS | DICOM Metadata |
| Pharmacy | Cerner Rx | XML, CSV |
| Billing & Finance | RCM Systems | CSV, SQL |
| Wearables/IoT | CGMs, BP Monitors | Streaming JSON via Event Hub |
| Research Data | Clinical Trials | Excel, REDCap, Flat Files |

#### 2. Ingestion Modalities
*   **Batch Ingestion:** Scheduled jobs (hourly, daily) pulling from files or DBs
*   **Streaming Ingestion:** Real-time telemetry from IoT, EMR events
*   **API-Based Ingestion:** Pull data from RESTful FHIR servers or internal APIs

#### 3. Compliance Considerations
*   **Audit every data movement**
*   Encrypt data at every step
*   Use minimum necessary data (Privacy Rule)
*   Support de-identification pipelines

---

### 3.2 Using Microsoft Fabric Data Factory
**Data Factory** in Microsoft Fabric is your control hub for data ingestion and orchestration.

#### Key Components
| Component | Function |
| :--- | :--- |
| Pipelines | Sequence of steps/tasks |
| Dataflows Gen2 | Low-code Power Query-based transformations |
| Notebooks | PySpark-based custom logic |
| Linked Services | External source connectors |

#### Ingestion Flow
```
Source System â†’ Linked Service â†’ Dataflow/Notebook â†’ Lakehouse (Bronze) â†’ Transformation
```

#### Hands-On Setup
1.  **Launch Data Factory** in your Fabric workspace
2.  Click **New â†’ Pipeline**
3.  Add a **Dataflow Gen2** step
4.  Connect to a **FHIR server** using REST API
5.  Map FHIR `Patient` resource to a table
6.  Sink data into `Bronze_FHIRPatients` in Lakehouse

---

### 3.3 Ingesting HL7 Messages
HL7v2 is still a dominant format in clinical interfaces. It is delimited text and often transmitted in batched files.

#### Common HL7 Messages:
*   ADT (Admit, Discharge, Transfer)
*   ORU (Observation Result)
*   MDM (Medical Document Management)

#### Steps:
1.  Use a **file connector** to pull HL7 batch files
2.  Apply **Spark notebook** to parse segments (e.g., MSH, PID, OBX)
3.  Map to interim tables (e.g., `Bronze_HL7_ADT`)
4.  Optionally, transform to **FHIR-equivalent resources** (`Patient`, `Encounter`)

#### HIPAA Tip
Track and log:

*   Source file path
*   File checksum (data integrity)
*   Timestamp of ingestion
*   User/service principal ID performing ingestion

---

### 3.4 Integrating FHIR APIs
FHIR (Fast Healthcare Interoperability Resources) enables standardized data exchange.

#### Steps to Ingest FHIR:
1.  Create REST API linked service
2.  Use GET requests with filters:
    *   `/Patient?_lastUpdated=gt2024-01-01`
    *   `/Observation?code=1234-5`
3.  Store response in **Lakehouse â†’ Bronze_FHIR**
4.  Use **Dataflow Gen2** to flatten JSON
5.  Transform into a structured table `Silver_Patient`

#### Tips:
*   Use pagination and throttling
*   Use `since` parameters for delta loads
*   Encrypt tokens & headers in Key Vault

---

### 3.5 Handling DICOM Metadata
Though Fabric doesnâ€™t process full image binaries, it can ingest DICOM metadata.

#### Steps:
1.  Use a pipeline to read **DICOM metadata CSV**
2.  Include fields like:
    *   PatientID
    *   Modality
    *   StudyInstanceUID
    *   AcquisitionDate
3.  Load into Lakehouse `Bronze_DICOM_Metadata`
4.  Link metadata to encounter data for cohorting

---

### 3.6 Streaming IoT Data (e.g., Bedside Monitors)
Vital signs and telemetry can be streamed into Fabric via **Real-Time Analytics (KQL)** and Eventstreams.

#### Setup:
1.  Create an Event Hub (Azure) â†’ Forward to Eventstream in Fabric
2.  Use KQL query to ingest into `Bronze_StreamingVitals`
3.  Optionally: apply feature engineering in Notebooks
4.  Store results in `Silver_PatientVitals`

#### HIPAA Compliance
*   Use private endpoints
*   Encrypt message payload
*   Log event IDs, timestamps, sender identity

---

### 3.7 Building a Medallion Architecture
The **Bronze â†’ Silver â†’ Gold** pattern is crucial for scalable ingestion.

| Layer | Purpose | Example |
| :--- | :--- | :--- |
| Bronze | Raw ingested data | HL7 ADT messages |
| Silver | Cleaned, conformed, standardized | FHIR `Patient` resource |
| Gold | Aggregated, analytical structures | Dim_Patient, Fact_Visit |

---

### 3.8 Lab: Ingesting and Integrating Patient Encounter Data
**Objective:** Design and conceptually build a data ingestion pipeline in Microsoft Fabric to ingest patient encounter data from two different sources (a batch HL7 file and a FHIR API) and integrate them into a unified Silver layer table.

**Scenario:** You need to create a consolidated view of patient encounters. Some encounter data arrives as daily HL7 ADT batch files, while newer systems provide encounter information via a FHIR API.

**Assumed Prerequisites:**
* A Fabric workspace (e.g., `DEV_DataIntegration_YourName`) and a Lakehouse (e.g., `HealthDataLH_YourName`) are set up.
* For HL7: An Azure Data Lake Storage (ADLS) Gen2 container where HL7 batch files are dropped (e.g., `hl7-landing-zone/adt_batch_YYYYMMDD.hl7`).
* For FHIR: Access to a FHIR server endpoint (e.g., `https://your-fhir-server.com/fhir/`) and necessary authentication (e.g., API key or OAuth token).
* Python library for HL7 parsing (e.g., `hl7apy`) would be used in a real Spark Notebook scenario. For this lab, the parsing logic will be simplified.

**Creating sample `adt_batch_YYYYMMDD.hl7` file content:**
For demonstration, a simplified HL7-like structure. A real HL7 file is much more complex.

```hl7
MSH|^~\&|SENDING_APP|SENDING_FACILITY|RECEIVING_APP|RECEIVING_FACILITY|20240515103000||ADT^A01^ADT_A01|MSG00001|P|2.3
EVN|A01|20240515103000
PID|1||PATID12345^^^MRN|ALTID98765^^^SSN|DOE^JOHN^^^^^L||19800101|M||WH|123 MAIN ST^^ANYTOWN^CA^90210||(555)555-1212||ENG|S||PATID12345|123-45-6789
PV1|1|I|ICU^101^A||||ADMDR007^SMITH^JOHN^P|||SUR||||||ADMDR007||A0|20240515102500
DG1|1||I21.3^ACUTE MYOCARDIAL INFARCTION^ICD10
MSH|^~\&|SENDING_APP|SENDING_FACILITY|RECEIVING_APP|RECEIVING_FACILITY|20240515110000||ADT^A01^ADT_A01|MSG00002|P|2.3
EVN|A01|20240515110000
PID|1||PATID67890^^^MRN||ROE^JANE^^^^^L||19750315|F||AS|456 OAK AVE^^ANYCITY^CA^90211||(555)555-2323||ENG|M||PATID67890|234-56-7890
PV1|1|I|MEDSURG^205^B||||ADMDR008^BROWN^EMILY^P|||MED||||||ADMDR008||A0|20240515105500
DG1|1||J44.9^COPD UNSPECIFIED^ICD10
```

**Creating sample FHIR Encounter JSON response (conceptual for `Bronze_FHIR_Encounters_Raw`):**
A single encounter resource example:
```json
{
  "resourceType": "Encounter",
  "id": "ENC789",
  "status": "finished",
  "class": {
    "system": "http://terminology.hl7.org/CodeSystem/v3-ActCode",
    "code": "IMP",
    "display": "inpatient encounter"
  },
  "subject": {
    "reference": "Patient/PATFHIR001",
    "display": "Walter White"
  },
  "period": {
    "start": "2024-05-16T10:00:00Z",
    "end": "2024-05-20T14:30:00Z"
  },
  "hospitalization": {
    "admitSource": {
      "coding": [
        {
          "system": "http://terminology.hl7.org/CodeSystem/admit-source",
          "code": "gp",
          "display": "General Practitioner"
        }
      ]
    },
    "dischargeDisposition": {
      "coding": [
        {
          "system": "http://terminology.hl7.org/CodeSystem/discharge-disposition",
          "code": "home",
          "display": "Home"
        }
      ]
    }
  },
  "serviceProvider": {
    "reference": "Organization/ORG123",
    "display": "General Hospital"
  },
  "_lastUpdated": "2024-05-20T15:00:00Z"
}
```

---
**Lab Tasks (Conceptual Steps & Code):**

**1. Ingest HL7 ADT Batch File:**
    * **Pipeline Design (Conceptual UI Steps):**
        * **Step 1.1:** In your Fabric workspace, create a new **Data pipeline**. Name it `Ingest_HL7_Encounters_Pipeline_YourName`.
        * **Step 1.2:** Add a **Copy data** activity to the pipeline.
            * **Source Configuration:**
                * Connection: Create a new connection to Azure Data Lake Storage Gen2.
                * File path: Point to the container and directory where `adt_batch_YYYYMMDD.hl7` files are dropped (e.g., `hl7-landing-zone/`). Use wildcards or parameters for the filename to pick up daily files (e.g., `adt_batch_*.hl7`).
                * File format: Text format.
            * **Sink Configuration:**
                * Data store type: Workspace.
                * Workspace data store type: Lakehouse.
                * Lakehouse: Select `HealthDataLH_YourName`.
                * Table: Create a new table named `Bronze_HL7_ADT_RawFiles`. This table will store the raw file content, filename, and ingestion timestamp.
                    * *Alternative for parsing:* Instead of directly sinking to a structured table, you might sink the raw file to the `Files` section of the Lakehouse, then use a Notebook activity for parsing. For this lab, we'll assume a Notebook activity follows if direct parsing in Copy Data is not feasible for complex HL7.

        * **Step 1.3 (Preferred Method for HL7 Parsing): Add a Notebook activity** to the pipeline *after* the raw file is landed (e.g., in the `Files` section of the Lakehouse, path: `Files/bronze/hl7_adt_raw/adt_batch_YYYYMMDD.hl7`).
            * Notebook: Create a new Notebook named `Parse_HL7_ADT_YourName`.
            * This Notebook will contain PySpark code to read the raw HL7 file, parse it, and write structured data to `Bronze_HL7_Encounters_Raw`.

    * **Notebook Code (`Parse_HL7_ADT_YourName` - PySpark):**
        *Assume the raw HL7 file `adt_batch_20240515.hl7` (from our sample) has been copied to the `Files/bronze/hl7_adt_raw/` directory in your Lakehouse `HealthDataLH_YourName`.*

        ```python
        from pyspark.sql import Row
        from pyspark.sql.functions import udf, col, explode, lit
        from pyspark.sql.types import StringType, StructType, StructField, ArrayType
        import datetime

        # Define Lakehouse and table names
        lakehouse_name = "HealthDataLH_YourName" # Replace YourName
        bronze_hl7_encounters_table = f"{lakehouse_name}.Bronze_HL7_Encounters_Raw"
        
        # Path to the raw HL7 file in the Lakehouse Files section
        # This path might come from a pipeline parameter in a real scenario
        raw_hl7_file_path = f"/lakehouses/{lakehouse_name}/Files/bronze/hl7_adt_raw/adt_batch_20240515.hl7"
        source_filename = "adt_batch_20240515.hl7" # This would also typically be dynamic
        ingestion_timestamp = datetime.datetime.now()

        # Read the raw HL7 file
        # Each line in the RDD will be a segment from the HL7 file
        # For simplicity, we'll read the whole file content first.
        # In a real scenario, you'd handle multiple files or split messages properly.
        try:
            hl7_content_rdd = spark.sparkContext.textFile(raw_hl7_file_path)
            full_hl7_content = "\n".join(hl7_content_rdd.collect())
        except Exception as e:
            print(f"Error reading file {raw_hl7_file_path}: {e}")
            dbutils.notebook.exit(f"Failed to read {raw_hl7_file_path}")


        # Simplified parsing logic for demonstration.
        # A real HL7 parser (like hl7apy) would be much more robust.
        def parse_simplified_hl7_messages(hl7_text):
            messages_data = []
            messages = hl7_text.strip().split('MSH|^~\\&|') # Basic split by MSH, not perfectly robust
            
            for msg_content in messages:
                if not msg_content.strip():
                    continue
                
                # Reconstruct the message with MSH prefix for parsing
                full_msg_str = "MSH|^~\\&|" + msg_content
                segments = full_msg_str.strip().split('\n')
                
                parsed_encounter = {
                    "PatientID_MRN": None, "PatientName": None, "PatientDOB": None, "PatientGender": None,
                    "VisitID": None, "AdmissionDateTime": None, "DischargeDateTime": None, # Discharge typically from A03
                    "PatientClass": None, "AttendingProviderID": None, "AttendingProviderName": None,
                    "DiagnosisCode": None, "DiagnosisDescription": None,
                    "RawMessageSegment": full_msg_str # Store the segment for audit
                }

                for segment in segments:
                    fields = segment.split('|')
                    segment_type = fields[0]

                    if segment_type == "PID":
                        try:
                            # PID-3 (Patient Identifier List - first rep, first component for MRN)
                            if len(fields) > 3 and fields[3]:
                                parsed_encounter["PatientID_MRN"] = fields[3].split('^')[0]
                            # PID-5 (Patient Name - first rep)
                            if len(fields) > 5 and fields[5]:
                                name_parts = fields[5].split('^')
                                parsed_encounter["PatientName"] = f"{name_parts[0]}, {name_parts[1]}" if len(name_parts) >= 2 else name_parts[0]
                            # PID-7 (Date/Time of Birth)
                            if len(fields) > 7 and fields[7]:
                                parsed_encounter["PatientDOB"] = fields[7]
                            # PID-8 (Administrative Sex)
                            if len(fields) > 8 and fields[8]:
                                parsed_encounter["PatientGender"] = fields[8]
                        except IndexError:
                            print(f"Warning: PID segment parsing error for message: {full_msg_str[:50]}...")
                            pass


                    elif segment_type == "PV1":
                        try:
                            # PV1-19 (Visit Number - if available, else use other logic for VisitID)
                            if len(fields) > 19 and fields[19]:
                                parsed_encounter["VisitID"] = fields[19].split('^')[0] 
                            # PV1-2 (Patient Class)
                            if len(fields) > 2 and fields[2]:
                                parsed_encounter["PatientClass"] = fields[2]
                            # PV1-7 (Attending Doctor - first rep)
                            if len(fields) > 7 and fields[7]:
                                prov_parts = fields[7].split('^')
                                parsed_encounter["AttendingProviderID"] = prov_parts[0]
                                parsed_encounter["AttendingProviderName"] = f"{prov_parts[1]}, {prov_parts[2]}" if len(prov_parts) >=3 else prov_parts[1] if len(prov_parts) >=2 else prov_parts[0]
                            # PV1-44 (Admission Date/Time)
                            if len(fields) > 44 and fields[44]:
                                parsed_encounter["AdmissionDateTime"] = fields[44]
                            # PV1-45 (Discharge Date/Time) - Usually in ADT-A03, A01 is admission
                            if len(fields) > 45 and fields[45]:
                                parsed_encounter["DischargeDateTime"] = fields[45] 
                        except IndexError:
                            print(f"Warning: PV1 segment parsing error for message: {full_msg_str[:50]}...")
                            pass
                    
                    elif segment_type == "DG1": # Assuming first DG1 is primary
                        try:
                            # DG1-3 (Diagnosis Code - ICD)
                            if len(fields) > 3 and fields[3]:
                                diag_parts = fields[3].split('^')
                                parsed_encounter["DiagnosisCode"] = diag_parts[0]
                                if len(diag_parts) > 1:
                                    parsed_encounter["DiagnosisDescription"] = diag_parts[1]
                        except IndexError:
                            print(f"Warning: DG1 segment parsing error for message: {full_msg_str[:50]}...")
                            pass
                
                if parsed_encounter["PatientID_MRN"] and parsed_encounter["AdmissionDateTime"]: # Basic check
                    messages_data.append(parsed_encounter)
            return messages_data

        # Parse the content
        parsed_hl7_data_list = parse_simplified_hl7_messages(full_hl7_content)

        if not parsed_hl7_data_list:
            print("No HL7 messages could be parsed. Exiting.")
            dbutils.notebook.exit("No HL7 data parsed.")

        # Create DataFrame
        hl7_df = spark.createDataFrame([Row(**x) for x in parsed_hl7_data_list])
        
        # Add audit columns
        final_hl7_df = hl7_df.withColumn("SourceFile", lit(source_filename)) \
                             .withColumn("IngestionTimestamp", lit(ingestion_timestamp).cast("timestamp"))

        # Write to Bronze Lakehouse table
        final_hl7_df.write.format("delta").mode("append").saveAsTable(bronze_hl7_encounters_table)
        print(f"Successfully ingested and parsed HL7 data into {bronze_hl7_encounters_table}")
        final_hl7_df.show(truncate=False)
        ```

---
**2. Ingest FHIR Encounter Resources:**
    * **Pipeline Design (Conceptual UI Steps):**
        * **Step 2.1:** In your Fabric workspace, create or use an existing Data pipeline (e.g., `Ingest_FHIR_Encounters_Pipeline_YourName`).
        * **Step 2.2:** Add a **Copy data** activity (or a **Dataflow Gen2** for more complex handling of API calls and pagination).
            * **Source Configuration (using Copy data with REST source):**
                * Connection: Create a new **REST** linked service.
                    * Base URL: `https://your-fhir-server.com/fhir/` (Replace with actual server)
                    * Authentication: Select appropriate type (e.g., Basic, OAuth2, API Key). Configure securely (e.g., use Azure Key Vault for secrets).
                * Relative URL: `Encounter` (or `Encounter?_lastUpdated=gt{LastProcessedTimestamp}` for delta loads â€“ LastProcessedTimestamp would be a pipeline variable or lookup).
                * Request method: GET.
                * Pagination rules: Configure if the API supports pagination (e.g., using Link headers or offset/limit parameters). This is critical for fetching all data.
            * **Sink Configuration:**
                * Data store type: Workspace.
                * Workspace data store type: Lakehouse.
                * Lakehouse: Select `HealthDataLH_YourName`.
                * Table: Create a new table named `Bronze_FHIR_Encounters_RawJSON`. This table will store the raw JSON responses from the API. It's good practice to include a column for the API call timestamp.
                    * Each row could represent one fetched JSON object (encounter) or a batch of responses.
                    * For simplicity here, we'll assume each row stores one encounter's JSON string.
        * *(Alternative: Use a Notebook activity with Python `requests` library for more custom control over API calls, error handling, and token management if Copy data activity is insufficient).*

    * **Notebook Code (Conceptual for processing raw JSON if landed by Copy Data):**
        *Suppose the `Bronze_FHIR_Encounters_RawJSON` table has a column `RawEncounterJSON` (string) and `APICallTimestamp`.*

        ```python
        from pyspark.sql.functions import from_json, col, lit, current_timestamp
        from pyspark.sql.types import StructType, StringType # Define schema based on expected JSON

        # Define Lakehouse and table names
        lakehouse_name = "HealthDataLH_YourName" # Replace YourName
        source_raw_json_table = f"{lakehouse_name}.Bronze_FHIR_Encounters_RawJSON"
        bronze_fhir_encounters_table = f"{lakehouse_name}.Bronze_FHIR_Encounters_Parsed" # Storing parsed data

        # Define a simplified schema for the FHIR Encounter resource
        # A more complete schema would be needed for production from FHIR specifications
        fhir_encounter_schema = StructType.fromJson({
            "fields": [
                {"metadata": {}, "name": "resourceType", "nullable": True, "type": "string"},
                {"metadata": {}, "name": "id", "nullable": True, "type": "string"},
                {"metadata": {}, "name": "status", "nullable": True, "type": "string"},
                {"metadata": {}, "name": "class", "nullable": True, "type": {"fields": [
                    {"metadata": {}, "name": "system", "nullable": True, "type": "string"},
                    {"metadata": {}, "name": "code", "nullable": True, "type": "string"},
                    {"metadata": {}, "name": "display", "nullable": True, "type": "string"}
                ], "type": "struct"}},
                {"metadata": {}, "name": "subject", "nullable": True, "type": {"fields": [
                    {"metadata": {}, "name": "reference", "nullable": True, "type": "string"},
                    {"metadata": {}, "name": "display", "nullable": True, "type": "string"}
                ], "type": "struct"}},
                {"metadata": {}, "name": "period", "nullable": True, "type": {"fields": [
                    {"metadata": {}, "name": "start", "nullable": True, "type": "string"}, # Timestamp
                    {"metadata": {}, "name": "end", "nullable": True, "type": "string"}   # Timestamp
                ], "type": "struct"}},
                {"metadata": {}, "name": "_lastUpdated", "nullable": True, "type": "string"} # Timestamp
            ], "type": "struct"
        })

        # Read the raw JSON data
        try:
            raw_json_df = spark.read.table(source_raw_json_table)
        except Exception as e:
            print(f"Error reading table {source_raw_json_table}: {e}")
            dbutils.notebook.exit(f"Failed to read {source_raw_json_table}")

        # Parse the JSON string column
        parsed_df = raw_json_df.withColumn("ParsedData", from_json(col("RawEncounterJSON"), fhir_encounter_schema))

        # Select the parsed fields and audit columns
        # Flatten the structure as needed
        fhir_encounters_df = parsed_df.select(
            col("ParsedData.id").alias("EncounterFHIR_ID"),
            col("ParsedData.status").alias("EncounterStatus"),
            col("ParsedData.class.code").alias("EncounterClassCode"),
            col("ParsedData.class.display").alias("EncounterClassDisplay"),
            col("ParsedData.subject.reference").alias("PatientFHIR_Reference"),
            col("ParsedData.period.start").alias("AdmissionDateTime"),
            col("ParsedData.period.end").alias("DischargeDateTime"),
            col("ParsedData._lastUpdated").alias("SourceLastUpdated"),
            col("APICallTimestamp"), # Assuming this column exists from the Copy activity
            col("RawEncounterJSON") # Keep raw JSON for audit/reprocessing
        ).withColumn("IngestionTimestamp", current_timestamp())

        # Write to a parsed Bronze table
        fhir_encounters_df.write.format("delta").mode("append").saveAsTable(bronze_fhir_encounters_table)
        print(f"Successfully ingested and parsed FHIR Encounter data into {bronze_fhir_encounters_table}")
        fhir_encounters_df.show(truncate=False)
        ```

---
**3. Conform and Integrate into a Silver Layer Table:**
    * **Tool:** Spark Notebook or Dataflow Gen2. (Using Spark Notebook for more complex logic demonstration).
    * **Notebook Name:** `Conform_Encounters_To_Silver_YourName`
    * **Logic (PySpark Code):**

        ```python
        from pyspark.sql.functions import col, lit, when, to_timestamp, expr, coalesce
        import datetime

        # Define Lakehouse and table names
        lakehouse_name = "HealthDataLH_YourName" # Replace YourName
        bronze_hl7_table = f"{lakehouse_name}.Bronze_HL7_Encounters_Raw"
        bronze_fhir_table = f"{lakehouse_name}.Bronze_FHIR_Encounters_Parsed" # Using the parsed FHIR table
        silver_unified_encounters_table = f"{lakehouse_name}.Silver_Unified_Encounters"

        # Read from Bronze tables
        try:
            hl7_enc_df = spark.read.table(bronze_hl7_table)
            fhir_enc_df = spark.read.table(bronze_fhir_table)
        except Exception as e:
            print(f"Error reading bronze tables: {e}")
            dbutils.notebook.exit("Failed to read bronze tables.")

        # --- Transform HL7 Data to Common Schema ---
        # Convert HL7 admission date (YYYYMMDDHHMMSS) to timestamp
        # Note: HL7 date parsing can be tricky; this is a simplified approach.
        transformed_hl7_df = hl7_enc_df.withColumn(
            "AdmissionDateTime_ts",
            when(col("AdmissionDateTime").isNotNull(), 
                 to_timestamp(col("AdmissionDateTime"), "yyyyMMddHHmmss"))
            .otherwise(None)
        ).withColumn(
            "DischargeDateTime_ts", # Assuming DischargeDateTime might also be in YYYYMMDDHHMMSS
             when(col("DischargeDateTime").isNotNull(), 
                 to_timestamp(col("DischargeDateTime"), "yyyyMMddHHmmss"))
            .otherwise(None)
        ).select(
            col("VisitID").alias("SourceSystemEncounterID"), # May need better EncounterID logic
            col("PatientID_MRN").alias("PatientIdentifier"),
            col("AdmissionDateTime_ts").alias("AdmissionDateTime"),
            col("DischargeDateTime_ts").alias("DischargeDateTime"),
            col("PatientClass").alias("EncounterType"), # Needs mapping to standard codes
            col("AttendingProviderID").alias("AttendingProviderIdentifier"),
            col("DiagnosisCode").alias("PrimaryDiagnosisCode"), # Assuming first DG1 is primary
            lit("HL7v2").alias("SourceSystem"),
            col("IngestionTimestamp").alias("SourceIngestionTimestamp")
        )

        # --- Transform FHIR Data to Common Schema ---
        transformed_fhir_df = fhir_enc_df.withColumn(
            "AdmissionDateTime_ts",
            to_timestamp(col("AdmissionDateTime")) 
        ).withColumn(
            "DischargeDateTime_ts",
            to_timestamp(col("DischargeDateTime"))
        ).withColumn(
            "PatientIdentifier", # Extract patient ID from 'PatientFHIR_Reference' (e.g., "Patient/PATFHIR001" -> "PATFHIR001")
            expr("substring(PatientFHIR_Reference, instr(PatientFHIR_Reference, '/') + 1)")
        ).select(
            col("EncounterFHIR_ID").alias("SourceSystemEncounterID"),
            col("PatientIdentifier"),
            col("AdmissionDateTime_ts").alias("AdmissionDateTime"),
            col("DischargeDateTime_ts").alias("DischargeDateTime"),
            col("EncounterClassCode").alias("EncounterType"), # Needs mapping to standard codes
            lit(None).cast("string").alias("AttendingProviderIdentifier"), # Assuming not directly available or needs lookup
            lit(None).cast("string").alias("PrimaryDiagnosisCode"), # Assuming not directly available or needs lookup from Condition resource
            lit("FHIR_API").alias("SourceSystem"),
            col("IngestionTimestamp").alias("SourceIngestionTimestamp")
        )
        
        # --- Union the transformed data ---
        # Ensure columns are aligned for the union
        common_columns = [
            "SourceSystemEncounterID", "PatientIdentifier", "AdmissionDateTime", "DischargeDateTime",
            "EncounterType", "AttendingProviderIdentifier", "PrimaryDiagnosisCode",
            "SourceSystem", "SourceIngestionTimestamp"
        ]
        
        # Select common columns in the same order
        aligned_hl7_df = transformed_hl7_df.select(common_columns)
        aligned_fhir_df = transformed_fhir_df.select(common_columns)
        
        unified_encounters_df = aligned_hl7_df.unionByName(aligned_fhir_df, allowMissingColumns=True)

        # --- Perform De-duplication (Example: based on PatientIdentifier and AdmissionDateTime) ---
        # This is a simplified de-duplication. Real-world scenarios might need more sophisticated logic.
        # Using SourceSystemEncounterID if it's globally unique across systems, or a combination.
        # For this example, let's assume (PatientIdentifier, AdmissionDateTime, SourceSystem) as a composite key for an encounter for deduplication.
        # We can also add a preference for source if duplicates exist (e.g., prefer FHIR if both exist for the same event).
        
        # Add a row number partitioned by a potential unique key combination
        window_spec = spark.catalog.Window.partitionBy("PatientIdentifier", "AdmissionDateTime").orderBy(col("SourceIngestionTimestamp").desc()) # Keep the latest ingested record
        
        deduplicated_df = unified_encounters_df.withColumn("row_num", expr("row_number() OVER (PARTITION BY PatientIdentifier, AdmissionDateTime ORDER BY SourceIngestionTimestamp DESC)")) \
                                            .filter(col("row_num") == 1) \
                                            .drop("row_num")

        # --- Add Silver layer audit columns ---
        final_silver_df = deduplicated_df.withColumn("SilverLoadTimestamp", lit(datetime.datetime.now()).cast("timestamp")) \
                                         .withColumn("EncounterSK", expr("md5(concat_ws('|', coalesce(SourceSystemEncounterID, ''), coalesce(PatientIdentifier,''), coalesce(cast(AdmissionDateTime as string),''), SourceSystem))")) # Example surrogate key

        # Write to Silver Lakehouse table
        final_silver_df.write.format("delta").mode("overwrite").saveAsTable(silver_unified_encounters_table) # Use overwrite for idempotency in DEV
        print(f"Successfully conformed and integrated encounter data into {silver_unified_encounters_table}")
        final_silver_df.show(truncate=False)
        ```

---
**Discussion Questions:**
1.  **When parsing the HL7 PID segment and the FHIR `Encounter.subject.reference`, what common patient identifier would you aim to extract or map to ensure you can link encounters to the correct patient?**
    * You would aim to extract or map to a **Master Patient Identifier (MPI)** or a common enterprise-wide patient identifier.
        * From HL7 PID-3 (Patient Identifier List), you'd typically look for the MRN (Medical Record Number) or a specific identifier assigned by an MPI system.
        * From FHIR `Encounter.subject.reference` (e.g., "Patient/PATFHIR001"), the `PATFHIR001` part is the logical ID of the Patient resource. This logical ID within the FHIR server should correspond to a unique patient, ideally linked to or being the MPI.
        * The goal is to resolve these potentially different source identifiers to a single, canonical patient identifier used across the integrated data.

2.  **Why is it beneficial to keep the `Bronze_HL7_Encounters_Raw` (or raw HL7 files/parsed data) and `Bronze_FHIR_Encounters_RawJSON` tables even after creating the `Silver_Unified_Encounters` table?**
    * **Auditability and Traceability:** Bronze tables store data in its original or near-original state. This allows for auditing back to the source and understanding exactly what data was received. This is crucial for compliance and data lineage.
    * **Reprocessing:** If errors are found in the transformation logic for the Silver layer, or if new requirements emerge, the raw Bronze data can be reprocessed without needing to re-ingest from the source systems. This saves time and reduces load on operational systems.
    * **Data Lineage & Debugging:** When troubleshooting issues in the Silver or Gold layers, having the raw Bronze data helps in tracing data transformations and identifying where potential errors or discrepancies were introduced.
    * **Historical Archive:** Bronze serves as an immutable historical archive of all data ingested, which can be important for long-term data retention policies or for future analytical needs not yet defined.
    * **Schema Evolution:** Source systems might change their schema. The Bronze layer captures this raw data, allowing for adaptation of transformation logic over time.

3.  **What are two key pieces of metadata or logging information you would ensure are captured during the ingestion of the HL7 batch file for audit and traceability purposes?**
    * **Source File Name and Path:** Knowing the exact file from which a record originated (e.g., `adt_batch_20240515_01.hl7`) is crucial for tracing data back to its entry point.
    * **Ingestion Timestamp:** Recording the date and time when the file (or each message/record within it) was ingested into the system. This helps in understanding data latency and in reconstructing the state of the data at a particular point in time.
    * *(Other important metadata: Checksum of the file (e.g., MD5 or SHA256) to ensure data integrity during transfer, number of messages/records in the file, status of ingestion (success/failure), user or service principal ID that performed the ingestion).*

4.  **If the FHIR API requires an OAuth2.0 token for authentication, how would you securely manage this token within your Fabric pipeline?**
    * **Azure Key Vault Integration:** The recommended approach is to store the client ID, client secret (or certificate), and token endpoint URL required for OAuth2.0 flow in Azure Key Vault.
    * **Fabric Linked Service Configuration:** When creating the REST Linked Service in Fabric Data Factory for the FHIR API, you would configure it to retrieve these secrets from Azure Key Vault. Fabric can use a Managed Identity (MSI) for the Fabric workspace/Data Factory to authenticate to Key Vault, avoiding the need to store secrets directly in the pipeline definition.
    * **Token Acquisition and Caching:**
        * **Copy Activity:** Some connectors in Fabric's Copy activity might have built-in OAuth2.0 handling where you configure the necessary parameters, and it manages token acquisition and refresh.
        * **Notebook/Custom Activity:** If more control is needed, you would use a Web activity or a Notebook (e.g., Python with `msal` or `requests_oauthlib` libraries) at the beginning of your pipeline to call the token endpoint, acquire the OAuth2.0 token, and then pass this token (e.g., as a pipeline variable or output) to subsequent activities that call the FHIR API. The token itself should be handled as a secret.
        * It's important to handle token expiration and refresh logic. The acquired token should be passed in the Authorization header (e.g., `Authorization: Bearer <token>`) of the API requests.
    * **Never hardcode tokens or secrets** directly in pipeline definitions, notebooks, or source code.

---

### 3.9 Quiz
**1. What Fabric tool is best suited for visual low-code ingestion?**
a) Notebooks
b) Dataflows Gen2
c) Power BI
**â†’ Answer: b**

**2. HL7 ADT messages are commonly used for?**
a) Imaging
b) Discharge and transfer events
c) Lab tests
**â†’ Answer: b**

**3. What does the Bronze layer represent?**
a) Raw ingested data
b) AI predictions
c) Visual dashboards
**â†’ Answer: a**

---

### 3.10 Cheat Sheet (Printable)
*   **Bronze = Raw data** (e.g., HL7, FHIR API response)
*   **Silver = Cleaned/conformed** (e.g., structured Encounter table)
*   **Gold = Aggregated/analytic** (e.g., dashboards, ML input)
*   **Dataflows Gen2** = Visual ingestion and transformation
*   **Notebooks** = Custom PySpark parsing (e.g., HL7, JSON)
*   **Eventstream + KQL** = Real-time device data
*   **Always log:** timestamp, source, user ID, file checksum

---

### Interactive Element for Section 3
[Embedded Quiz: Data Pipeline Best Practices](./visualizations-html/Section%203%20-%20Data%20Ingestion%20and%20Integration.html)

---

## ðŸ“˜ Section 4: Data Modeling and Transformation {#section-4-data-modeling-and-transformation}

---

### 4.0 Overview
Data modeling and transformation are the backbone of usable analytics and AI. Once healthcare data is ingestedâ€”be it from FHIR APIs, HL7 batches, or DICOM metadataâ€”it needs to be **cleansed**, **standardized**, **linked**, and **modeled** to create business-ready structures.

In Microsoft Fabric, this is achieved through **Lakehouse-based medallion architecture**, supported by **Dataflows Gen2**, **Spark Notebooks**, and **SQL-based Warehouses**. This section will train data engineers to create resilient data models, ensure data quality, and transform fragmented datasets into conformed, analytics-ready layers.

---

### 4.1 Introduction to Healthcare Data Modeling
<figure>
  <img src="./images/4.1-Healthcare%20Data%20Modeling%20&%20Transformation%20Diagram.png" alt="Data Modeling & Transformation Diagram" style="width:100%;" />
  <figcaption><em>Figure: Data Modeling & Transformation Diagram.</em></figcaption>
</figure>

Healthcare data is complex. A single patient journey might touch multiple departments, systems, and events.

#### Key Modeling Concepts:
| Concept | Description |
| :--- | :--- |
| Entity Modeling | Identifying real-world entities (Patients, Visits, Medications) |
| Relationship Modeling | Capturing associations (e.g., a Patient has many Encounters) |
| Normalization | Structuring data to reduce redundancy (3NF for OLTP) |
| Denormalization | Flattening for performance (used in OLAP for analytics) |

---

### 4.2 The Medallion Architecture: Revisited
The **Bronze â†’ Silver â†’ Gold** approach is a modeling pattern that guides transformation.

| Layer | Description | Example |
| :--- | :--- | :--- |
| Bronze | Raw data | HL7 segments, FHIR JSON |
| Silver | Cleansed, standardized, FHIR-aligned | `Patient`, `Encounter`, `Condition` |
| Gold | Denormalized, aggregated | `Dim_Patient`, `Fact_Visit`, `Patient_360` |

#### Benefits:
*   Supports **lineage tracking** (HIPAA audit)
*   Enforces **data quality rules** progressively
*   Enables **AI model training** and **Power BI reporting**

---

### 4.3 Tools for Transformation in Fabric
#### 1. **Dataflows Gen2**
*   Visual, no/low-code interface using Power Query
*   Best for:
    *   Joins
    *   Conditional logic
    *   Lookups
    *   Formatting

#### 2. **Notebooks (PySpark, Python)**
*   Best for:
    *   Advanced transformations
    *   HL7 parsing
    *   JSON unnesting
    *   Data cleansing pipelines
    *   De-identification

#### 3. **SQL in Warehouses**
*   Use for:
    *   View creation (`CREATE VIEW`)
    *   Aggregate metrics
    *   Fact/dimension tables

---

### 4.4 Modeling FHIR Resources
FHIR defines **modular â€œresourcesâ€** such as `Patient`, `Encounter`, `Condition`, `Observation`, etc.

#### Example Mapping: HL7 to FHIR
| HL7 Segment | FHIR Resource | Field Mapping |
| :--- | :--- | :--- |
| PID | Patient | PID-3 â†’ Patient.identifier |
| PV1 | Encounter | PV1-2 â†’ Encounter.class |
| OBX | Observation | OBX-5 â†’ Observation.valueQuantity |

#### Steps:
1.  Parse HL7 using Notebooks â†’ Bronze
2.  Map segments â†’ FHIR fields â†’ Silver
3.  Store structured tables (e.g., `Silver_FHIR_Encounter`)
4.  Link via reference fields (e.g., `Encounter.subject = Patient.id`)

> ðŸ’¡ *Use FHIR canonical URLs and codings for interoperability*

---

### 4.5 Dimensional Modeling for Analytics (Gold Layer)
The Gold Layer consists of **business-oriented data marts** that power BI reports and ML models.

#### Example: Dimensional Model
| Table | Purpose | Sample Columns |
| :--- | :--- | :--- |
| Dim_Patient | Master view of patients | PatientID, DOB, Sex, Race |
| Dim_Provider | Care team | ProviderID, NPI, Specialty |
| Fact_Encounter | Visit-level events | EncounterID, PatientID, Date, Type |
| Fact_LabResult | Labs | ResultID, LOINC, Value, Date |

#### Keys:
*   Use **surrogate keys** (non-PHI)
*   Maintain **PatientID mapping tables** for de-identification
*   Add **record audit columns**: `CreatedBy`, `UpdatedOn`, `SourceSystem`

---

### 4.6 Schema Evolution and Validation
Healthcare data is **dynamic**â€”new codes, structures, regulations emerge.

#### Best Practices:
*   Use **Delta format** in OneLake to enable:
    *   **Schema evolution** (new fields without breaking pipeline)
    *   **Time travel** for audit and rollback
*   Enforce **data contracts** via Power Query schema checks
*   Use **validation layers**:
    *   Required fields (`Patient.birthDate`)
    *   Value ranges (A1c between 2.0â€“20.0%)
    *   Categorical validity (e.g., `Sex` âˆˆ [M, F, O, U])

---

### 4.7 Data Quality and Referential Integrity
#### Key Techniques:
| Technique | Tool | Purpose |
| :--- | :--- | :--- |
| Null Checks | Dataflow or Notebook | Ensure required fields |
| Lookup Validation | SQL JOINs or Dataflows | Ensure valid foreign keys |
| De-duplication | PySpark or SQL | Remove redundant records |
| Anomaly Detection | Notebooks (z-score, IQR) | Identify outliers |

#### Healthcare Use Case:
*Two systems record the same patient differently.* Use **Master Patient Index (MPI)** or **hash-based linking** (e.g., SSN + DOB + ZIP) to consolidate.

> ðŸ›¡ï¸ *All patient IDs must be masked, encrypted, or tokenized in Gold layer for external analytics.*

---

### 4.8 Lab: Modeling Patient Encounters for Analytics
**Objective:** Design and conceptually implement the transformation of raw patient encounter data (from Bronze) into a structured Silver layer table and then into a simple Gold layer dimensional model.

**Assumed Bronze Layer Data (from previous ingestion labs):**

*   `Bronze_HL7_Encounters_Raw` (parsed from HL7 ADT messages in Lakehouse: `HealthDataLH_YourName`)
*   `Bronze_FHIR_Encounters_Parsed` (parsed JSON from FHIR API in Lakehouse: `HealthDataLH_YourName`)
*   A `Bronze_Patients_Raw` table. For this lab, let's define its schema and provide sample data creation.

**Creating Sample `Bronze_Patients_Raw` Data (PySpark Notebook Cell):**

This would typically be ingested from an EHR dump or FHIR Patient resources.

```python
from pyspark.sql import Row
import datetime

lakehouse_name = "HealthDataLH_YourName" # Replace YourName
bronze_patients_table = f"{lakehouse_name}.Bronze_Patients_Raw"

# Sample patient data (could come from various sources)
patient_data = [
    Row(PatientSourceID="PATID12345", MRN="MRN001", SSN_Token="TOKEN_SSN1", FirstName="John", LastName="Doe", DateOfBirth=datetime.date(1980, 1, 1), Gender="M", Race="WH", Ethnicity="N", ZipCode="90210", SourceSystem="EHR_SystemA", IngestionTimestamp=datetime.datetime.now()),
    Row(PatientSourceID="PATID67890", MRN="MRN002", SSN_Token="TOKEN_SSN2", FirstName="Jane", LastName="Roe", DateOfBirth=datetime.date(1975, 3, 15), Gender="F", Race="AS", Ethnicity="N", ZipCode="90211", SourceSystem="EHR_SystemA", IngestionTimestamp=datetime.datetime.now()),
    Row(PatientSourceID="PATFHIR001", MRN="MRN003", SSN_Token="TOKEN_SSN3", FirstName="Walter", LastName="White", DateOfBirth=datetime.date(1965, 9, 7), Gender="M", Race="WH", Ethnicity="N", ZipCode="87101", SourceSystem="FHIR_API", IngestionTimestamp=datetime.datetime.now()),
    Row(PatientSourceID="EXTRALONGPATIENTID004", MRN="MRN004", SSN_Token="TOKEN_SSN4", FirstName="Sarah", LastName="Connor", DateOfBirth=datetime.date(1985, 5, 10), Gender="F", Race="WH", Ethnicity="H", ZipCode="90001", SourceSystem="EHR_SystemB", IngestionTimestamp=datetime.datetime.now()),
    Row(PatientSourceID="PATID12345", MRN="MRN001", SSN_Token="TOKEN_SSN1", FirstName="John", LastName="Doe", DateOfBirth=datetime.date(1980, 1, 1), Gender="M", Race="WH", Ethnicity="N", ZipCode="90210", SourceSystem="EHR_SystemA_OldRecord", IngestionTimestamp=datetime.datetime.now()-datetime.timedelta(days=10)) # Duplicate for testing dedupe
]

patients_df = spark.createDataFrame(patient_data)

patients_df.write.format("delta").mode("overwrite").saveAsTable(bronze_patients_table)
print(f"Sample data written to {bronze_patients_table}")
spark.read.table(bronze_patients_table).show()
```

**Assumed Silver Layer Data (from Lab 3.8):**

*   `Silver_Unified_Encounters` (in Lakehouse: `HealthDataLH_YourName`)

**Lab Tasks (Conceptual Steps & Code):**

**1. (Re-run/Verify) Create `Silver_Unified_Encounters` Table:**

*   **Tool:** Spark Notebook (from Lab 3.8).
*   **Logic:** Ensure the PySpark code from Lab 3.8, Task 3 (Conform and Integrate into a Silver Layer Table) has been run and the `Silver_Unified_Encounters` table exists in your `HealthDataLH_YourName` Lakehouse. This table should have columns like `SourceSystemEncounterID`, `PatientIdentifier`, `AdmissionDateTime`, `DischargeDateTime`, `EncounterType`, `AttendingProviderIdentifier`, `PrimaryDiagnosisCode`, `SourceSystem`, `SilverLoadTimestamp`, `EncounterSK`.

---

**2. Create `Dim_Patient` (Dimension Table - Gold Layer):**

*   **Tool:** Spark Notebook.
*   **Notebook Name:** `Create_Dim_Patient_Gold_YourName`
*   **Logic (PySpark Code):**

```python
from pyspark.sql.functions import col, lit, current_timestamp, expr, monotonically_increasing_id, md5, concat_ws, year, month, dayofmonth, datediff, current_date, coalesce
import datetime

lakehouse_name = "HealthDataLH_YourName" # Replace YourName
bronze_patients_table = f"{lakehouse_name}.Bronze_Patients_Raw"
dim_patient_table = f"{lakehouse_name}.Dim_Patient" # Gold Layer Table

# Read from Bronze_Patients_Raw table
try:
    raw_patients_df = spark.read.table(bronze_patients_table)
except Exception as e:
    print(f"Error reading table {bronze_patients_table}: {e}")
    dbutils.notebook.exit(f"Failed to read {bronze_patients_table}")

# --- De-duplicate patient records ---
# Assuming MRN is a good candidate for a natural key for de-duplication
# Keep the record with the latest ingestion timestamp in case of duplicates
window_spec_patient = spark.catalog.Window.partitionBy("MRN").orderBy(col("IngestionTimestamp").desc())

deduplicated_patients_df = raw_patients_df.withColumn("row_num", expr("row_number() OVER (PARTITION BY MRN ORDER BY IngestionTimestamp DESC)")) \
    .filter(col("row_num") == 1) \
    .drop("row_num")

# --- Select and Transform for Dimension Table ---
dim_patient_df = deduplicated_patients_df.select(
    col("MRN").alias("PatientNaturalKey"), # Natural Key from source system (e.g., MRN)
    col("PatientSourceID").alias("SourcePatientID"),
    col("FirstName"),
    col("LastName"),
    col("DateOfBirth").cast("date"),
    col("Gender"), # Consider mapping to standard codes if necessary
    col("Race"),   # Consider mapping
    col("Ethnicity"), # Consider mapping
    col("ZipCode"),
    col("SourceSystem").alias("PatientSourceSystem")
).withColumn(
    "PatientKey", md5(col("PatientNaturalKey")) # Surrogate Key (MD5 hash of MRN for simplicity)
    # For a robust SK, an incrementing ID or a more sophisticated hash might be used.
    # monotonically_increasing_id() can be used but has caveats in distributed environments for strict sequential IDs.
).withColumn(
    "FullName", expr("concat(FirstName, ' ', LastName)")
).withColumn(
    "Age", expr("floor(datediff(current_date(), DateOfBirth) / 365.25)").cast("int") # Calculated Age
).withColumn(
    "GoldLoadTimestamp", current_timestamp()
).withColumn(
    "EffectiveStartDate", lit(datetime.date(1900, 1, 1)).cast("date") # For Type 2 SCD, default
).withColumn(
    "EffectiveEndDate", lit(None).cast("date") # For Type 2 SCD, default
).withColumn(
    "IsCurrent", lit(True).cast("boolean") # For Type 2 SCD
)

# Reorder columns for clarity in the dimension table
final_dim_patient_df = dim_patient_df.select(
    "PatientKey", "PatientNaturalKey", "SourcePatientID", "FirstName", "LastName", "FullName", "DateOfBirth", "Age", "Gender",
    "Race", "Ethnicity", "ZipCode", "PatientSourceSystem",
    "EffectiveStartDate", "EffectiveEndDate", "IsCurrent", "GoldLoadTimestamp"
)

# Write to Gold Layer Dim_Patient table
final_dim_patient_df.write.format("delta").mode("overwrite").saveAsTable(dim_patient_table) # Use "overwrite" for this lab; "merge" for SCD Type 2 updates
print(f"Successfully created/updated {dim_patient_table}")
final_dim_patient_df.show(truncate=False)
```

---

**3. Create `Fact_Encounter` (Fact Table - Gold Layer):**

*   **Tool:** Spark Notebook.
*   **Notebook Name:** `Create_Fact_Encounter_Gold_YourName`
*   **Logic (PySpark Code):**

    *(This assumes `Dim_Patient` has been created. For `Dim_Date` and `Dim_Provider`, we will create simplified placeholder versions or skip them for this lab's core focus on `Fact_Encounter` creation. In a real scenario, these would be properly built dimensions.)*

```python
from pyspark.sql.functions import col, lit, current_timestamp, expr, to_date, datediff, year, month, dayofmonth, coalesce, when, explode
import datetime

lakehouse_name = "HealthDataLH_YourName" # Replace YourName
silver_unified_encounters_table = f"{lakehouse_name}.Silver_Unified_Encounters"
dim_patient_table = f"{lakehouse_name}.Dim_Patient"
fact_encounter_table = f"{lakehouse_name}.Fact_Encounter" # Gold Layer Table

# --- (Optional) Create a simple Dim_Date if not existing for lookup ---
# This is a very basic Dim_Date for joining. A full Dim_Date is more comprehensive.
dim_date_table = f"{lakehouse_name}.Dim_Date"
try:
    spark.read.table(dim_date_table).limit(1).collect()
    print(f"{dim_date_table} already exists.")
except:
    print(f"Creating simplified {dim_date_table}...")
    date_range_df = spark.sql("SELECT sequence(to_date('2020-01-01'), to_date('2030-12-31'), interval 1 day) as date_array")
    dates_df = date_range_df.select(explode(col("date_array")).alias("FullDate"))
    simple_dim_date_df = dates_df.select(
        expr("replace(cast(FullDate as string), '-', '')").cast("int").alias("DateKey"), # Format YYYYMMDD
        col("FullDate").cast("date"),
        year(col("FullDate")).alias("Year"),
        month(col("FullDate")).alias("Month"),
        dayofmonth(col("FullDate")).alias("Day")
    )
    simple_dim_date_df.write.format("delta").mode("overwrite").saveAsTable(dim_date_table)
    print(f"Simplified {dim_date_table} created.")

# Read source tables
try:
    encounters_df = spark.read.table(silver_unified_encounters_table)
    patients_dim_df = spark.read.table(dim_patient_table).select("PatientKey", "PatientNaturalKey") # Only need keys for join
    date_dim_df = spark.read.table(dim_date_table).select("DateKey", "FullDate")
except Exception as e:
    print(f"Error reading source tables for Fact_Encounter: {e}")
    dbutils.notebook.exit("Failed to read source tables.")

# Join Encounters with Dim_Patient to get PatientKey
# Assuming encounters_df.PatientIdentifier corresponds to patients_dim_df.PatientNaturalKey (e.g., MRN)
fact_df_intermediate = encounters_df.join(
    patients_dim_df,
    encounters_df.PatientIdentifier == patients_dim_df.PatientNaturalKey,
    "left_outer"
).select(
    encounters_df["*"], # Select all columns from encounters_df
    patients_dim_df["PatientKey"]
)

# Join with Dim_Date for AdmissionDateKey
fact_df_intermediate = fact_df_intermediate.join(
    date_dim_df.alias("dim_admission_date"),
    to_date(fact_df_intermediate.AdmissionDateTime) == col("dim_admission_date.FullDate"),
    "left_outer"
).select(
    fact_df_intermediate["*"],
    col("dim_admission_date.DateKey").alias("AdmissionDateKey")
)

# Join with Dim_Date for DischargeDateKey
fact_df_intermediate = fact_df_intermediate.join(
    date_dim_df.alias("dim_discharge_date"),
    to_date(fact_df_intermediate.DischargeDateTime) == col("dim_discharge_date.FullDate"),
    "left_outer"
).select(
    fact_df_intermediate["*"],
    col("dim_discharge_date.DateKey").alias("DischargeDateKey")
)

# --- Select Measures and Foreign Keys for Fact Table ---
fact_encounter_df = fact_df_intermediate.withColumn(
    "LengthOfStayInDays",
    when(col("DischargeDateTime").isNotNull() & col("AdmissionDateTime").isNotNull(),
         datediff(to_date(col("DischargeDateTime")), to_date(col("AdmissionDateTime")))
    ).otherwise(None).cast("int")
).select(
    col("EncounterSK").alias("EncounterKey"), # Surrogate key from Silver layer or newly generated
    col("PatientKey"), # Foreign Key from Dim_Patient
    col("AdmissionDateKey"), # Foreign Key from Dim_Date
    col("DischargeDateKey"), # Foreign Key from Dim_Date
    # Add ProviderKey if Dim_Provider exists and is joined
    # col("ProviderKey"),
    col("SourceSystemEncounterID").alias("EncounterNaturalKey"), # Natural key from source
    col("EncounterType"),
    col("PrimaryDiagnosisCode"),
    col("AttendingProviderIdentifier"), # Could be a degenerate dimension or FK to Dim_Provider
    col("LengthOfStayInDays"),
    # Add other measures like TotalCharges if available
    # lit(1).alias("EncounterCount"), # Useful measure
    col("SourceSystem"),
    current_timestamp().alias("GoldLoadTimestamp")
).filter(col("PatientKey").isNotNull()) # Only load encounters that have a matching patient in Dim_Patient

# Write to Gold Layer Fact_Encounter table
fact_encounter_df.write.format("delta").mode("overwrite").saveAsTable(fact_encounter_table)
print(f"Successfully created/updated {fact_encounter_table}")
fact_encounter_df.show(truncate=False)
```

---

**4. (Optional) Create `Patient_Visit_Analytics_View` (Gold Layer View):**

*   **Tool:** SQL Endpoint of the `HealthDataLH_YourName` Lakehouse or a Fabric Warehouse.
*   **Logic (SQL Code):**

    Open a new SQL query window connected to your Lakehouse SQL endpoint.

    ```sql
    -- Ensure you are in the context of your Lakehouse, e.g., USE HealthDataLH_YourName;
    -- Or fully qualify table names if needed: HealthDataLH_YourName.dbo.Fact_Encounter

    CREATE OR ALTER VIEW Patient_Visit_Analytics_View AS
    SELECT
        p.PatientNaturalKey AS PatientMRN,
        p.FullName AS PatientFullName,
        p.DateOfBirth AS PatientDateOfBirth,
        p.Age AS PatientAge,
        p.Gender AS PatientGender,
        fe.EncounterNaturalKey,
        fe.EncounterType,
        fe.PrimaryDiagnosisCode,
        adm_dt.FullDate AS AdmissionDate,
        dis_dt.FullDate AS DischargeDate,
        fe.LengthOfStayInDays,
        fe.AttendingProviderIdentifier,
        fe.SourceSystem AS EncounterSourceSystem
        -- Add more fields from dimensions (Dim_Provider, Dim_Diagnosis) as needed
    FROM
        HealthDataLH_YourName.dbo.Fact_Encounter fe
    JOIN
        HealthDataLH_YourName.dbo.Dim_Patient p ON fe.PatientKey = p.PatientKey
    LEFT JOIN
        HealthDataLH_YourName.dbo.Dim_Date adm_dt ON fe.AdmissionDateKey = adm_dt.DateKey
    LEFT JOIN
        HealthDataLH_YourName.dbo.Dim_Date dis_dt ON fe.DischargeDateKey = dis_dt.DateKey
    -- LEFT JOIN
    --     HealthDataLH_YourName.dbo.Dim_Provider dp ON fe.ProviderKey = dp.ProviderKey -- If Dim_Provider exists
    ;

    -- Test the view
    SELECT * FROM Patient_Visit_Analytics_View LIMIT 10;
    ```

---

**Discussion Questions:**

1.  **In `Dim_Patient`, why is it generally better to use a system-generated `PatientKey` (surrogate key) as the primary key instead of using the `PatientIdentifier` (natural key from the source system) directly?**

    *   **Stability:** Natural keys (like MRN) can sometimes change, be reassigned, or have errors in the source system. Surrogate keys are controlled within the data warehouse and remain stable, protecting the fact tables from these source system changes.
    *   **Performance:** Surrogate keys are typically integers (or a fixed-length hash like in our example), which are more efficient for joins in database systems compared to potentially long or composite string-based natural keys.
    *   **Decoupling:** Using surrogate keys decouples the data warehouse model from the operational source system's keying structure. This allows the source system to evolve its keys without breaking the warehouse.
    *   **Handling Slowly Changing Dimensions (SCDs):** Surrogate keys are essential for implementing SCD Type 2, where you need to track historical changes to dimension attributes. A new surrogate key is assigned for each version of a patient's record.
    *   **Integration of Multiple Sources:** If patients come from multiple source systems with different natural key formats or potential overlaps, a surrogate key provides a single, unambiguous way to identify a unique patient entity in the warehouse.

2.  **How can Microsoft Purview (discussed in later sections) help in understanding the lineage of data as it moves from `Bronze_HL7_Encounters_Raw` to `Silver_Unified_Encounters` and finally into `Fact_Encounter`?**

    *   **Automated Lineage Tracking:** Microsoft Purview can scan Fabric items (Lakehouses, Notebooks, Data Factory pipelines) and automatically capture data lineage. It visualizes how data flows from source tables (e.g., `Bronze_HL7_Encounters_Raw`) through transformation processes (e.g., the Spark Notebooks that create `Silver_Unified_Encounters` and `Fact_Encounter`) to the final analytical tables and even into Power BI reports.
    *   **Impact Analysis:** If there's a change proposed to `Bronze_HL7_Encounters_Raw` or a transformation rule, lineage helps identify all downstream tables, reports, and processes (like `Silver_Unified_Encounters`, `Fact_Encounter`) that will be affected.
    *   **Troubleshooting & Root Cause Analysis:** If data quality issues are found in `Fact_Encounter`, lineage allows data stewards or engineers to trace back to the origin of the data and the transformations applied at each step, helping to pinpoint where the issue was introduced.
    *   **Compliance and Governance:** For regulations like HIPAA, demonstrating data provenance and how sensitive data elements are processed and transformed is critical. Purview's lineage provides this auditable trail.
    *   **Data Discovery:** Business users or analysts can use lineage to understand where the data in their reports or analytical models originates, increasing trust and understanding of the data.

3.  **If a new field, like `DischargeDisposition`, needs to be added to the `Silver_Unified_Encounters` table later, how does using Delta Lake format for your Lakehouse tables make this process easier?**

    *   **Schema Evolution:** Delta Lake supports schema evolution, which means you can add new columns (like `DischargeDisposition`) to an existing Delta table without rewriting the entire table or breaking existing queries that don't use the new column.
        *   The Spark Notebook or Dataflow Gen2 performing the transformation to `Silver_Unified_Encounters` can be updated to include the new `DischargeDisposition` field.
        *   When this updated job runs and writes to the Delta table, the new column will be added to the table's schema automatically (if `mergeSchema` option is used or if the write mode supports it).
    *   **Schema Enforcement:** While allowing evolution, Delta Lake also offers schema enforcement. This prevents accidental schema changes due to bad data, but for intentional additions like `DischargeDisposition`, schema evolution handles it gracefully.
    *   **No Downtime for Readers:** Existing queries and downstream processes that read from `Silver_Unified_Encounters` but do not yet reference the new `DischargeDisposition` column will continue to work without modification. They will simply not see the new column until they are updated to use it.
    *   **Data Backfill:** After adding the column, historical records will have null values for `DischargeDisposition`. You can then run a separate job to backfill this new column for existing records if the data is available.

---

### 4.9 Quiz
**1. What is the Silver layer used for?**
a) Raw ingestion
b) AI predictions
c) Cleaned, conformed data
**â†’ Answer: c**

**2. What tool is ideal for low-code data transformations in Fabric?**
a) SQL Warehouse
b) Dataflows Gen2
c) Notebooks
**â†’ Answer: b**

**3. What is a surrogate key?**
a) Original patient MRN
b) A business term
c) A generated, non-PHI identifier
**â†’ Answer: c**

---

### 4.10 Cheat Sheet (Printable)
*   **Bronze** = Raw (HL7, FHIR JSON)
*   **Silver** = FHIR-aligned, clean tables
*   **Gold** = Analytics-ready dimensional model
*   **Dim_Patient** = Patient metadata
*   **Fact_Encounter** = Visit-level records
*   **Dataflows Gen2** = Power Query visual tool
*   **Notebooks** = Spark/Python transformation logic
*   **Delta Format** = Schema evolution + audit + rollback
*   **Use Purview** = Lineage + classification

---

### Interactive Element for Section 4
[Scenario-Based Interactive Questions](./visualizations-html/Section%204%20-%20Data%20Modeling%20and%20Transformation.html)

---

## ðŸ“˜ Section 5: Security, Compliance, and Governance {#section-5-security-compliance-and-governance}

---

### 5.0 Overview
Healthcare organizations face immense pressure to maintain the **confidentiality**, **integrity**, and **availability** of Protected Health Information (PHI). With the introduction of Microsoft Fabric, these responsibilities must now be applied in a cloud-native, highly-integrated analytics environment.

This section explores how to **build and enforce a secure data governance strategy** within Fabric, ensuring full alignment with **HIPAA**, **HITRUST**, and best practices for modern healthcare data platforms. Youâ€™ll learn to implement role-based access controls (RBAC), encryption, audit logging, and governance classification using **Microsoft Purview**, all within a structured data lifecycle model.

---

### 5.1 Regulatory Frameworks in Focus
<figure>
  <img src="./images/5.1%20Regulatory%20Frameworks%20in%20Focus.png" alt="Security, Compliance & Governance Diagram" style="width:100%;" />
  <figcaption><em>Figure: Security, Compliance & Governance Diagram.</em></figcaption>
</figure>

#### HIPAA (Health Insurance Portability and Accountability Act)
*   Applies to **Covered Entities** (hospitals, clinics, etc.) and **Business Associates** (vendors handling PHI)
*   Key provisions:
    *   **Privacy Rule**: Controls access and sharing
    *   **Security Rule**: Requires technical safeguards
    *   **Breach Notification Rule**: Mandates disclosure of data breaches

#### HITRUST CSF (Common Security Framework)
*   Certifiable framework that combines:
    *   HIPAA
    *   NIST
    *   ISO/IEC 27001
*   Often required by large providers or payers
*   Strong on **access controls**, **audit**, and **data integrity**

---

### 5.2 Fabricâ€™s Built-In Security Architecture
#### Core Security Features
| Feature | Description |
| :--- | :--- |
| **RBAC** | Role-Based Access Control for workspaces, items, pipelines |
| **Encryption** | At-rest and in-transit via Microsoft-managed or customer-managed keys (CMK) |
| **Audit Logs** | Logs of all access, modification, sharing |
| **Purview Integration** | Classify, label, and monitor data usage |
| **Sensitivity Labels** | Tag data with HIPAA or confidential classification |
| **Data Loss Prevention (DLP)** | Prevent export/sharing of sensitive data |
| **Conditional Access** | Restrict access by device, IP, compliance state |

---

### 5.3 Implementing Role-Based Access Control (RBAC)
Fabric enforces RBAC at multiple levels:

| Level | Resource | Roles |
| :--- | :--- | :--- |
| Tenant | Entire Fabric tenant | Global Admin, Fabric Admin |
| Workspace | Data projects | Admin, Member, Contributor, Viewer |
| Item | Individual datasets, notebooks | Owner, Viewer, Contributor |

#### Best Practices:
*   **Least privilege**: Grant only needed access
*   Use **security groups** in Entra ID to manage roles
*   Review permissions quarterly

---

### 5.4 Encryption and Key Management
#### Encryption in Fabric
| Type | Scope | Key |
| :--- | :--- | :--- |
| In-Transit | All data moving between services | TLS 1.2+ |
| At-Rest | Stored in OneLake and Warehouse | AES-256 |
| CMK | Customer-managed | Azure Key Vault |

#### HIPAA Considerations
*   Encrypt all PHI at rest and in transit
*   Rotate keys on a scheduled basis
*   Log all key access and changes

---

### 5.5 Auditing and Activity Monitoring
Audit logs are essential for HIPAA compliance and forensic review.

#### Using Microsoft Purview Audit
*   Tracks:
    *   Who accessed what
    *   When and from where
    *   Actions taken (view, download, delete)
*   Enables:
    *   **Access investigation**
    *   **Breach forensics**
    *   **Usage pattern tracking**

#### Integration with SIEM (e.g., Sentinel)
*   Export logs to Azure Monitor or Microsoft Sentinel
*   Correlate with other events (e.g., login anomalies)

---

### 5.6 Sensitivity Labels and Data Classification
Microsoft Purview allows you to classify and tag data.

#### Steps to Enable:
1.  Enable **Microsoft Purview Governance Portal**
2.  Set up **data classification rules**:
    *   PHI: Names, DOB, MRNs, SSN
    *   PII: Email, phone, address
3.  Apply **sensitivity labels**:
    *   HIPAA-HIGH, Confidential-HITRUST
4.  Set **auto-labeling policies** for:
    *   Excel files
    *   CSV uploads
    *   JSON APIs

#### Fabric Application
*   Labels are inherited across:
    *   Lakehouses
    *   Reports
    *   Pipelines
*   Use **DLP policies** to:
    *   Block export of labeled data
    *   Alert on downloads or external sharing

---

### 5.7 Conditional Access and DLP
Using **Microsoft Entra ID (Azure AD)**:

*   Define policies:
    *   Block access from unmanaged devices
    *   Require MFA for privileged access
    *   Restrict export of confidential data

#### DLP Actions
| Action | Scenario |
| :--- | :--- |
| Block Download | Report contains `HIPAA-HIGH` data |
| Alert | Export of data from PHI-labeled table |
| Restrict Sharing | Only allow internal users |

---

### 5.8 Data Governance with Microsoft Purview
#### Core Functions
| Function | Purpose |
| :--- | :--- |
| Data Catalog | Discover and tag datasets |
| Classification | Tag PHI/PII/Sensitive |
| Lineage | Track data from source to report |
| Access Policies | Control data actions |

#### Compliance Alignment
*   Lineage enables **audit trail for HIPAAâ€™s Access & Integrity controls**
*   Classification supports **minimum necessary principle**
*   Policies help prevent **inadvertent disclosure**

---

### 5.9 Lab: Configure Governance for a New Dataset with PHI
**Module Alignment:** Section 5: Security, Compliance, and Governance

**Objective:**
*   Understand and apply sensitivity labels to datasets containing Protected Health Information (PHI) within Microsoft Fabric.
*   (Conceptual) Understand how Data Loss Prevention (DLP) rules can be associated with sensitivity labels to protect data.
*   Explore how to enable and review audit tracking for activities on sensitive data.
*   (Conceptual) Understand how Microsoft Purview can be used to visualize data lineage.
*   Configure basic role-based access control (RBAC) on a Fabric item (e.g., a Lakehouse table).

**Scenario:**
Valley General Hospital has just ingested a new dataset containing patient appointment details and sensitive visit notes into the `CardiologyLakehouse`. This data is highly confidential and subject to HIPAA regulations. As a data engineer with a focus on governance, your task is to apply appropriate security and compliance measures to this dataset.

**Prerequisites:**
*   Microsoft Fabric Workspace (e.g., `DEV_CardiologyAnalytics`).
*   Microsoft Fabric Lakehouse (e.g., `CardiologyLakehouse`).
*   A sample table in the Lakehouse representing the new sensitive dataset. We will create one.
*   Permissions to manage sensitivity labels in your Microsoft 365 compliance center (this might require Global Admin or Compliance Admin roles for initial setup of labels. For applying existing labels in Fabric, appropriate workspace permissions are needed).
*   Permissions to manage access to items within your Fabric workspace.
*   (Optional but Recommended) Familiarity with the Microsoft Purview compliance portal.

**Tools to be Used:**
*   Microsoft Fabric Lakehouse
*   Microsoft Fabric Workspace settings
*   (Conceptual/UI Exploration) Microsoft Purview compliance portal (for understanding where labels and DLP are configured)
*   Fabric Notebook (for creating sample data)

**Estimated Time:** 60 minutes (Actual application of labels and DLP might depend on M365 admin configurations. This lab will focus on the Fabric side and conceptual understanding of Purview integration.)

**Tasks / Step-by-Step Instructions:**

**Part 1: Create Sample Sensitive Dataset**

1.  **Open or Create a Notebook:**
    *   In your `DEV_CardiologyAnalytics` workspace, open an existing notebook or create a new one (e.g., `SensitiveData_Governance_Setup`).
    *   Ensure your `CardiologyLakehouse` is attached.

2.  **Generate Sample Data and Table:**
    *   In a PySpark cell, create a sample DataFrame and save it as a table in your Lakehouse. This table will represent the new sensitive dataset.
    ```python
    from pyspark.sql import Row
    from pyspark.sql.functions import col, lit
    from pyspark.sql.types import StructType, StructField, StringType, DateType, TimestampType

    # Define schema for the sensitive appointments data
    schema = StructType([
        StructField("AppointmentID", StringType(), False),
        StructField("PatientID", StringType(), False),
        StructField("AppointmentDate", DateType(), True),
        StructField("ProviderName", StringType(), True),
        StructField("VisitNotes_PHI", StringType(), True), # This column contains sensitive PHI
        StructField("LastUpdated", TimestampType(), True)
    ])

    # Sample data including PHI in VisitNotes_PHI
    data = [
        Row(AppointmentID="APP001", PatientID="P001", AppointmentDate="2023-06-01", ProviderName="Dr. Smith", VisitNotes_PHI="Patient reports chest pain. ECG ordered. Discussed family history of heart disease.", LastUpdated="2023-06-01T10:30:00"),
        Row(AppointmentID="APP002", PatientID="P002", AppointmentDate="2023-06-01", ProviderName="Dr. Jones", VisitNotes_PHI="Follow-up for hypertension. BP 140/90. Medication adjusted.", LastUpdated="2023-06-01T11:15:00"),
        Row(AppointmentID="APP003", PatientID="P003", AppointmentDate="2023-06-02", ProviderName="Dr. Smith", VisitNotes_PHI="Routine check-up. Patient feels well. No new complaints. Advised on diet and exercise.", LastUpdated="2023-06-02T09:00:00")
    ]

    df_sensitive_appointments = spark.createDataFrame(data, schema)

    df_sensitive_appointments.printSchema()
    display(df_sensitive_appointments)

    # Write to Gold table in Lakehouse (assuming it's curated sensitive data)
    table_name = "CardiologyLakehouse.gold_sensitive_appointments"
    df_sensitive_appointments.write.format("delta").mode("overwrite").saveAsTable(table_name)
    print(f"Table '{table_name}' created successfully with sensitive PHI.")
    ```
    *   Run the cell to create the `gold_sensitive_appointments` table.

**Part 2: Apply Sensitivity Labels (in Microsoft Fabric)**

*   **Note:** The availability and names of sensitivity labels are configured by an administrator in the Microsoft Purview compliance portal. For this lab, we'll assume a label like "Confidential - PHI" or "HIPAA-HIGH" exists. If not, you might only be able to see default labels or none.

1.  **Navigate to the Lakehouse Table:**
    *   Go to your `CardiologyLakehouse`.
    *   Under the `Tables` section, find the `gold_sensitive_appointments` table.
2.  **Set Sensitivity Label:**
    *   Click the three dots (**...**) next to the `gold_sensitive_appointments` table.
    *   Select **Settings**.
    *   In the settings pane that opens on the right, look for a section related to **Sensitivity label** (the exact UI might vary slightly).
    *   Click **Edit** or the current label (if one is set, e.g., "None").
    *   A dialog will appear listing available sensitivity labels. Choose an appropriate label that signifies highly confidential PHI (e.g., "Confidential - PHI", "Highly Confidential", or a custom "HIPAA-HIGH" if configured).
    *   Click **Save** or **Apply**.
    *   The table in the Lakehouse explorer might now show an icon or text indicating the applied sensitivity label.

    *   **Alternative (if labeling at dataset level for Power BI):**
        *   If you create a Power BI dataset from this table, you can also set sensitivity labels at the dataset level within the Power BI service settings for that dataset. Fabric aims to inherit these.

**Part 3: Understand Data Loss Prevention (DLP) - Conceptual**

*   DLP policies are configured in the **Microsoft Purview compliance portal** by compliance administrators, not directly within the Fabric workspace by a typical data engineer.
*   These policies are linked to sensitivity labels. For example, a DLP policy could state: "If a file or dataset is labeled 'Confidential - PHI', then block download to unmanaged devices, block sharing with external users, and audit the activity."

1.  **How it Works (Conceptual):**
    *   When you applied the "Confidential - PHI" label to your `gold_sensitive_appointments` table (or a Power BI dataset built on it):
        *   Fabric (and other M365 services) become aware of this classification.
        *   If a DLP policy exists for this label, it will automatically be enforced.
        *   For example, if a user tries to export data from a Power BI report built on this labeled dataset to an Excel file on a personal device, the DLP policy might block the action or generate an alert for administrators.

2.  **Where to Configure (for Admins):**
    *   Microsoft Purview compliance portal (`compliance.microsoft.com`).
    *   Navigate to "Data loss prevention" -> "Policies".
    *   Create a policy, define conditions (e.g., content contains sensitivity label "Confidential - PHI"), and define actions (e.g., restrict access, block activities, send notifications).

**Part 4: Enable and Review Audit Tracking**

*   Auditing is generally enabled by default for most activities in Microsoft Fabric and Microsoft 365. Audit logs capture user and admin activities.

1.  **Where to Review Audit Logs (Typically for Admins/Compliance Officers):**
    *   **Microsoft Purview compliance portal:**
        *   Navigate to `compliance.microsoft.com`.
        *   Go to the **Audit** section.
        *   You can search the audit log for specific activities, users, date ranges, and workloads (e.g., "Power BI", "Microsoft Fabric").
        *   Activities related to accessing or modifying your `gold_sensitive_appointments` table (e.g., viewing it in a notebook, querying it via SQL, changing its label) would be logged here.
    *   **Fabric Monitoring Hub:**
        *   Within Fabric, the Monitoring Hub provides insights into pipeline runs, Spark applications, and other activities. While not the primary audit log for compliance, it's useful for operational monitoring.

2.  **What to Look For:**
    *   Access to the `gold_sensitive_appointments` table (who read it, when).
    *   Changes to the sensitivity label.
    *   Attempts to share or export data from reports built on this table.
    *   Failed access attempts.

**Part 5: Understand Data Lineage with Microsoft Purview - Conceptual**

*   Microsoft Purview automatically scans your Fabric workspaces (if configured by an admin) and maps the relationships between data assets.

1.  **How it Works (Conceptual):**
    *   If Purview has scanned your `CardiologyLakehouse`:
        *   It would identify the `gold_sensitive_appointments` table.
        *   If this table was created by a Notebook (as in Part 1), Purview would show this relationship (lineage from the notebook job to the table).
        *   If a Power BI dataset and report are then created on top of `gold_sensitive_appointments`, Purview would extend the lineage to show `Notebook -> gold_sensitive_appointments_table -> Power_BI_Dataset -> Power_BI_Report`.
2.  **Viewing Lineage (in Purview Data Catalog):**
    *   Users with access to Purview can search for the `gold_sensitive_appointments` asset.
    *   The asset details page in Purview would have a "Lineage" tab showing an interactive graph of its upstream sources and downstream consumers.
    *   This is crucial for understanding data flow, impact analysis, and compliance reporting.

**Part 6: Configure Role-Based Access Control (RBAC) on the Table/Lakehouse**

*   While workspace roles (Admin, Member, Contributor, Viewer) provide broad access, you might want more granular control, especially for highly sensitive data. Fabric allows sharing individual items with specific permissions. SQL-level permissions can also be set on tables via the SQL Analytics Endpoint.

1.  **Sharing a Specific Lakehouse Table (Item-Level Sharing):**
    *   Navigate to your `CardiologyLakehouse`.
    *   Find the `gold_sensitive_appointments` table.
    *   Click the three dots (**...**) next to the table name.
    *   Select **Share**.
    *   In the "Share" dialog:
        *   Enter the name or email of a user or group (e.g., `Cardiology_Analysts_Group`).
        *   Choose the permission level:
            *   **Read:** Allows querying the table (e.g., via SQL endpoint or Notebook).
            *   **ReadData:** (SQL Permissions) Allows `SELECT` on the table data.
            *   **ReadWriteData:** (SQL Permissions) Allows `SELECT`, `INSERT`, `UPDATE`, `DELETE`.
            *   **Build:** (For Power BI datasets) Allows users to build reports on top of this data.
        *   You can also specify if they can re-share.
        *   Add a message (optional).
        *   Click **Grant access**.
    *   This provides more granular access to this specific table beyond the general workspace role.

2.  **SQL Permissions via SQL Analytics Endpoint (More Advanced):**
    *   Open the SQL Analytics Endpoint for your `CardiologyLakehouse`.
    *   You can use T-SQL `GRANT`, `DENY`, `REVOKE` statements to manage permissions on tables for specific users or roles (AAD groups).
        ```sql
        -- Example (run in SQL Analytics Endpoint, not PySpark notebook):
        -- GRANT SELECT ON CardiologyLakehouse.gold_sensitive_appointments TO [user_or_group_email@domain.com];
        -- GRANT SELECT ON CardiologyLakehouse.gold_sensitive_appointments TO [AAD_Group_Name];
        ```
    *   This is powerful for fine-grained control, especially for users accessing data via SQL tools.

**Expected Outcome / Deliverables:**
*   The `gold_sensitive_appointments` table in `CardiologyLakehouse` has an appropriate sensitivity label applied.
*   A conceptual understanding of how DLP policies (configured in Purview) would interact with this label.
*   Knowledge of where to look for audit logs related to activities on sensitive data.
*   A conceptual understanding of how Purview provides data lineage.
*   Experience with sharing a specific Fabric item (the table) with granular permissions, demonstrating the principle of least privilege.

**Questions from Manual & Answers LINK TO HTML?:**

*   **Q1: Why is it important to apply the "principle of least privilege" when granting viewer roles or any access to sensitive datasets like PHI?**
    *   **A1:** The principle of least privilege dictates that users should only be granted the minimum levels of access â€“ or permissions â€“ necessary to perform their job duties. For sensitive datasets like PHI:
        *   **Reduces Risk of Unauthorized Disclosure:** Limiting access minimizes the attack surface. If an account is compromised, the potential for PHI exposure is lessened if that account only had viewer access to a small subset of data.
        *   **Minimizes Accidental Data Modification/Deletion:** Viewer roles prevent users from unintentionally altering or deleting critical PHI.
        *   **Compliance with Regulations:** HIPAA and other privacy regulations mandate strict access controls. Least privilege is a core tenet of demonstrating due diligence in protecting sensitive information.
        *   **Prevents Data Misuse:** Ensures that users cannot access or use PHI for purposes outside their legitimate job functions.
        *   **Simplifies Auditing:** When access is tightly controlled, auditing user activity and investigating incidents becomes more straightforward.

*   **Q2: How does Microsoft Purview support investigations in case of a potential data breach involving the `gold_sensitive_appointments` dataset?**
    *   **A2:** Microsoft Purview provides several capabilities crucial for breach investigations:
        *   **Audit Logs:** Purview's unified audit log captures detailed records of activities across Microsoft 365 services, including Fabric. Investigators can search for who accessed the `gold_sensitive_appointments` dataset, when they accessed it, what actions they performed (e.g., read, export, label change), their IP address, etc. This helps pinpoint the scope and timeline of a breach.
        *   **Data Lineage:** The lineage graph in Purview can show where the `gold_sensitive_appointments` data originated and where it flowed (e.g., to which Power BI reports or other downstream processes). This helps understand the potential blast radius of a breach â€“ what other assets might be compromised if this dataset was breached.
        *   **Sensitivity Labels & Data Classification:** Knowing the dataset was labeled as containing PHI helps prioritize the investigation. Purview can also show other assets with the same label that might be at risk or related.
        *   **Activity Alerts:** If alerts were configured (e.g., for unusual access patterns or attempts to download large volumes of PHI-labeled data), these alerts would provide early indicators and valuable forensic information.
        *   **Data Discovery:** Purview's data map can help identify all locations where similar sensitive data (or copies of the breached data) might exist within the organization's data estate.

*   **Q3: What typically triggers a Data Loss Prevention (DLP) policy to block an action like downloading a report containing data from `gold_sensitive_appointments`?**
    *   **A3:** A DLP policy block is typically triggered by a combination of conditions defined within the policy. Common triggers include:
        1.  **Sensitivity Label Detection:** The primary trigger is often the presence of a specific sensitivity label (e.g., "Confidential - PHI") on the content (the report or its underlying dataset, `gold_sensitive_appointments`).
        2.  **Content Inspection (Keywords/Patterns):** DLP policies can also be configured to inspect content for specific keywords (e.g., "patient record," "diagnosis"), regular expressions (e.g., patterns matching Social Security Numbers or MRNs), or built-in sensitive information types. If the report content matches these patterns, the policy can be triggered even without an explicit label, though label-based is more common for structured data.
        3.  **Context of the Action:**
            *   **Destination:** Attempting to download to an unmanaged device (personal laptop), a non-trusted IP address range, or a USB drive.
            *   **Recipient:** Attempting to share the report or data with external users (outside the organization) or specific unauthorized internal groups.
            *   **Application:** Attempting to copy data to an unsanctioned application (e.g., personal cloud storage).
        4.  **User Attributes:** Policies can sometimes consider the user's role, department, or group membership.
        5.  **Quantity of Sensitive Data:** Some policies might trigger if a large volume of sensitive data is involved in the action.

        When these conditions are met, the DLP policy's defined action (e.g., block the download, encrypt the content, notify an administrator, require justification) is enforced.

---

### 5.10 Quiz
**1. What does HIPAA require for stored data?**
a) Compression
b) Encryption
c) Zip format
**â†’ Answer: b**

**2. What tool in Microsoft Fabric supports audit and classification?**
a) Power BI
b) Purview
c) SQL Warehouse
**â†’ Answer: b**

**3. Which control prevents data downloads?**
a) Notebooks
b) DLP
c) RBAC
**â†’ Answer: b**

---

### 5.11 Cheat Sheet (Printable)
*   **HIPAA** = Privacy + Security + Breach Notification
*   **HITRUST** = Certifiable control framework
*   **RBAC** = Control access by role and project
*   **Encryption** = TLS in-transit, AES-256 at-rest
*   **Purview** = Classify, label, and audit PHI
*   **Sensitivity Labels** = HIPAA-HIGH, HITRUST, Confidential
*   **DLP** = Prevent export or misuse of sensitive data
*   **Audit Logs** = Track every action on data
*   **Lineage Diagrams** = Visualize movement of PHI

---

### Interactive Element for Section 5
[Case Study: Secure Data Workflow](./visualizations-html/Section%205%20-%20Security,%20Compliance,%20and%20Governance.html)

---

## ðŸ“˜ Section 6: Analytics and Reporting {#section-6-analytics-and-reporting}

---

### 6.0 Overview
Transforming raw data into actionable intelligence is one of the most vital outcomes of any healthcare data platform. Microsoft Fabric streamlines this process by offering **native integration with Power BI**, enabling secure, compliant, real-time visualization and reporting directly from Lakehouse and Warehouse models using the **DirectLake** connection.

This section equips data engineers with the knowledge to build high-performance analytics pipelines, design healthcare-specific dashboards, and implement **role-based data access** to meet regulatory and operational requirements.

---

### 6.1 Analytics in the Healthcare Context
<figure>
  <img src="./images/6.1-Analytics%20&%20Reporting%20Diagram.png" alt="Analytics & Reporting Diagram" style="width:100%;" />
  <figcaption><em>Figure: Analytics & Reporting Diagram.</em></figcaption>
</figure>

Healthcare analytics can be broadly categorized into:

| Category | Examples |
| :--- | :--- |
| **Clinical** | Patient readmission risk, Disease prevalence, ICU alerts |
| **Operational** | Bed utilization, Appointment no-shows, Staff productivity |
| **Financial** | Denial rates, Billing lags, Reimbursement analytics |
| **Public Health** | Vaccination tracking, Infection surveillance |

Power BI in Fabric supports real-time dashboards, KPI trend reports, and interactive drilldowns.

---

### 6.2 DirectLake vs Import vs DirectQuery
#### Power BI Connection Modes:
| Mode | Performance | Use Case |
| :--- | :--- | :--- |
| Import | Fastest, but data is duplicated | Stable, infrequent refresh |
| DirectQuery | Live query against Warehouse | Real-time needs, higher latency |
| **DirectLake** | Fabric-only: live query on OneLake | Best performance + no duplication |

> ðŸ’¡ Use **DirectLake** for patient-level reports with large datasets from Lakehouse tables.

---

### 6.3 Connecting Power BI to Fabric
#### Steps to Visualize Lakehouse Data:
1.  In Fabric Workspace â†’ Select Lakehouse
2.  Click â€œNew Power BI Datasetâ€
3.  Choose the table (`Gold_Patient_360`, `Fact_Visit`)
4.  Power BI creates dataset â†’ Click â€œCreate Reportâ€
5.  Build visualizations:
    *   Bar: Readmissions by Hospital
    *   Line: Avg LOS over time
    *   Matrix: Patient counts by provider + condition

#### Notes:
*   Data remains in **OneLake**â€”no duplication
*   Changes in Lakehouse are reflected live

---

### 6.4 Common Healthcare Dashboards
#### Example Dashboards:
| Name | Key Visuals | Audience |
| :--- | :--- | :--- |
| **Clinical Quality** | HEDIS scores, Mortality rates | CMOs, Care Managers |
| **Operations** | Daily census, OR delays | Admin, Ops Leaders |
| **Financial** | Payer mix, Reimbursement lag | CFO, Billing |
| **Patient Engagement** | Portal logins, Message response | Marketing, Patient Services |

Each dashboard should follow **visual best practices**:

*   Use muted colors for clinical severity
*   Sort by most recent month
*   Display **counts**, **rates**, and **trends**
*   Ensure accessible color contrast

---

### 6.5 Implementing Row-Level Security (RLS)
In healthcare, **data segmentation is mandatory** for compliance and privacy.

#### Use Case Examples:
| User Role | View Restrictions |
| :--- | :--- |
| Clinic Director | Only their location |
| Provider | Only their patients |
| Analyst | Aggregated, de-identified |

#### Steps to Implement RLS:
1.  Create a **security table**:
    ```
    SecurityMatrix(UserEmail, Department, AccessLevel)
    ```
2.  Define roles in Power BI â†’ Manage Roles
3.  Use DAX filters:
    ```
    [Department] = LOOKUPVALUE(SecurityMatrix[Department], SecurityMatrix[UserEmail], USERPRINCIPALNAME())
    ```

---

### 6.6 Embedding and Publishing Dashboards
Dashboards can be:

*   Shared via Power BI Service
*   Embedded in EHR portals (e.g., Epic Hyperspace via iframe)
*   Scheduled as PDF/email snapshots
*   Restricted with Sensitivity Labels and DLP

#### Governance Reminders:
*   Apply **sensitivity labels** to all datasets
*   Enable **export restrictions**
*   Track report access via **Purview activity logs**

---

### 6.7 Performance Tuning Tips
To optimize Power BI performance:

| Strategy | Description |
| :--- | :--- |
| Use Measures | Replace calculated columns |
| Avoid Bi-Directional Filters | Simplify relationships |
| Limit Visuals per Page | Reduce render time |
| Index Fact Tables | Pre-aggregate where possible |
| Prefer DirectLake | Avoid dataset duplication |

---

### 6.8 Lab: Build a Readmissions Dashboard with Row-Level Security (RLS)
**Module Alignment:** Section 6: Analytics and Reporting

**Objective:**
*   Connect Power BI to Microsoft Fabric Lakehouse data using the DirectLake connection mode.
*   Build an interactive Power BI dashboard to visualize hospital readmission data.
*   Implement Row-Level Security (RLS) in Power BI to restrict data visibility based on user roles (e.g., a provider seeing only their patients' readmission data).
*   Apply a sensitivity label to the Power BI dataset and report.
*   Publish the Power BI report to a Fabric workspace.

**Scenario:**
Valley General Hospital's quality improvement team needs a dashboard to track 30-day patient readmissions. The dashboard should allow analysis by diagnosis and provider. Crucially, when a specific healthcare provider views the dashboard, they should only see readmission data pertaining to patients under their care to maintain privacy and relevance.

**Prerequisites:**
*   Microsoft Fabric Workspace (e.g., `DEV_CardiologyAnalytics`).
*   Microsoft Fabric Lakehouse (e.g., `CardiologyLakehouse`).
*   Gold-layer tables created in Lab 4.8: `fact_encounter`, `dim_patient`, `dim_date`.
    *   `fact_encounter` should have `patient_sk`, `admission_date_sk`, `length_of_stay_days`, and a column indicating if it was a readmission (e.g., `is_readmission` (boolean/int)) and a `provider_id` or `provider_sk`.
    *   `dim_patient` should have `patient_sk` and `natural_patient_id`.
    *   `dim_date` should have `date_sk` and date attributes.
    *   *For this lab, we will augment the `fact_encounter` table created in Lab 4.8 with a `provider_id` and an `is_readmission` flag if it doesn't already exist.*
*   Power BI Desktop (optional, as much can be done in Fabric, but useful for complex modeling). For this lab, we'll primarily use the Fabric portal experience.
*   Permissions to create and publish Power BI content in the Fabric workspace.
*   Sample user email addresses for testing RLS (you can use your own or test accounts if available).
*   Sensitivity labels (e.g., "Confidential - PHI") configured in Microsoft Purview and available in Fabric.

**Tools to be Used:**
*   Microsoft Fabric Lakehouse (SQL Analytics Endpoint)
*   Microsoft Fabric Power BI (creating datasets and reports directly in the service)
*   (Conceptual) Power BI Desktop for RLS definition if preferred.

**Estimated Time:** 75 - 90 minutes

**Tasks / Step-by-Step Instructions:**

**Part 1: Augment Gold Layer Data (If Necessary)**

1.  **Open or Create a Notebook:**
    *   In your `DEV_CardiologyAnalytics` workspace, open the `Encounter_Gold_Modeling` notebook or create a new one.
    *   Ensure your `CardiologyLakehouse` is attached.

2.  **Add `provider_id` and `is_readmission` to `fact_encounter`:**
    *   If your `fact_encounter` table from Lab 4.8 doesn't have `provider_id` and `is_readmission`, run the following PySpark code to add them. This is a simplified way to add these for the lab; in reality, this data would come from source systems.
    ```python
    from pyspark.sql.functions import col, when, lit, monotonically_increasing_id, rand

    # Load existing fact_encounter if it exists, otherwise create a base for augmentation
    try:
        df_fact_encounter_existing = spark.read.table("CardiologyLakehouse.fact_encounter")
        
        # Check if columns exist
        existing_cols = df_fact_encounter_existing.columns
        needs_provider = "provider_id" not in existing_cols
        needs_readmission_flag = "is_readmission" not in existing_cols

        if needs_provider:
            # Simulate provider_id - assign one of three providers randomly
            df_fact_encounter_existing = df_fact_encounter_existing.withColumn("provider_id",
                when(rand() < 0.33, "DrSmith@valleygeneral.org")
                .when(rand() < 0.66, "DrJones@valleygeneral.org")
                .otherwise("DrBrown@valleygeneral.org")
            )
            print("Added simulated provider_id column.")

        if needs_readmission_flag:
            # Simulate is_readmission flag - ~20% readmission rate
            df_fact_encounter_existing = df_fact_encounter_existing.withColumn("is_readmission",
                when(rand() < 0.2, True).otherwise(False)
            )
            print("Added simulated is_readmission column.")

        if needs_provider or needs_readmission_flag:
            df_fact_encounter_existing.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable("CardiologyLakehouse.fact_encounter")
            print("fact_encounter table updated with provider_id and is_readmission.")
        else:
            print("fact_encounter table already has provider_id and is_readmission.")
            
        df_fact_encounter_augmented = spark.read.table("CardiologyLakehouse.fact_encounter")
        display(df_fact_encounter_augmented.select("natural_encounter_id", "provider_id", "is_readmission").limit(10))

    except Exception as e:
        print(f"Error loading or augmenting fact_encounter: {e}. Please ensure it was created in Lab 4.8.")
        # If it truly doesn't exist, you might need to run parts of Lab 4.8 first
        # For simplicity, if it fails, we'll create a minimal one here.
        # This is a fallback, ideally Lab 4.8 is completed.
        if "Table or view not found" in str(e):
            print("Creating a minimal fact_encounter for this lab.")
            data = [("ENC001", 101, 20230101, None, "outpatient", 0, 1, "SystemA", "DrSmith@valleygeneral.org", False),
                    ("ENC002", 102, 20230105, 20230110, "inpatient", 5, 1, "SystemB", "DrJones@valleygeneral.org", True),
                    ("ENC003", 101, 20230210, None, "outpatient", 0, 1, "SystemA", "DrSmith@valleygeneral.org", False)]
            columns = ["natural_encounter_id", "patient_sk", "admission_date_sk", "discharge_date_sk", 
                       "encounter_class", "length_of_stay_days", "encounter_count", "source_system", 
                       "provider_id", "is_readmission"]
            df_minimal_fact = spark.createDataFrame(data, columns)
            df_minimal_fact.write.format("delta").mode("overwrite").saveAsTable("CardiologyLakehouse.fact_encounter")
            print("Minimal fact_encounter created.")


    # Also ensure dim_patient has provider_id if RLS is based on a mapping table
    # For this lab, RLS will filter fact_encounter directly by provider_id (user's email)
    # A more robust RLS might use a mapping table: UserEmail -> ProviderID_SK -> ProviderID_Natural
    ```
    *   Run the cell.

**Part 2: Create a Power BI Dataset in Fabric (DirectLake)**

1.  **Navigate to your Lakehouse:** Open `CardiologyLakehouse`.
2.  **Create a New Power BI Dataset:**
    *   In the Lakehouse explorer view, on the top ribbon, click **New Power BI dataset**.
    *   A dialog "New Power BI dataset" will appear.
    *   Select the tables you need for the readmission dashboard:
        *   `fact_encounter`
        *   `dim_patient`
        *   `dim_date`
        *   (If you had a `dim_provider` and `dim_diagnosis`, you'd select them too).
    *   Click **Confirm**.
    *   Fabric will create a new Power BI dataset connected to your Lakehouse tables in DirectLake mode. It will open the dataset modeling view.
3.  **Manage Relationships (If Necessary):**
    *   Fabric often automatically detects relationships based on column names and data types.
    *   Go to the **Model view** (icon on the left that looks like a table relationship diagram).
    *   Verify or create relationships:
        *   `fact_encounter.patient_sk` to `dim_patient.patient_sk` (Many-to-One, single filter direction from dim to fact)
        *   `fact_encounter.admission_date_sk` to `dim_date.date_sk` (Many-to-One, single filter direction)
        *   (If you had `discharge_date_sk` actively related, you might make the admission date relationship inactive or use DAX USERELATIONSHIP. For simplicity, we'll focus on admission date for now).
4.  **Rename the Dataset:**
    *   In the dataset view, find the dataset name (likely `CardiologyLakehouse` or similar).
    *   Go to **File -> Rename**.
    *   Rename it to `ReadmissionAnalytics_Dataset`.
    *   Save your changes.

**Part 3: Implement Row-Level Security (RLS)**

*   We will create a role that filters the `fact_encounter` table based on the `provider_id` column, assuming `provider_id` stores the provider's email address which matches the User Principal Name (UPN) of the logged-in Power BI user.

1.  **Define RLS Roles and Rules:**
    *   In the Power BI dataset modeling view (within Fabric/Power BI service), on the top ribbon, click **Manage roles** (under "Home" or "Modeling" tab).
    *   In the "Manage roles" dialog:
        *   Click **Create**.
        *   **Role name:** Enter `ProviderView`.
        *   Select the `fact_encounter` table from the list of tables.
        *   In the DAX filter expression box for `fact_encounter`, enter the following DAX expression:
            ```dax
            [provider_id] = USERPRINCIPALNAME()
            ```
            *   This expression filters the `fact_encounter` table to only show rows where the `provider_id` matches the email address (UPN) of the user currently viewing the report.
        *   Click **Save**.
2.  **Test the Role (Optional but Recommended):**
    *   Still in the "Manage roles" dialog (or by clicking "View as" on the ribbon):
        *   Select the `ProviderView` role.
        *   You can optionally enter a user's email address (UPN) in the "Enter a user or group to test this role" field to simulate how that user would see the data. For now, just selecting the role is often enough to see if it applies a filter.
        *   Click **OK** or **Apply**.
        *   Navigate back to the report view (if you had one open) or the data view for `fact_encounter`. The data should now be filtered as if you were a provider whose UPN is in the `provider_id` column. If your UPN isn't one of the simulated provider_ids, you might see no data.
        *   To stop viewing as the role, click "Stop viewing" on the yellow banner that appears.

**Part 4: Create the Readmissions Power BI Report**

1.  **Create a New Report:**
    *   With `ReadmissionAnalytics_Dataset` open, click **Create report** -> **Auto-create** or **Start from scratch**. Let's choose **Start from scratch** for more control.
    *   A blank Power BI report canvas will open within the Fabric portal.
2.  **Add Visualizations:**
    *   **Slicer for Diagnosis (Placeholder):**
        *   Since we don't have `dim_diagnosis` fully set up, we'll skip this or use `encounter_class` from `fact_encounter` as a proxy.
        *   Add a Slicer visual. Drag `fact_encounter[encounter_class]` to the "Field" well.
    *   **Bar Chart: Readmissions by Provider:**
        *   Add a "Stacked bar chart" visual.
        *   **Y-axis:** Drag `fact_encounter[provider_id]` to the Y-axis.
        *   **X-axis:** Drag `fact_encounter[natural_encounter_id]` to the X-axis. Right-click it and select "Count (Distinct)" to get the total number of encounters.
        *   **Legend (or Small Multiples for readmissions):** Drag `fact_encounter[is_readmission]` to the Legend. This will show bars split by True/False for readmission.
    *   **KPI Card: Overall Readmission Rate:**
        *   First, create a measure for Readmission Rate. In the "Data" pane, right-click on `fact_encounter` -> **New measure**.
        *   Enter DAX:
            ```dax
            Readmission Rate =
            DIVIDE(
                CALCULATE(COUNTROWS('fact_encounter'), 'fact_encounter'[is_readmission] = TRUE()),
                COUNTROWS('fact_encounter')
            )
            ```
        *   Add a "Card" visual. Drag the `[Readmission Rate]` measure to the "Fields" well. Format it as a percentage.
    *   **Table: Readmission Details:**
        *   Add a "Table" visual.
        *   Add fields like: `dim_patient[natural_patient_id]`, `fact_encounter[admission_date_sk]` (or better, the actual date from `dim_date`), `fact_encounter[provider_id]`, `fact_encounter[is_readmission]`, `fact_encounter[length_of_stay_days]`.
3.  **Arrange and Format:**
    *   Arrange the visuals on the page.
    *   Use the "Format visual" pane to improve titles, colors, and text sizes.
4.  **Save the Report:**
    *   Click **File -> Save**.
    *   Name: `Hospital Readmission Dashboard`.
    *   Ensure it's saved to your `DEV_CardiologyAnalytics` workspace.

**Part 5: Apply Sensitivity Label to Dataset and Report**

1.  **Label the Dataset:**
    *   Navigate to your `DEV_CardiologyAnalytics` workspace.
    *   Find `ReadmissionAnalytics_Dataset`. Click the three dots (**...**) -> **Settings**.
    *   In the settings pane, find **Sensitivity label**.
    *   Select an appropriate label (e.g., "Confidential - PHI" or "HIPAA-HIGH").
    *   Click **Apply**.
2.  **Label the Report (Often Inherited):**
    *   Open the `Hospital Readmission Dashboard` report.
    *   The sensitivity label might be inherited from the dataset. If not, or if you want to set it explicitly, go to **File -> Sensitivity label** (within the report view) and apply the same label.

**Part 6: Publish and Test RLS**

1.  **Report is Already in Workspace:** Since we created it directly in Fabric, it's already "published" to the `DEV_CardiologyAnalytics` workspace.
2.  **Assign Users to RLS Roles:**
    *   Go to your `DEV_CardiologyAnalytics` workspace.
    *   Find `ReadmissionAnalytics_Dataset`. Click the three dots (**...**) -> **Security**.
    *   You will see the `ProviderView` role.
    *   Next to `ProviderView`, type the email address of a user you want to test with (e.g., one of the simulated provider emails like `DrSmith@valleygeneral.org`, or your own email if you added it to the `provider_id` column for some test data).
    *   Click **Add**. Then click **Save**.
3.  **Test RLS:**
    *   If you have access to the account you added to the `ProviderView` role (e.g., `DrSmith@valleygeneral.org`), log in to Fabric with that account.
    *   Navigate to the `DEV_CardiologyAnalytics` workspace and open the `Hospital Readmission Dashboard`.
    *   The user should only see data where `fact_encounter[provider_id]` matches their UPN.
    *   Alternatively, as an admin, you can use the "View as" functionality within the report or dataset settings to test the role.

**Expected Outcome / Deliverables:**
*   An augmented `fact_encounter` table with `provider_id` and `is_readmission` columns.
*   A Power BI dataset (`ReadmissionAnalytics_Dataset`) in Fabric using DirectLake mode, connected to the Gold layer tables.
*   An RLS role (`ProviderView`) defined on the dataset that filters data by `provider_id`.
*   An interactive Power BI report (`Hospital Readmission Dashboard`) visualizing readmission metrics.
*   The dataset and report are classified with an appropriate sensitivity label.
*   Understanding of how to assign users to RLS roles and test the security context.

**Questions from Manual & Answers: - LINK TO HTML?**

*   **Q1: Why is DirectLake the preferred connection mode for Power BI reports built on top of Fabric Lakehouse data for live analytics?**
    *   **A1:** DirectLake is preferred because:
        *   **Performance:** It reads Delta Parquet files directly from OneLake without needing to query a SQL endpoint or import/duplicate data into a separate Power BI proprietary format. This significantly reduces latency and improves query performance, especially for large datasets, making it feel like "Import" mode speed with "DirectQuery" data freshness.
        *   **No Data Duplication:** Data remains in OneLake. This avoids data silos, reduces storage costs, and ensures Power BI reports are always querying the single source of truth.
        *   **Real-time Analytics:** Changes made to the Delta tables in the Lakehouse (e.g., by Spark jobs or SQL) are immediately reflected in Power BI reports without needing a dataset refresh schedule (for the data itself, though model changes might need a refresh).
        *   **Simplified Architecture:** It streamlines the data flow from Lakehouse to Power BI.

*   **Q2: What is the primary purpose of Row-Level Security (RLS) in a healthcare analytics context?**
    *   **A2:** The primary purpose of RLS in healthcare analytics is to **restrict data access at the row level based on the identity or role of the user viewing the report or querying the dataset.** This ensures that users only see the data they are authorized to see, which is critical for:
        *   **HIPAA Compliance and Patient Privacy:** Preventing unauthorized access to Protected Health Information (PHI). For example, a doctor should only see their own patients' data, not data for all patients in the hospital.
        *   **Data Minimization:** Adhering to the principle of "minimum necessary" access.
        *   **Relevance:** Providing users with a view of the data that is most relevant to their specific role or department, reducing information overload.
        *   **Security:** Protecting sensitive data from internal threats or accidental exposure.

*   **Q3: How does applying a sensitivity label (e.g., "HIPAA-HIGH") to a Power BI report and its underlying dataset help in protecting the data?**
    *   **A3:** Applying a sensitivity label helps protect data in several ways:
        *   **Visual Indication:** It provides a clear visual marking on the report (and in the service) indicating the data's sensitivity level, reminding users to handle it appropriately.
        *   **Downstream Protection:** Labels can be inherited. If data is exported from a labeled report to Excel or PowerPoint, the label (and its associated protections, if any) can persist in those files.
        *   **Integration with DLP Policies:** Sensitivity labels are a key trigger for Data Loss Prevention (DLP) policies configured in Microsoft Purview. A DLP policy might:
            *   Block or audit attempts to share the labeled report with external users.
            *   Prevent downloading the report to unmanaged devices.
            *   Block printing or copying content from the report.
        *   **Access Control (Conditional Access):** In conjunction with Azure AD Conditional Access policies, access to reports with specific sensitivity labels can be further restricted (e.g., requiring MFA, or blocking access from non-compliant devices).
        *   **Auditing and Reporting:** Activities on labeled content are often audited with more scrutiny, and compliance reports can be generated based on data classifications.
        *   **User Awareness:** It raises user awareness about the nature of the data they are handling.

---

### 6.9 Quiz
**1. What connection mode keeps data in OneLake while allowing fast analytics?**
a) Import
b) DirectQuery
c) DirectLake
**â†’ Answer: c**

**2. What Power BI feature restricts user-specific access?**
a) Bookmarks
b) RLS
c) Filters
**â†’ Answer: b**

**3. Which type of dashboard shows operational performance?**
a) Mortality tracking
b) No-show appointment rate
c) HbA1c trend
**â†’ Answer: b**

---

### 6.10 Cheat Sheet (Printable)
*   **DirectLake** = Live OneLake access, no duplication
*   **RLS** = User-based data segmentation
*   **Power BI** = Integrated in Fabric for real-time visuals
*   **HIPAA Label** = Applied via Purview for PHI reports
*   **Dashboards** = Clinical, operational, financial, engagement
*   **Export Control** = Enforced via DLP & sensitivity labels

---

### Interactive Element for Section 6
[Interactive Analytics & Reporting Element](./visualizations-html/Section%206%20-%20Analytics%20and%20Reporting.html)

---

## ðŸ“˜ Section 7: Machine Learning and AI Integration {#section-7-machine-learning-and-ai-integration}

---

### 7.0 Overview
Machine Learning (ML) and Artificial Intelligence (AI) are transforming healthcare by enabling predictive insights that were once impossible with traditional analytics. From predicting patient readmissions to automating claims fraud detection, the potential of AI is vast.

Microsoft Fabric empowers healthcare data engineers to build, train, deploy, and operationalize ML models using **integrated notebooks, PySpark, and Azure Machine Learning (Azure ML)**. This section will guide you through the end-to-end process of developing explainable, regulatory-compliant AI workflows in Fabric, ensuring trust, fairness, and clinical relevance.

---

### 7.1 AI in Healthcare: Use Cases
| Use Case | Description |
| :--- | :--- |
| **Readmission Prediction** | Predict likelihood of a patient returning within 30 days |
| **Sepsis Detection** | Alert clinicians of early warning signs |
| **Clinical Deterioration** | Track ICU patients for vital instability |
| **Claims Anomaly Detection** | Identify billing errors and fraud |
| **Patient Segmentation** | Stratify populations for care coordination |
| **No-Show Prediction** | Forecast appointment attendance risks |

These models must be built and deployed with **transparency**, **privacy**, and **interpretability** at the forefront.

#### 7.1.1 AI/ML Workflow in Microsoft Fabric
<figure>
  <img src="./images/7.1-diagram_AI%3AML%20Workflow%20in%20Microsoft%20Fabric.png" alt="AI/ML Workflow in Microsoft Fabric" style="width:100%;" />
  <figcaption><em>Figure: Overview of the AI/Machine Learning workflow within Microsoft Fabric, from data sources to actionable insights.</em></figcaption>
</figure>

---

### 7.2 AI Architecture in Fabric
Microsoft Fabric enables ML workflows through:

| Component | Function |
| :--- | :--- |
| **Lakehouse** | Stores training/validation datasets |
| **Notebooks** | Develop ML models using Python, Spark MLlib, Scikit-Learn |
| **MLflow Integration** | Track experiments, hyperparameters, metrics |
| **Azure ML** | Model registration, endpoint deployment, governance |
| **Power BI** | Visualize predictions |
| **Purview** | Monitor AI data lineage, classifications |

> ðŸ’¡ Notebooks support Python, Spark, SQL, and R kernels.

---

### 7.3 ML Development Workflow in Fabric
#### Step-by-Step Pipeline:
1.  **Define Business Problem**
    *   E.g., Predict 30-day readmission for CHF patients
2.  **Ingest & Clean Data**
    *   Use Lakehouse Bronze â†’ Silver â†’ Gold pipeline
3.  **Engineer Features**
    *   Prior admissions, vitals, age, comorbidities
4.  **Split Dataset**
    *   70/30 train-test split
5.  **Train Model**
    *   Logistic Regression, XGBoost, Random Forest
6.  **Evaluate Performance**
    *   AUC, Precision, Recall, F1
7.  **Track via MLflow**
    *   Log metrics, hyperparameters, artifacts
8.  **Deploy Model**
    *   Azure ML endpoint or Fabric Notebook
9.  **Score New Data**
    *   Write back predictions to `Gold_PatientRisk`
10. **Visualize in Power BI**
    *   Create dashboards filtered by risk bands

---

### 7.4 Building in Notebooks
#### Sample: Logistic Regression for Readmissions
```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score
import mlflow

# Assuming df is your Spark DataFrame loaded from Gold_Admissions
# df = spark.read.format("delta").load("/Tables/Gold_Admissions")
# For simplicity, let's assume pandas_df is already prepared
# pandas_df = df.toPandas() 

# Example: Replace with actual feature and target column names
# X = pandas_df[["Age", "PriorVisits", "ChronicConditions"]]
# y = pandas_df["Readmitted"]

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# with mlflow.start_run():
#     model = LogisticRegression()
#     model.fit(X_train, y_train)

#     y_pred = model.predict(X_test)
#     y_pred_proba = model.predict_proba(X_test)[:,1] # For AUC

#     print(classification_report(y_test, y_pred))
#     auc_score = roc_auc_score(y_test, y_pred_proba)
#     print("AUC:", auc_score)
    
#     mlflow.log_metric("auc", auc_score)
#     mlflow.sklearn.log_model(model, "readmission_model")

```

> ðŸ“Œ Use **Delta Tables** to persist scoring outputs.

---

### 7.5 Azure ML Integration
Models developed in Fabric can be pushed to **Azure ML** for:

*   Centralized **model registry**
*   Scalable **batch inference**
*   Realtime **REST endpoints**
*   **CI/CD pipelines** for model promotion

#### HIPAA Notes:
*   Ensure endpoints are secured
*   Use only **de-identified or tokenized** data
*   Monitor access via Azure Monitor & Purview

---

### 7.6 Explainability and Responsible AI
#### Tools:
| Tool | Purpose |
| :--- | :--- |
| **SHAP** | Feature attribution |
| **Fairlearn** | Bias auditing |
| **MLflow** | Model traceability |
| **Purview** | Data lineage |

#### Example: Bias Check with Fairlearn
```python
# from fairlearn.metrics import MetricFrame, selection_rate
# Assuming y_test, y_pred, and sensitive features (e.g., pandas_df["Sex"]) are defined
# sensitive_features_test = X_test["Sex"] # Or however sensitive features are aligned with y_test
# grouped_on_sex = MetricFrame(metrics=selection_rate,
#                              y_true=y_test,
#                              y_pred=y_pred,
#                              sensitive_features=sensitive_features_test)
# print(grouped_on_sex.by_group)
```

> ðŸ›¡ï¸ Explainability is crucial for **clinician trust** and **regulatory defense**.

#### 7.6.1 AI for Chronic Disease Progression Modeling
<figure>
  <img src="./images/7.6.1-diagram_Conceptual%20workflow%20for%20AI-driven%20chronic%20disease%20progression%20modeling%20using%20Microsoft%20Fabric.png" alt="AI for Chronic Disease Progression Modeling Workflow" style="width:100%;" />
  <figcaption><em>Figure: Conceptual workflow for AI-driven chronic disease progression modeling using Microsoft Fabric.</em></figcaption>
</figure>

---

### 7.7 HIPAA and HITRUST Considerations
*   All training data must be **de-identified or covered under BAA**
*   Secure PHI using **Customer Managed Keys**
*   Enable **audit logging** on all Notebook sessions
*   Perform **periodic access reviews**
*   Implement **Human-in-the-Loop (HITL)** for critical decisions

---

### 7.8 Lab: Build a Readmission Risk Model with MLflow Tracking
**Module Alignment:** Section 7: Machine Learning and AI Integration

**Objective:**
*   Develop a machine learning model to predict 30-day hospital readmission risk using patient data.
*   Utilize a Microsoft Fabric Notebook with PySpark and scikit-learn for model development.
*   Perform basic feature engineering, data cleaning, and model training.
*   Integrate MLflow to track experiment parameters, metrics, and the trained model.
*   Persist model predictions back to a Gold layer table in the Lakehouse.
*   (Conceptual) Understand how these predictions could be visualized in Power BI.

**Scenario:**
Valley General Hospital wants to proactively identify patients at high risk of readmission within 30 days of discharge. As a data scientist/engineer, you are tasked with building a classification model using historical patient and encounter data. The model's performance and artifacts need to be tracked using MLflow for reproducibility and governance.

**Prerequisites:**
*   Microsoft Fabric Workspace (e.g., `DEV_CardiologyAnalytics`).
*   Microsoft Fabric Lakehouse (e.g., `CardiologyLakehouse`).
*   Gold-layer tables: `fact_encounter` (augmented in Lab 6.8 to include `is_readmission` and `provider_id`) and `dim_patient`.
    *   `fact_encounter` needs columns like `patient_sk`, `admission_date_sk`, `length_of_stay_days`, `encounter_class`, `is_readmission` (boolean target variable), and other potential features.
    *   `dim_patient` needs `patient_sk` and demographic features like `age_group_sim`, `gender_sim`.
    *   *For this lab, we will ensure these tables have some features suitable for modeling. If not fully populated from previous labs, we'll add/simulate them.*
*   Familiarity with creating and running Fabric Notebooks.
*   Basic understanding of machine learning concepts (classification, feature engineering, train-test split, evaluation metrics like AUC, precision, recall).
*   Basic knowledge of Python, PySpark, and scikit-learn.

**Tools to be Used:**
*   Microsoft Fabric Lakehouse
*   Microsoft Fabric Notebook (PySpark, Python, scikit-learn)
*   MLflow (integrated within Fabric)

**Estimated Time:** 90 - 120 minutes

**Tasks / Step-by-Step Instructions:**

**Part 1: Prepare Data for Modeling**

1.  **Open or Create a Notebook:**
    *   In your `DEV_CardiologyAnalytics` workspace, create a new Notebook (e.g., `Readmission_Risk_Modeling`) or open an existing one.
    *   Ensure your `CardiologyLakehouse` is attached.

2.  **Load and Prepare Feature Set:**
    *   In a PySpark cell, load data from `fact_encounter` and `dim_patient`. Join them and select/engineer features.
    ```python
    # Load tables
    df_fact_encounter = spark.read.table("CardiologyLakehouse.fact_encounter")
    df_dim_patient = spark.read.table("CardiologyLakehouse.dim_patient")

    # Join fact and dimension tables
    df_model_input = df_fact_encounter.join(
        df_dim_patient,
        df_fact_encounter.patient_sk == df_dim_patient.patient_sk,
        "inner"
    ).select(
        df_fact_encounter.natural_encounter_id,
        df_fact_encounter.patient_sk, # Keep for potential future use or joining predictions
        df_dim_patient.age_group_sim.alias("age_group"),
        df_dim_patient.gender_sim.alias("gender"),
        df_fact_encounter.encounter_class,
        df_fact_encounter.length_of_stay_days,
        # Add more features if available, e.g., number of prior visits, specific diagnosis codes (would require dim_diagnosis)
        # For simplicity, we'll use these.
        # Ensure the target variable 'is_readmission' is present and boolean or 0/1
        col("is_readmission").cast("boolean").alias("label") # Our target variable
    )

    # Handle missing values (simple imputation for this lab)
    # For length_of_stay_days, fill with mean or median if appropriate, or 0 if it makes sense
    # For categorical, fill with a specific category like 'Unknown'
    mean_los = df_model_input.select(mean(col("length_of_stay_days"))).first()[0]
    if mean_los is None: # Handle case where all LOS are null initially
        mean_los = 0 

    df_model_input = df_model_input.fillna({
        "length_of_stay_days": mean_los, # Example: fill with mean
        "age_group": "Unknown",
        "gender": "Unknown",
        "encounter_class": "Unknown"
    })
    
    # Ensure label column does not have nulls for training
    df_model_input = df_model_input.na.drop(subset=["label"])


    # Convert to Pandas DataFrame for scikit-learn (for smaller datasets)
    # For larger datasets, consider using Spark MLlib or distributed training.
    # This lab uses scikit-learn for simplicity with MLflow.
    pandas_df = df_model_input.toPandas()

    print(f"Prepared dataset for modeling with {pandas_df.shape[0]} rows and {pandas_df.shape[1]} columns.")
    display(pandas_df.head())
    pandas_df.info()
    ```
    *   Run the cell. This creates a Pandas DataFrame ready for scikit-learn.

**Part 2: Feature Engineering and Preprocessing (scikit-learn)**

1.  **Encode Categorical Features and Split Data:**
    *   In a new Python cell (ensure the notebook cell language is Python if you switched from PySpark explicitly):
    ```python
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
    from sklearn.compose import ColumnTransformer
    from sklearn.naive_bayes import GaussianNB # Using Naive Bayes for simplicity
    from sklearn.linear_model import LogisticRegression # Alternative
    from sklearn.ensemble import RandomForestClassifier # More powerful alternative
    from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix
    import mlflow
    import mlflow.sklearn # For scikit-learn specific logging
    import numpy as np # For handling potential NaN/inf if any remain after fillna

    # Ensure pandas_df is available from the previous PySpark cell
    # If it's not (e.g. running cells independently), you might need to re-run the PySpark cell
    # or load data here if saved as an intermediate file.

    if 'pandas_df' not in locals() or pandas_df.empty:
        print("pandas_df is not defined or empty. Please run the previous cell to generate it.")
        # As a fallback for testing, create a dummy pandas_df
        data_dummy = {
            'natural_encounter_id': [f'E{i}' for i in range(100)],
            'patient_sk': range(100),
            'age_group': np.random.choice(['20-29', '30-45', '46+', 'Unknown'], 100),
            'gender': np.random.choice(['Male', 'Female', 'Unknown'], 100),
            'encounter_class': np.random.choice(['inpatient', 'outpatient', 'emergency', 'Unknown'], 100),
            'length_of_stay_days': np.random.randint(0, 15, 100),
            'label': np.random.choice([True, False], 100, p=[0.2, 0.8])
        }
        pandas_df = pd.DataFrame(data_dummy)
        pandas_df['length_of_stay_days'] = pandas_df['length_of_stay_days'].astype(float) # Ensure numeric
        print("Created a dummy pandas_df for testing.")


    # Separate features (X) and target (y)
    X = pandas_df.drop(columns=['label', 'natural_encounter_id', 'patient_sk']) # Drop identifiers and target
    y = pandas_df['label'].astype(int) # Ensure target is integer (0 or 1)

    # Identify categorical and numerical features
    categorical_features = ['age_group', 'gender', 'encounter_class']
    numerical_features = ['length_of_stay_days']

    # Create preprocessor
    preprocessor = ColumnTransformer(
        transformers=[
            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),
            ('num', StandardScaler(), numerical_features)
        ],
        remainder='passthrough' # Keep other columns if any (shouldn't be if X is defined correctly)
    )

    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

    # Apply preprocessing
    X_train_processed = preprocessor.fit_transform(X_train)
    X_test_processed = preprocessor.transform(X_test)

    print("Data preprocessing complete.")
    print(f"X_train_processed shape: {X_train_processed.shape}")
    print(f"X_test_processed shape: {X_test_processed.shape}")
    ```
    *   Run the cell. This performs one-hot encoding for categorical features and scaling for numerical features.

**Part 3: Train Model and Track with MLflow**

1.  **Train a Classifier and Log with MLflow:**
    *   In a new Python cell:
    ```python
    # MLflow experiment setup
    # Fabric automatically creates an experiment associated with the notebook.
    # You can also set a custom experiment name.
    # mlflow.set_experiment("ReadmissionRiskExperiment_Notebook") # Optional: Set experiment name

    with mlflow.start_run() as run:
        run_id = run.info.run_id
        print(f"MLflow Run ID: {run_id}")

        # --- Model Training ---
        # Using RandomForestClassifier for better potential performance
        model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
        # model = LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced') # Alternative
        
        model.fit(X_train_processed, y_train)

        # --- Predictions ---
        y_pred_train = model.predict(X_train_processed)
        y_pred_test = model.predict(X_test_processed)
        y_pred_proba_test = model.predict_proba(X_test_processed)[:, 1] # Probabilities for AUC

        # --- Evaluation Metrics ---
        train_accuracy = accuracy_score(y_train, y_pred_train)
        test_accuracy = accuracy_score(y_test, y_pred_test)
        precision = precision_score(y_test, y_pred_test, zero_division=0)
        recall = recall_score(y_test, y_pred_test, zero_division=0)
        f1 = f1_score(y_test, y_pred_test, zero_division=0)
        auc = roc_auc_score(y_test, y_pred_proba_test) if len(np.unique(y_test)) > 1 else 0.5 # AUC requires multiple classes

        print(f"Test Accuracy: {test_accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1-score: {f1:.4f}")
        print(f"AUC: {auc:.4f}")
        print("\nConfusion Matrix (Test Set):")
        print(confusion_matrix(y_test, y_pred_test))

        # --- MLflow Logging ---
        # Log parameters
        mlflow.log_param("model_type", model.__class__.__name__)
        if hasattr(model, 'get_params'):
            mlflow.log_params(model.get_params()) # Log all model hyperparameters

        # Log metrics
        mlflow.log_metric("train_accuracy", train_accuracy)
        mlflow.log_metric("test_accuracy", test_accuracy)
        mlflow.log_metric("precision", precision)
        mlflow.log_metric("recall", recall)
        mlflow.log_metric("f1_score", f1)
        mlflow.log_metric("auc", auc)

        # Log the trained model
        # The 'artifact_path' is relative to the run's artifact directory
        mlflow.sklearn.log_model(sk_model=model, artifact_path="readmission_risk_model",
                                 serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_PICKLE)
        print("Model, parameters, and metrics logged to MLflow.")

        # Log the preprocessor (important for inference)
        mlflow.sklearn.log_model(sk_model=preprocessor, artifact_path="preprocessor")
        print("Preprocessor logged to MLflow.")

        # (Optional) Log feature names after one-hot encoding for interpretability
        try:
            feature_names_out = preprocessor.get_feature_names_out()
            mlflow.log_text("\n".join(feature_names_out), "feature_names.txt")
        except Exception as e:
            print(f"Could not log feature names: {e}")
            
        # (Optional) Log a sample of test predictions
        # test_predictions_df = pd.DataFrame({'true_label': y_test, 'predicted_label': y_pred_test, 'predicted_probability': y_pred_proba_test})
        # mlflow.log_text(test_predictions_df.head(20).to_csv(index=False), "sample_test_predictions.csv")

    print(f"MLflow Run completed. Check the 'Runs' tab for this notebook or the MLflow experiment UI.")
    ```
    *   Run the cell. This trains the model, calculates various performance metrics, and logs everything (parameters, metrics, model artifact, preprocessor) to MLflow.
    *   After running, you can navigate to your workspace, find the "Experiments" section (or look for MLflow runs associated with your notebook), and explore the logged run.

**Part 4: Persist Predictions to Lakehouse**

1.  **Make Predictions on the Full Dataset and Save:**
    *   We'll use the trained model and preprocessor from the MLflow run to make predictions on the original `pandas_df` (or a fresh load of it) and save these predictions.
    *   In a new Python cell:
    ```python
    # Load the logged model and preprocessor from MLflow
    # Replace 'RUN_ID_HERE' with the actual run_id printed in the previous cell output
    # Or, you can get the latest run for the current experiment
    
    # Get the latest run ID for the current notebook's experiment
    current_experiment = mlflow.get_experiment_by_name(mlflow.get_run(run_id=None).data.tags['mlflow.source.name']) # Gets current notebook path as experiment name
    if current_experiment:
        latest_run = mlflow.search_runs(experiment_ids=[current_experiment.experiment_id], order_by=["start_time DESC"], max_results=1).iloc[0]
        logged_run_id = latest_run.run_id
        print(f"Using latest MLflow Run ID: {logged_run_id}")

        logged_model_path = f"runs:/{logged_run_id}/readmission_risk_model"
        logged_preprocessor_path = f"runs:/{logged_run_id}/preprocessor"

        loaded_model = mlflow.sklearn.load_model(logged_model_path)
        loaded_preprocessor = mlflow.sklearn.load_model(logged_preprocessor_path)

        # Prepare the full dataset for prediction (using the original pandas_df before train/test split)
        X_full = pandas_df.drop(columns=['label', 'natural_encounter_id', 'patient_sk']) # Same features as training

        # Apply the loaded preprocessor
        X_full_processed = loaded_preprocessor.transform(X_full)

        # Make predictions
        full_predictions = loaded_model.predict(X_full_processed)
        full_prediction_probabilities = loaded_model.predict_proba(X_full_processed)[:, 1] # Probability of being readmitted

        # Add predictions back to the original pandas_df
        pandas_df_with_predictions = pandas_df.copy()
        pandas_df_with_predictions['predicted_readmission_label'] = full_predictions
        pandas_df_with_predictions['predicted_readmission_probability'] = full_prediction_probabilities
        
        # Select relevant columns for the output table
        df_predictions_output = pandas_df_with_predictions[['natural_encounter_id', 'patient_sk', 'label', 'predicted_readmission_label', 'predicted_readmission_probability']]
        
        # Convert Pandas DataFrame with predictions back to Spark DataFrame to save in Lakehouse
        spark_df_predictions = spark.createDataFrame(df_predictions_output)

        spark_df_predictions.printSchema()
        display(spark_df_predictions.limit(10))

        # Write predictions to a Gold table
        spark_df_predictions.write.format("delta").mode("overwrite").saveAsTable("CardiologyLakehouse.gold_patient_readmission_predictions")
        print("gold_patient_readmission_predictions table created successfully.")
    else:
        print("Could not find the MLflow experiment for this notebook to load the model.")

    ```
    *   Run the cell. This loads the model and preprocessor from the specified MLflow run, applies them to the full dataset, and saves the encounter identifiers along with their original label, predicted label, and prediction probability to a new Gold table.

**Part 5: Conceptual - Visualize Predictions in Power BI**

*   The `gold_patient_readmission_predictions` table can now be used in Power BI.
*   You would create a new Power BI dataset (or update an existing one) to include this table.
*   In a Power BI report, you could:
    *   Create a table showing patients with high readmission probability (`predicted_readmission_probability > 0.7`).
    *   Visualize the distribution of prediction probabilities.
    *   Compare actual readmissions (`label`) with predicted readmissions (`predicted_readmission_label`) to assess model performance on new data (though this table has predictions on the training/test data combined).
    *   Filter by provider (if `provider_id` was joined into `df_predictions_output`) to show high-risk patients for specific clinicians.

**Expected Outcome / Deliverables:**
*   A trained machine learning model (e.g., RandomForestClassifier) for readmission prediction.
*   An MLflow experiment run associated with the notebook, containing:
    *   Logged hyperparameters of the model.
    *   Logged evaluation metrics (accuracy, precision, recall, F1-score, AUC).
    *   The serialized model artifact.
    *   The serialized preprocessor artifact.
*   A Delta table named `gold_patient_readmission_predictions` in `CardiologyLakehouse` containing encounter identifiers and their predicted readmission status and probabilities.
*   Understanding of the end-to-end ML workflow within Fabric using Notebooks and MLflow.

**Questions from Manual & Answers: - LINK TO HTML?**

*   **Q1: What makes a machine learning model "explainable," and why is this particularly important in healthcare decision-making?**
    *   **A1:**
        *   **Explainable Model:** An explainable AI (XAI) model is one whose internal workings and decision-making processes can be understood by humans. Instead of being a "black box," it provides insights into *why* it made a particular prediction or decision. Techniques like SHAP (SHapley Additive exPlanations) or LIME (Local Interpretable Model-agnostic Explanations) can help identify which input features contributed most to a prediction.
        *   **Importance in Healthcare:**
            1.  **Clinical Trust and Adoption:** Clinicians are more likely to trust and use AI-driven recommendations if they understand the reasoning behind them. A black box model that simply outputs a risk score without explanation is less likely to be adopted.
            2.  **Patient Safety and Accountability:** If a model makes an incorrect prediction leading to an adverse patient outcome, explainability is crucial for understanding the failure, debugging the model, and determining accountability.
            3.  **Bias Detection and Fairness:** Explainability techniques can help uncover if a model is relying on sensitive attributes (like race or gender, even if indirectly) in a biased way, which is ethically and legally problematic in healthcare.
            4.  **Regulatory Compliance:** Regulatory bodies (like the FDA for medical devices incorporating AI) are increasingly emphasizing the need for model transparency and explainability.
            5.  **Improving Models:** Understanding which features are driving predictions can provide insights back to data scientists to improve feature engineering or identify data quality issues.
            6.  **Personalized Medicine:** Understanding why a model predicts a certain risk for an *individual* patient can help tailor interventions more effectively.

*   **Q2: How do tools like SHAP (SHapley Additive exPlanations) values assist clinicians in understanding and trusting model predictions?**
    *   **A2:** SHAP values assist clinicians by providing **feature-level importance for individual predictions**. For a specific patient predicted to be at high risk of readmission, SHAP can show:
        *   Which specific factors (e.g., "length of previous stay = 10 days," "age_group = 70+", "number of chronic conditions = 5") contributed most to increasing that risk score.
        *   Which factors might have decreased the risk score.
        *   The magnitude of each feature's contribution.
        This allows a clinician to see if the model's reasoning aligns with their clinical judgment. If the top reasons make clinical sense, trust in the prediction increases. If a seemingly irrelevant feature is driving the prediction, it might indicate a model issue or an unexpected correlation worth investigating. This transparency moves beyond just a risk score to a more actionable insight.

*   **Q3: Why is model lineage (tracking data, code, parameters, and versions used to train a model) important for compliance and reproducibility in healthcare AI?**
    *   **A3:** Model lineage is critical for:
        *   **Reproducibility:** If you need to retrain the model or reproduce a previous result (e.g., for validation or to understand a past prediction), lineage ensures you can use the exact same dataset version, code version, environment, and hyperparameters. MLflow helps capture much of this.
        *   **Auditing and Compliance:** Regulatory bodies or internal auditors may require proof of how a model was developed, validated, and what data it was trained on. Complete lineage provides this audit trail, demonstrating due diligence and adherence to development standards (e.g., for HIPAA security rule compliance regarding data integrity and access).
        *   **Debugging and Error Analysis:** If a model starts performing poorly or making unexpected predictions, lineage helps trace back to changes in data, code, or dependencies that might have caused the issue.
        *   **Model Versioning and Management:** As models are updated or retrained, lineage helps track different versions, their performance, and the data they were trained on, allowing for rollback if a new version underperforms.
        *   **Impact Analysis:** If an issue is found in an upstream data source, lineage helps identify all models trained on that data that might be affected and require retraining or revalidation.
        *   **Transparency and Trust:** Documented lineage contributes to the overall transparency of the AI system, building trust with stakeholders, including clinicians and patients.

---

### 7.9 Quiz
**1. What is MLflow used for in Fabric?**
a) Data ingestion
b) Tracking models and metrics
c) Sharing reports
**â†’ Answer: b**

**2. What is a common model used for binary predictions in healthcare?**
a) K-Means
b) Logistic Regression
c) PCA
**â†’ Answer: b**

**3. Which tool helps explain feature influence?**
a) SHAP
b) KQL
c) RLS
**â†’ Answer: a**

---

### 7.10 Cheat Sheet (Printable)
*   **MLflow** = Track metrics, parameters, artifacts
*   **Azure ML** = Register, deploy, govern models
*   **SHAP** = Explainability for predictions
*   **Fairlearn** = Bias auditing
*   **Delta Tables** = Persist scored outputs
*   **HIPAA** = Requires secure, explainable AI
*   **Gold_PatientRisk** = Recommended output table

---

### Interactive Element for Section 7
[Interactive ML & AI Element](./visualizations-html/Section%207%20-%20Machine%20Learning%20and%20AI%20Integration.html)

---

## ðŸ“˜ Section 8: Performance Optimization {#section-8-performance-optimization}

---

### 8.0 Overview
As healthcare data volumes growâ€”spanning patient records, telemetry, imaging metadata, and financial transactionsâ€”so too does the complexity of managing system performance. Even a well-architected Fabric deployment can suffer from latency, bottlenecks, and resource contention without targeted tuning.

This section empowers Data Engineers to identify, troubleshoot, and resolve performance issues across **Microsoft Fabricâ€™s components**: Lakehouse, Spark Notebooks, SQL Warehouses, and Power BI reports. Strategies outlined here will ensure seamless operation, faster insights, and cost-effective resource consumptionâ€”without compromising security or compliance.

---

### 8.1 Key Areas of Optimization
| Layer | Focus |
| :--- | :--- |
| **Lakehouse** | Partitioning, file sizes, metadata |
| **Notebooks (Spark)** | Caching, parallelism, memory tuning |
| **SQL Warehouses** | Indexing, query plans, concurrency |
| **Power BI Reports** | DAX formulas, visual load, data reduction |
| **Pipelines** | Step execution time, parallelism |
| **Storage and I/O** | Delta file structure, read/write latency |

#### Data-Driven ED Optimization Workflow
<figure>
  <img src="./images/8.1-diagram_Data-Driven%20ED%20Optimization%20Workflow.png" alt="Data-Driven Emergency Department Optimization Workflow" style="width:100%;" />
  <figcaption><em>Figure: Data-Driven Emergency Department Optimization Workflow using Microsoft Fabric.</em></figcaption>
</figure>

---

### 8.2 Lakehouse and Delta Optimization
#### A. Partitioning
Effective partitioning improves scan performance by reducing file reads.

| Recommended Partition Fields | Example |
| :--- | :--- |
| EncounterDate | `2024-01-01` |
| FacilityID | `Hospital_001` |
| Year-Month | `2024-01` |

```python
# df.write.format("delta").partitionBy("YearMonth").save("path/to/partitioned_table")
```

> ðŸ“Œ Avoid over-partitioning (>5000 small partitions) which increases metadata overhead.

#### B. Optimize and Vacuum
*   Use `OPTIMIZE` to compact small files
*   Use `VACUUM` to remove obsolete files and save storage

```sql
OPTIMIZE Gold_Encounter
VACUUM Gold_Encounter RETAIN 168 HOURS
```

> ðŸ§¼ Run these during low-traffic hours to prevent job conflicts.

---

### 8.3 Spark and Notebook Optimization
#### A. Resource Management
*   Allocate appropriate Spark clusters (cores, RAM)
*   Monitor executor memory and garbage collection
*   Use `df.persist()` or `df.cache()` selectively

```python
# df.cache()
# df.count()  # Materializes cache
```

#### B. Parallelism
*   Use `.repartition()` before wide transformations
*   Avoid `collect()` on large DataFrames
*   Use broadcast joins for small dimension tables

```python
# df = df.repartition("EncounterDate")
```

#### C. UDF Performance
Avoid using Python UDFs where native Spark SQL functions can sufficeâ€”use **PySpark SQL functions** instead.

---

### 8.4 SQL Warehouse Query Tuning
#### A. Indexing and Statistics
*   Create indexes on commonly filtered columns:
```sql
CREATE INDEX idx_admitdate ON Fact_Visit(AdmitDate)
```
*   Refresh table statistics regularly:
```sql
ANALYZE TABLE Fact_Visit COMPUTE STATISTICS
```

#### B. Query Plan Analysis
*   Use **EXPLAIN PLAN** to identify:
    *   Cartesian joins
    *   Table scans
    *   Unfiltered subqueries

#### C. Materialized Views
*   Use for high-use aggregations:
```sql
CREATE MATERIALIZED VIEW ReadmissionRate AS
SELECT ProviderID, COUNT(*) WHERE Readmitted=1 GROUP BY ProviderID
```

---

### 8.5 Power BI Optimization
#### A. Data Model Best Practices
| Practice | Why |
| :--- | :--- |
| Star Schema | Improves join efficiency |
| Avoid Calculated Columns | Use Measures |
| Hide Unused Fields | Reduce memory usage |

#### B. DAX Performance
*   Avoid `CALCULATE` within nested filters
*   Use `SUMX()` and `FILTER()` sparingly
*   Prefer `VAR` blocks for complex logic

#### C. Visual Optimization
*   Limit visuals per page to under 8
*   Use bookmarks for drilldowns instead of heavy cross-filters
*   Pre-filter large datasets

---

### 8.6 Monitoring and Diagnostics
#### Tools:
| Tool | Usage |
| :--- | :--- |
| **Fabric Monitoring Hub** | View pipeline performance, errors |
| **Spark UI** | Executor logs, DAG analysis |
| **Power BI Performance Analyzer** | Visual-level timings |
| **SQL Execution Plans** | Identify bottlenecks |
| **Purview** | Lineage tracing and access analysis |

#### Metrics to Watch:
*   Query latency
*   Task failure rate
*   Dataset refresh time
*   File scan percentage
*   Memory spills

---

### 8.7 Lab: Diagnose and Optimize a Slow Power BI Report
**Module Alignment:** Section 8: Performance Optimization

**Objective:**
*   Identify performance bottlenecks in a Power BI report using the Performance Analyzer.
*   Apply DAX optimization techniques by converting calculated columns to measures.
*   Implement data model best practices by reducing unnecessary fields in visuals.
*   Understand how Lakehouse table optimization (partitioning, `OPTIMIZE` command) can contribute to better Power BI performance when using DirectLake.

**Scenario:**
Valley General Hospital's cardiology department uses a Power BI report to track patient encounter trends. Recently, clinicians have reported that the main page of this report is loading very slowly, especially during morning rounds when multiple users access it. Your task is to diagnose the performance issues and implement optimizations.

**Prerequisites:**
*   Microsoft Fabric Workspace (e.g., `DEV_CardiologyAnalytics`).
*   Microsoft Fabric Lakehouse (e.g., `CardiologyLakehouse`).
*   Gold-layer tables: `fact_encounter`, `dim_patient`, `dim_date` (as created/augmented in previous labs).
    *   `fact_encounter` should have a variety of columns, including some dates and numerical values.
*   A Power BI report built on these tables within the Fabric workspace. We will create a sample "slow" report for this lab.
*   Permissions to edit Power BI reports and datasets in the Fabric workspace.
*   (Optional but helpful) Power BI Desktop for more in-depth DAX editing or model viewing, though we will focus on the Fabric service experience.

**Tools to be Used:**
*   Microsoft Fabric Power BI (report and dataset editing in the service)
*   Power BI Performance Analyzer
*   Microsoft Fabric Notebook (for Lakehouse table optimization)
*   Microsoft Fabric Lakehouse (SQL Analytics Endpoint - conceptual for verifying table optimizations)

**Estimated Time:** 75 - 90 minutes

**Tasks / Step-by-Step Instructions:**

**Part 1: Create a Sample "Slow" Power BI Report**

1.  **Create/Open a Notebook to Prepare Data (If Needed):**
    *   Ensure your `fact_encounter`, `dim_patient`, and `dim_date` tables are populated. If `fact_encounter` is small, let's add more rows to simulate a larger dataset that might cause slowness.
    ```python
    # In a Fabric Notebook
    # This cell is optional if your fact_encounter is already reasonably large (e.g., >10,000 rows)
    # Forcing a larger table to better demonstrate performance issues.

    df_fact_encounter = spark.read.table("CardiologyLakehouse.fact_encounter")
    current_rows = df_fact_encounter.count()
    print(f"Current rows in fact_encounter: {current_rows}")

    if current_rows < 10000: # Let's aim for at least 50k-100k to see some effect
        num_multiples = (50000 // current_rows) + 1 if current_rows > 0 else 50000
        
        # Create an empty list to hold DataFrames
        dfs_to_union = []
        if current_rows > 0:
            for i in range(num_multiples):
                # Create a new DataFrame by adding a suffix to encounter_id to ensure uniqueness if it's a key
                # and slightly varying some data to avoid perfect duplicates if that matters for your scenario
                df_new_iteration = df_fact_encounter.withColumn("natural_encounter_id", concat(col("natural_encounter_id"), lit(f"_copy{i}")))
                dfs_to_union.append(df_new_iteration)
            
            # Union all DataFrames
            if dfs_to_union:
                df_fact_encounter_large = dfs_to_union[0]
                for i in range(1, len(dfs_to_union)):
                    df_fact_encounter_large = df_fact_encounter_large.unionByName(dfs_to_union[i], allowMissingColumns=True)
                
                df_fact_encounter_large.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable("CardiologyLakehouse.fact_encounter_large")
                print(f"Created fact_encounter_large with approximately {df_fact_encounter_large.count()} rows.")
                # For the rest of the lab, we'll assume the report will be built on 'fact_encounter_large'
                # Or you can rename/overwrite 'fact_encounter'
                # spark.sql("DROP TABLE IF EXISTS CardiologyLakehouse.fact_encounter")
                # spark.sql("ALTER TABLE CardiologyLakehouse.fact_encounter_large RENAME TO CardiologyLakehouse.fact_encounter")
                # print("Renamed fact_encounter_large to fact_encounter.")
            else:
                print("No data to multiply.")
        else:
            print("fact_encounter is empty, cannot multiply rows.")
    else:
        print("fact_encounter is already large enough.")

    # For the lab, let's assume we will use 'fact_encounter' and it has been made larger.
    # If you created 'fact_encounter_large', either use that name or rename it.
    ```
    *   Run the cell. This step is to ensure the dataset is non-trivial.

2.  **Create a New Power BI Dataset and Report in Fabric:**
    *   Navigate to `CardiologyLakehouse`.
    *   Click **New Power BI dataset**. Select `fact_encounter` (the potentially enlarged one), `dim_patient`, and `dim_date`. Click **Confirm**.
    *   Rename the dataset to `EncounterTrends_Slow_Dataset`.
    *   Verify relationships in the Model view (as in Lab 6.8).
    *   Click **Create report -> Start from scratch**.
3.  **Design the "Slow" Report Page:**
    *   **Add a Calculated Column (Bad Practice for this scenario):**
        *   Go to the Data view in the dataset. Select `fact_encounter`.
        *   Click **New column**.
        *   Enter DAX: `EncounterYearMonth = FORMAT('fact_encounter'[admission_date_sk], "YYYY-MM")`
            *   *(Note: `admission_date_sk` from `dim_date` would be better, or the actual date column from `fact_encounter` if it's a date type. We are using `admission_date_sk` from `fact_encounter` which should be an integer key. For this to work as a date, you'd ideally use the actual admission_date from `fact_encounter` before it's keyed, or join back to `dim_date` and use its date column. Let's assume `fact_encounter` has an `admission_full_date` column of type Date for this calculated column)*
            *   Let's refine this. Assuming `fact_encounter` has `admission_date_sk` and `dim_date` has `date_sk` and `calendar_date`.
            *   In the `fact_encounter` table (Data view), create this calculated column (this is intentionally inefficient for the lab):
                ```dax
                AdmissionDateFromDim = LOOKUPVALUE(dim_date[calendar_date], dim_date[date_sk], 'fact_encounter'[admission_date_sk])
                ```
            *   Then another calculated column:
                ```dax
                EncounterYearMonth_CC = FORMAT([AdmissionDateFromDim], "YYYY-MM")
                ```
    *   **Visual 1: Table with Many Columns and the Calculated Column**
        *   Add a "Table" visual.
        *   Drag many fields into it:
            *   `dim_patient[natural_patient_id]`
            *   `dim_patient[gender_sim]`
            *   `dim_patient[age_group_sim]`
            *   `fact_encounter[natural_encounter_id]`
            *   `fact_encounter[encounter_class]`
            *   `fact_encounter[length_of_stay_days]`
            *   `fact_encounter[EncounterYearMonth_CC]` (the calculated column)
            *   `fact_encounter[provider_id]`
            *   `fact_encounter[is_readmission]`
            *   `dim_date[full_date_description]` (related via `admission_date_sk`)
    *   **Visual 2: Matrix with High Cardinality Fields**
        *   Add a "Matrix" visual.
        *   **Rows:** `dim_patient[natural_patient_id]`
        *   **Columns:** `fact_encounter[EncounterYearMonth_CC]`
        *   **Values:** `fact_encounter[natural_encounter_id]` (Count)
    *   **Visual 3: Card with Complex Measure (using the CC)**
        *   Create a new measure in `fact_encounter`:
            ```dax
            ComplexCount_CC = COUNTROWS(FILTER('fact_encounter', 'fact_encounter'[EncounterYearMonth_CC] = "2023-01"))
            ```
        *   Add a "Card" visual with this `[ComplexCount_CC]` measure.
    *   Save the report as `EncounterTrends_Slow_Report`.

**Part 2: Diagnose with Performance Analyzer**

1.  **Open Performance Analyzer:**
    *   View the `EncounterTrends_Slow_Report` in your Fabric workspace.
    *   Go to the **Optimize** tab on the Power BI ribbon (if in Desktop) or find **Performance analyzer** under the "View" tab in the Power BI service/Fabric report view.
    *   Click **Start recording**.
2.  **Interact with the Report:**
    *   Click **Refresh visuals** (on the Performance Analyzer pane).
    *   If you have slicers, interact with them.
3.  **Analyze Results:**
    *   Stop recording.
    *   The Performance Analyzer pane will show the time taken for each visual element, broken down into:
        *   **DAX Query:** Time to execute the DAX query against the model.
        *   **Visual Display:** Time to render the visual on the screen.
        *   **Other:** Time for other operations.
    *   Identify the visuals with the longest "DAX Query" times. The table and matrix with the calculated column `EncounterYearMonth_CC` and many fields are likely culprits. The card with `ComplexCount_CC` might also be slow.
    *   You can copy the DAX query for a slow visual and analyze it further in DAX Studio (external tool, optional for this lab).

**Part 3: Implement Optimizations**

1.  **Optimization 1: Convert Calculated Column to Measure / Use `dim_date`**
    *   The `EncounterYearMonth_CC` calculated column is inefficient because it's calculated row-by-row and stored, increasing model size and potentially slowing down queries that use it, especially in DirectLake if not optimally materialized.
    *   **Better Approach:** Create a `YearMonth` column in `dim_date` or use measures.
    *   **Step 3.1.1: Add YearMonth to `dim_date` (if not already there from Lab 4.8):**
        *   Go back to your Fabric Notebook used for creating `dim_date`.
        *   Modify the `dim_date` creation to include a `YearMonth` column (e.g., "YYYY-MM" format).
        ```python
        # In your dim_date creation cell from Lab 4.8, add:
        # .withColumn("year_month", date_format(col("calendar_date"), "yyyy-MM"))
        # Then re-run the cell to update dim_date table.
        # Example:
        # df_dim_date = df_dim_date.withColumn("date_sk", date_format(col("calendar_date"), "yyyyMMdd").cast("int")) \
        #                          .withColumn("year_month", date_format(col("calendar_date"), "yyyy-MM")) \ # ADD THIS
        # ... rest of the columns
        # df_dim_date.write.format("delta").mode("overwrite").option("overwriteSchema","true").saveAsTable("CardiologyLakehouse.dim_date")
        ```
        *   Refresh your `EncounterTrends_Slow_Dataset` in Fabric to pick up schema changes (usually automatic with DirectLake, but a manual refresh of the dataset can be triggered from workspace if needed for model metadata).
    *   **Step 3.1.2: Modify Report Visuals:**
        *   Open `EncounterTrends_Slow_Report` for editing.
        *   Remove the calculated column `fact_encounter[EncounterYearMonth_CC]` from all visuals.
        *   Delete the calculated columns `[AdmissionDateFromDim]` and `[EncounterYearMonth_CC]` from the `fact_encounter` table in the dataset's Data pane.
        *   Drag `dim_date[year_month]` (assuming you added it and it's related via `admission_date_sk`) to the visuals where `EncounterYearMonth_CC` was used (e.g., in the Matrix columns).
    *   **Step 3.1.3: Modify Complex Measure:**
        *   Edit the `ComplexCount_CC` measure. Instead of filtering on the calculated column, filter on `dim_date[year_month]`.
            ```dax
            ComplexCount_Optimized =
            CALCULATE(
                COUNTROWS('fact_encounter'),
                FILTER(
                    ALL('dim_date'[year_month]), -- Ensure you are filtering the date dimension
                    'dim_date'[year_month] = "2023-01"
                )
            )
            ```
        *   Replace the old measure in the Card visual with this new one.

2.  **Optimization 2: Reduce Unnecessary Fields in Visuals**
    *   Review the "Table" visual. Does it truly need all those columns displayed by default? High numbers of columns, especially those with high cardinality, can slow down rendering and DAX query generation.
    *   Remove any columns from the table visual that are not essential for the primary view. Users can use drill-through or tooltips for more details if needed.

3.  **Optimization 3: Use Measures Instead of Implicit Measures or Summarized Columns**
    *   For the Matrix visual's "Values" field, instead of dragging `fact_encounter[natural_encounter_id]` and setting it to "Count," create an explicit measure:
        ```dax
        Total Encounters = COUNTROWS('fact_encounter')
        ```
    *   Use `[Total Encounters]` in the Matrix values. Explicit measures often give Power BI more optimization opportunities.

4.  **Re-test with Performance Analyzer:**
    *   Save the report.
    *   Clear the previous Performance Analyzer recording.
    *   Start recording again and refresh visuals.
    *   Compare the new timings. You should see improvements, especially in "DAX Query" times for the modified visuals.

**Part 4: Lakehouse Table Optimization (Conceptual for Power BI Impact)**

*   While DirectLake is fast, optimizing the underlying Delta tables in the Lakehouse can further enhance performance, especially for very large tables.
1.  **Partitioning (if applicable):**
    *   If `fact_encounter` is extremely large and frequently filtered by date, partitioning it by a date-derived column (e.g., `EncounterYear`, `EncounterYearMonth`) in the Lakehouse (during its creation with Spark) can speed up queries that use those partitions as filters.
    *   *Note: For this lab, we won't re-partition as it's a more involved Spark operation, but it's an important concept.*
2.  **Run `OPTIMIZE` and `VACCUM` on Lakehouse Tables:**
    *   Compacting small files (`OPTIMIZE`) and removing old, unreferenced files (`VACUUM`) can improve read performance for Spark and, by extension, DirectLake queries.
    *   Open a Fabric Notebook:
    ```python
    # Run OPTIMIZE on key tables
    spark.sql("OPTIMIZE CardiologyLakehouse.fact_encounter")
    spark.sql("OPTIMIZE CardiologyLakehouse.dim_patient")
    spark.sql("OPTIMIZE CardiologyLakehouse.dim_date")
    print("OPTIMIZE command completed for relevant tables.")

    # Run VACUUM (be cautious with retention period in production)
    # spark.sql("VACUUM CardiologyLakehouse.fact_encounter RETAIN 168 HOURS") # Retain 7 days
    # print("VACUUM command completed for fact_encounter.")
    ```
    *   Run this cell. While the immediate impact on an already open Power BI report might not be instantly visible without a dataset refresh (for metadata) or re-query, these are good maintenance practices for the Lakehouse.

**Expected Outcome / Deliverables:**
*   An understanding of how to use Power BI Performance Analyzer to identify bottlenecks.
*   An optimized version of the `EncounterTrends_Slow_Report` with:
    *   Calculated columns replaced by measures or dimension table attributes.
    *   Reduced number of fields in some visuals.
*   Demonstrably faster load times for the report page (as shown by Performance Analyzer).
*   Knowledge of Lakehouse table maintenance commands (`OPTIMIZE`) that contribute to overall query performance.

**Questions from Manual & Answers: LINK TO HTML?**

*   **Q1: Why are DAX Measures generally preferred over Calculated Columns for aggregations or dynamic calculations in Power BI, especially for performance?**
    *   **A1:**
        *   **Calculation Timing & Storage:**
            *   **Calculated Columns:** Are computed row by row during data refresh and stored in the model. This consumes memory and increases model size. For every row, the DAX expression is evaluated and its result materialized.
            *   **Measures:** Are calculated at query time, only when they are used in a visual. They are not stored in the model, so they don't increase model size or refresh time directly.
        *   **Context Transition:** Measures are evaluated in the context of the visual or filter they are placed in. Calculated columns are evaluated in the row context of their table and do not inherently respond to report filters in the same dynamic way without further context transition in measures that use them.
        *   **Performance:**
            *   For large tables, calculated columns can significantly slow down data refresh and increase memory footprint.
            *   While complex measures can also be slow if poorly written, they are generally more efficient for aggregations because they operate on aggregated data based on the current filter context, rather than pre-calculating for every row.
            *   Calculated columns can sometimes inhibit query optimization techniques that the Power BI engine (VertiPaq) uses.
        *   **Flexibility:** Measures are more flexible as they dynamically respond to filters and slicers in the report.

*   **Q2: What are common causes of slow file scans or query performance against tables in a Fabric Lakehouse when queried by Power BI in DirectLake mode?**
    *   **A2:**
        *   **Too Many Small Files:** Delta Lake tables can accumulate many small Parquet files, especially after frequent small appends or updates. Querying many small files is less efficient than querying fewer, larger files. The `OPTIMIZE` command helps compact these.
        *   **Lack of or Ineffective Partitioning:** If large tables are not partitioned, or partitioned on columns with very high cardinality or columns not frequently used in filters, queries might have to scan much more data than necessary.
        *   **Schema Complexity:** Very wide tables (hundreds of columns) can lead to more data being read, even if only a few columns are selected, depending on how Parquet files store column stripes.
        *   **Data Skew in Partitions:** If data is partitioned, but one partition is vastly larger than others, queries hitting that partition will be slow.
        *   **Outdated Table Statistics:** The query optimizer relies on statistics about the data distribution. If statistics are stale, it might generate suboptimal query plans. Running `ANALYZE TABLE ... COMPUTE STATISTICS` can help.
        *   **Complex Predicates/Filters in Power BI:** Even with DirectLake, if the DAX queries generated by Power BI visuals involve very complex filtering logic that doesn't translate well to efficient Delta table scans (e.g., filtering on computationally intensive derived values not present in the table), performance can suffer.
        *   **Insufficient Fabric Capacity:** If the Fabric capacity allocated to the workspace is under-provisioned for the query load, queries will queue or run slowly due to resource contention.

*   **Q3: How can caching strategies, either within Power BI or at other layers, improve report rendering speed for frequently accessed reports?**
    *   **A3:**
        *   **Power BI Service Query Caching:** The Power BI service automatically caches query results for visuals. When a user opens a report, if the same query (with the same filter context) has been executed recently and the underlying data hasn't changed significantly (or the cache hasn't expired), Power BI can serve the result from its cache instead of re-querying the data source (even DirectLake). This dramatically speeds up report loading for subsequent users or visits. Cache duration varies (e.g., typically up to an hour, can be influenced by dataset refresh).
        *   **Browser Caching:** Browsers cache static assets of the Power BI report (like images, report structure), which helps in rendering the report shell faster on subsequent visits.
        *   **DirectLake and OneLake Caching:** OneLake itself might have caching layers for frequently accessed Delta/Parquet file footers or metadata, speeding up the "query" part of DirectLake. Fabric capacities also manage memory for caching data read from OneLake.
        *   **Materialized Views (in SQL Warehouse/Lakehouse):** For very complex or common aggregations that are still slow even with DirectLake, you could pre-calculate them and store them in materialized views (if using a Warehouse) or aggregated Gold tables in the Lakehouse. Power BI would then query these pre-aggregated results, which is a form of manual caching/pre-computation.
        *   **Power BI Premium Per User (PPU) / Premium Capacity Features:** These capacities offer more control over caching and performance, including potentially larger cache sizes and more aggressive caching.
        *   **Dashboard Tile Caching:** Tiles pinned to Power BI dashboards have their own caching mechanism and refresh schedule, which can provide quick views of key metrics.

        It's important to note that for DirectLake, the primary benefit is already avoiding the import model's refresh latency. Caching then further optimizes the query execution part for repeated views.

---

### 8.8 Quiz
**1. What Spark function improves performance by materializing data?**
a) collect()
b) persist()
c) printSchema()
**â†’ Answer: b**

**2. Which SQL command compacts files in Lakehouse?**
a) DELETE
b) MERGE
c) OPTIMIZE
**â†’ Answer: c**

**3. What tool helps analyze Power BI visual load time?**
a) SQL Profiler
b) Performance Analyzer
c) Report Builder
**â†’ Answer: b**

---

### 8.9 Cheat Sheet (Printable)
*   **OPTIMIZE** = Compact Delta files
*   **VACUUM** = Clean up obsolete files
*   **Repartition()** = Increase parallelism
*   **df.cache()** = Persist intermediate results
*   **EXPLAIN PLAN** = View query execution steps
*   **Materialized Views** = Improve dashboard performance
*   **DAX Measures** = Faster and lighter than calculated columns
*   **Performance Analyzer** = Power BI diagnostics tool

---

### Interactive Element for Section 8
[Interactive Performance Optimization Element](./visualizations-html/Section%208%20-%20Performance%20Optimization.html)

---

## ðŸ“˜ Section 9: Collaboration and Sharing {#section-9-collaboration-and-sharing}

---

### 9.0 Overview
In healthcare data environments, insights are rarely created in isolation. Collaboration between data engineers, analysts, clinicians, compliance officers, and operational leaders is essential. Microsoft Fabricâ€™s architecture is designed to support **secure, scalable, and governed collaboration** on data projectsâ€”while enforcing stringent data protection policies.

This section details how to manage access, coordinate across teams, share datasets and dashboards securely, and maintain compliance in collaborative workflows. It also includes hands-on configuration for real-world workspace role assignments and approval flows.

---

### 9.1 Principles of Secure Collaboration in Healthcare
#### 9.1.1 Patient 360 Data Integration Flow
<figure>
  <img src="./images/9.1.1-diagram_Data%20integration%20flow%20for%20building%20a%20Patient%20360%20view%20using%20Microsoft%20Fabric.png" alt="Patient 360 Data Integration Flow" style="width:100%;" />
  <figcaption><em>Figure: Data integration flow for building a Patient 360 view using Microsoft Fabric.</em></figcaption>
</figure>

#### A. Principle of Least Privilege
Only the minimum access necessary for a userâ€™s role should be granted. This limits risk exposure and ensures HIPAA compliance.

#### B. Segregation of Duties
Preventing data manipulation and analysis by the same individual helps maintain data integrity.

#### C. Role-Based Collaboration
| Role | Primary Task | Typical Tools Used |
| :--- | :--- | :--- |
| Data Engineer | Pipeline development, Lakehouse modeling | Notebooks, Dataflows |
| Analyst | Report creation, data exploration | Power BI, Dataflows |
| Clinician Leader | Interpretation, clinical insight validation | Power BI |
| Compliance Officer | Auditing, access review | Purview, Audit Logs |

#### Patient 360 Data Integration Flow (Example of Collaborative Data Product)
<figure>
  <img src="./images/9.1.2-diagram-Patient%20360%20Data%20Integration%20Flow%20%28Example%20of%20Collaborative%20Data%20Product%29.png" alt="Patient 360 Data Integration Flow - Example of Collaborative Data Product" style="width:100%;" />
  <figcaption><em>Figure: Patient 360 Data Integration Flow (Example of Collaborative Data Product).</em></figcaption>
</figure>

---

### 9.2 Fabric Workspace Structure for Team Collaboration
Fabric workspaces are logical containers for collaboration.

#### Workspace Elements:
*   **Lakehouse**: Shared storage for raw and processed data
*   **Pipelines**: Orchestrate ingestion and transformation
*   **Notebooks**: Enable code-based collaboration
*   **Power BI Reports**: Interactive data sharing
*   **Dataflows/Items**: Reusable logic and visual prep

#### Best Practices:
*   Name workspaces clearly: `Cardiology_Analytics_DEV`, `ED_Operational_Metrics_PROD`
*   Assign team-specific permissions
*   Use **Dev-Test-Prod** workspace promotion

---

### 9.3 Assigning Roles in a Workspace
| Role | Permissions |
| :--- | :--- |
| Admin | Full control, including role assignments |
| Member | Can edit content, cannot manage roles |
| Contributor | Can add items (e.g., notebooks) |
| Viewer | Read-only access |

#### Steps to Assign Roles:
1.  Open Fabric â†’ Select Workspace
2.  Click â€œAccessâ€ â†’ â€œAdd Userâ€
3.  Choose role per team member
4.  Review permissions quarterly

---

### 9.4 Collaboration with Notebooks
Fabric notebooks support multi-user editing and sharing.

#### Features:
*   Share notebook links within teams
*   Use version control for edits
*   Add Markdown comments and instructions
*   Share outputs with analysts (dataframes, plots)

> ðŸ“Œ Do not embed PHI directly in shared notebooks unless access is governed and logged.

---

### 9.5 Sharing Power BI Reports Securely
#### Options:
*   Publish to Workspace â†’ Limit viewers with RLS
*   Share via Power BI app â†’ Assign viewer group
*   Embed securely in EHR portal or SharePoint
*   Export to PDF â†’ Only for de-identified data

#### HIPAA-Safe Sharing Practices:
*   Use **Sensitivity Labels**: "HIPAA-HIGH", "Confidential"
*   Enable **Data Loss Prevention (DLP)**: Block unauthorized export
*   Log all sharing activity in **Purview Audit**

---

### 9.6 Approval Flows and Access Reviews
#### Microsoft Entra (Azure AD):
*   Set **access expiration policies**
*   Create **approval workflows** for workspace roles
*   Automate **quarterly access reviews** for compliance
*   Send review reports to Compliance Officers

---

### 9.7 Data Sharing Outside the Organization
#### Internal Collaboration
*   Use Azure B2B guest access for trusted collaborators
*   Enforce:
    *   MFA
    *   Session limits
    *   Conditional access

#### External Sharing Risks
*   Never share PHI outside without legal, contractual, and compliance approvals
*   Encrypt data at source and apply governance policies
*   Use **data de-identification pipelines** prior to export

---

### 9.8 Lab: Configure Collaboration Settings for a New Project Workspace
**Module Alignment:** Section 9: Collaboration and Sharing

**Objective:**
*   Create a new Microsoft Fabric workspace tailored for a specific project.
*   Assign different workspace roles (Admin, Member, Contributor, Viewer) to team members based on their project responsibilities, demonstrating the principle of least privilege.
*   Understand how to share specific Fabric items (e.g., a Power BI report) with appropriate permissions.
*   Apply a sensitivity label to a shared item to govern its usage.
*   (Conceptual) Understand how access reviews can be configured for compliance.

**Scenario:**
Valley General Hospital is initiating a new project to analyze Emergency Department (ED) efficiency. A cross-functional team has been assembled, including data engineers, data analysts, and ED clinical leads. You need to set up a dedicated Fabric workspace for this project, ensuring each team member has the appropriate level of access to collaborate effectively while adhering to data governance policies.

**Prerequisites:**
*   Microsoft Fabric enabled Microsoft 365 tenant.
*   Permissions to create workspaces in Fabric.
*   A few sample user email addresses (you can use your own, test accounts, or colleagues' emails if they are part of your M365 tenant and you have permission to add them for testing purposes).
*   A sample Power BI report (can be a simple one created for this lab or one from a previous lab).
*   Sensitivity labels (e.g., "Confidential - Internal Use" or "HIPAA-HIGH") configured in Microsoft Purview and available in Fabric.

**Tools to be Used:**
*   Microsoft Fabric Portal (Workspace creation and management)
*   Power BI (for sharing a report item)

**Estimated Time:** 45 - 60 minutes

**Tasks / Step-by-Step Instructions:**

**Part 1: Create a New Project Workspace**

1.  **Navigate to Microsoft Fabric:** Open `app.fabric.microsoft.com`.
2.  **Create New Workspace:**
    *   Click on **Workspaces** in the left navigation pane.
    *   Click **+ New workspace**.
    *   **Name:** `ED_Efficiency_Project_Q2` (or a similar descriptive name indicating project and timeframe).
    *   **Description:** (Optional) "Workspace for the Q2 Emergency Department Efficiency Analysis Project. Contains ED patient flow data, staffing data, and performance dashboards."
    *   **Domain:** (Optional) Assign to a relevant domain (e.g., "Clinical Analytics," "Operational Improvement").
    *   **Capacity:** Assign the workspace to a Fabric capacity.
    *   Click **Apply**.
    *   Your new workspace `ED_Efficiency_Project_Q2` is now created.

**Part 2: Assign Workspace Roles to Team Members**

*   For this part, you'll simulate adding team members with different roles. Replace the placeholder email addresses with actual test user emails if you have them.

1.  **Access Workspace Management:**
    *   Open the `ED_Efficiency_Project_Q2` workspace.
    *   In the top right corner of the workspace view, click on **Manage access**.
2.  **Assign Roles:**
    *   **Data Engineering Lead (Admin):**
        *   Click **+ Add people or groups**.
        *   **Enter name or email:** `data.engineer.lead@valleygeneral.org` (replace with a real test email or your own).
        *   **Role:** Select **Admin**. (Admins have full control, can manage content, settings, and access).
        *   Click **Add**.
    *   **Data Analyst (Contributor):**
        *   Click **+ Add people or groups**.
        *   **Enter name or email:** `data.analyst.ed@valleygeneral.org` (replace).
        *   **Role:** Select **Contributor**. (Contributors can create, edit, and delete content like reports, datasets, notebooks, and dataflows. They can publish reports. They cannot manage workspace settings or access for others).
        *   Click **Add**.
    *   **ED Clinical Lead / Manager (Viewer):**
        *   Click **+ Add people or groups**.
        *   **Enter name or email:** `ed.manager@valleygeneral.org` (replace).
        *   **Role:** Select **Viewer**. (Viewers can view and interact with reports and dashboards but cannot edit content or see underlying datasets/dataflows unless explicitly shared with build permissions).
        *   Click **Add**.
    *   **(Optional) Another Data Engineer (Member):**
        *   Click **+ Add people or groups**.
        *   **Enter name or email:** `junior.data.engineer@valleygeneral.org` (replace).
        *   **Role:** Select **Member**. (Members can do most things Contributor can, plus publish Power BI apps and share content. They still cannot manage workspace access).
        *   Click **Add**.
3.  **Review Assigned Roles:**
    *   In the "Manage access" pane, you should now see the list of users and their assigned roles for the workspace.

**Part 3: Share a Specific Power BI Report with Row-Level Security (RLS)**

*   Assume you have a Power BI report in this workspace (e.g., `ED_Performance_Dashboard`) that has RLS configured (e.g., a "DepartmentalView" role that filters data by department). If you don't have one, you can quickly create a dummy report or use one from a previous lab and imagine RLS is set up.

1.  **Create or Identify a Sample Report:**
    *   If you don't have a report in `ED_Efficiency_Project_Q2`, quickly create one:
        *   Go to the workspace, click **+ New -> Report**.
        *   Choose "Pick a published dataset" (if you have one) or "Paste or manually enter data" to create a very simple report with one visual.
        *   Save it as `ED_Department_Summary_Report`.
2.  **Share the Report with Specific Permissions:**
    *   In the `ED_Efficiency_Project_Q2` workspace, find your `ED_Department_Summary_Report`.
    *   Click the three dots (**...**) next to the report name.
    *   Select **Share**.
    *   In the "Share report" dialog:
        *   **Enter name or email:** `specific.stakeholder@valleygeneral.org` (a user who is NOT already a member of the workspace with full view access, or a Viewer who needs build permissions on this specific report's dataset).
        *   **Allow recipients to share this report:** Uncheck this for tighter control.
        *   **Allow recipients to build content with the data associated with this report:** Check this if you want this specific stakeholder to be able to create their own reports using the underlying dataset (grants "Build" permission on the dataset). For a clinical lead who might want to explore, this could be useful.
        *   **Send an email notification:** Optional.
        *   Click **Grant access**.
    *   This demonstrates sharing a specific item, potentially with different permissions than the user's general workspace role.

3.  **Apply Sensitivity Label to the Report:**
    *   Open the `ED_Department_Summary_Report`.
    *   Go to **File -> Sensitivity label**.
    *   Choose an appropriate label, for example, "Confidential - Internal Use" or "HIPAA-HIGH" if it contains PHI (even aggregated).
    *   Click **Apply**.

**Part 4: (Conceptual) Configure Quarterly Access Reviews**

*   Access reviews are typically configured in **Microsoft Entra ID (Azure AD) Privileged Identity Management (PIM)** or through **Entra ID Access Reviews** features, often by an Identity Administrator or Compliance Administrator, not directly within the Fabric workspace UI by a data engineer.

1.  **Understanding the Process:**
    *   **Purpose:** To regularly review who has access to sensitive resources (like your Fabric workspace or specific roles within it) and ensure that access is still necessary and appropriate. This is a key compliance control.
    *   **Setup (Admin Task in Entra ID):**
        *   An administrator would go to the Microsoft Entra admin center.
        *   Navigate to "Identity Governance" -> "Access Reviews."
        *   Create a new access review.
        *   **Scope:** Define what is being reviewed (e.g., members of an AAD group that has been granted access to the Fabric workspace, or direct assignments to the workspace if supported for review).
        *   **Reviewers:** Assign who will perform the review (e.g., the workspace owner like `data.engineer.lead@valleygeneral.org`, or their manager).
        *   **Frequency:** Set it to occur quarterly.
        *   **Actions upon completion:** Define what happens if access is denied during the review (e.g., access is automatically removed).
    *   **Performing the Review:**
        *   When the review period starts, the assigned reviewers receive notifications.
        *   They go to the "My Access" portal or Entra ID to approve or deny access for each user/group in scope.

2.  **Your Role as Workspace Admin/Data Engineer:**
    *   You might be assigned as a reviewer for your workspace.
    *   You need to understand the importance of these reviews and participate diligently to maintain compliance.

**Expected Outcome / Deliverables:**
*   A new Fabric workspace named `ED_Efficiency_Project_Q2` is created.
*   Simulated team members are assigned appropriate workspace roles (Admin, Contributor, Viewer).
*   A Power BI report within the workspace is shared with a specific user, potentially with different permissions than their workspace role.
*   The shared Power BI report has a sensitivity label applied.
*   A conceptual understanding of how quarterly access reviews are set up and their importance for governance.

**Questions from Manual & Answers:**

*   **Q1: Why is it generally better to assign analysts the "Contributor" role rather than the "Member" role if their primary job is to create reports and datasets but not manage workspace access or publish apps?**
    *   **A1:** The "Contributor" role aligns more closely with the principle of least privilege for analysts whose main tasks are content creation and editing.
        *   **Contributors can:** Create, edit, and delete content they have access to (reports, datasets, dataflows, notebooks), and publish reports to the workspace. This is usually sufficient for an analyst's development tasks.
        *   **Contributors cannot:** Manage workspace access for other users, modify workspace settings, or publish/manage Power BI Apps for the workspace.
        *   **Members can** do everything a Contributor can, PLUS they can publish Power BI apps and share content more broadly within the workspace context (e.g., update an app).
        *   By assigning "Contributor," you limit the potential for analysts to inadvertently change workspace settings, manage permissions incorrectly, or impact the distribution of content via Apps if that's not part of their designated responsibilities. It provides a safer scope for their work.

*   **Q2: Which Microsoft Purview tool or Fabric feature is primarily used to verify who has accessed or modified specific data items or reports within a workspace?**
    *   **A2:** The **Microsoft Purview Audit log** (accessed via the Microsoft Purview compliance portal) is the primary tool for verifying who has accessed or modified specific data items (like Lakehouse tables, datasets) or reports within a Fabric workspace.
        *   Fabric activities are logged to the unified audit log. Administrators or compliance officers can search these logs for activities related to specific Fabric items, users, and timeframes.
        *   While the Fabric Monitoring Hub shows operational logs for pipeline runs and Spark jobs, the Purview Audit log is the authoritative source for compliance-related access and modification tracking.

*   **Q3: How do Sensitivity Labels affect the ability to share Fabric content (like a Power BI report) externally or download it?**
    *   **A3:** Sensitivity Labels themselves are primarily for classification and visual marking. Their direct effect on sharing and downloading is determined by **Data Loss Prevention (DLP) policies** and **Conditional Access policies** that are configured (usually in Microsoft Purview and Azure AD) to act upon these labels:
        *   **Trigger for DLP Policies:** If a DLP policy is in place that targets a specific sensitivity label (e.g., "HIPAA-HIGH"), it can:
            *   **Block external sharing:** Prevent users from sharing reports or datasets labeled "HIPAA-HIGH" with users outside the organization.
            *   **Block download:** Prevent users from downloading the report or its data to unmanaged/personal devices.
            *   **Audit actions:** Log attempts to share or download, even if not blocked.
            *   **Display policy tips:** Warn users about the sensitivity of the data when they attempt certain actions.
        *   **Inform Conditional Access Policies:** Azure AD Conditional Access policies can potentially use information about the sensitivity of data being accessed (though this is more common for SharePoint/OneDrive currently) to enforce stricter access controls (e.g., requiring MFA, blocking access from untrusted networks if a user is trying to access a "Highly Confidential" report).
        *   **User Awareness:** The label itself makes users more aware of the data's sensitivity, potentially making them more cautious about sharing or downloading it.
        *   **Inheritance:** Labels can be inherited from datasets to reports, and if content is exported (e.g., to Excel), the label and any associated encryption/protection can persist.

        So, while the label itself is a tag, its power to restrict sharing/download comes from the associated governance policies that reference it.

---

### 9.9 Quiz
**1. What is the most restrictive Fabric role?**
a) Admin
b) Viewer
c) Member
**â†’ Answer: b**

**2. What principle requires giving users only the access they need?**
a) Separation of Duties
b) Privileged Escalation
c) Least Privilege
**â†’ Answer: c**

**3. Which tool supports version control and collaboration in Fabric notebooks?**
a) Power BI
b) Azure DevOps
c) Native Fabric Notebooks
**â†’ Answer: c**

---

### 9.10 Cheat Sheet (Printable)
*   **Least Privilege** = Only access whatâ€™s required
*   **RBAC** = Fabric role-based workspace permissions
*   **Notebooks** = Shared development with version history
*   **Power BI** = Share securely with RLS and labels
*   **Sensitivity Labels** = "HIPAA-HIGH", "Confidential"
*   **DLP Policies** = Block export/share of PHI
*   **Purview Audit Logs** = Track all data actions
*   **Access Reviews** = Quarterly compliance validation

---

### Interactive Element for Section 9
[Interactive Collaboration & Sharing Element](./visualizations-html/Section%209%20-%20Collaboration%20and%20Sharing.html)

---

## ðŸ“˜ Section 10: Advanced Features and Customization {#section-10-advanced-features-and-customization}

---

### 10.0 Overview
Once foundational systems are in place, the true power of Microsoft Fabric lies in its **advanced capabilities and customization options**. These allow healthcare data engineers to extend the platform across external tools, develop complex integrations, and tailor environments to unique organizational workflowsâ€”all without sacrificing security, performance, or compliance.

This section details how to use advanced Fabric features such as OneLake shortcuts, custom visuals, APIs, and Git integration to elevate and personalize your healthcare data ecosystem.

---

### 10.1 Extending OneLake with Shortcuts
OneLake allows users to create **shortcuts to external storage systems**, making data instantly accessible in Fabric without duplication.

#### Use Cases:
| Source System | Shortcut Purpose |
| :--- | :--- |
| Azure Data Lake Gen2 | Access archived EHR batch files |
| Amazon S3 | Pull research data from genomic labs |
| Google Cloud Storage | Collaborate with external research partners |

#### How to Create a Shortcut:
1.  Open Fabric Workspace â†’ Lakehouse
2.  Click â€œNewâ€ â†’ â€œShortcutâ€
3.  Select source: ADLS, S3, or GCS
4.  Authenticate (service principal or SAS)
5.  Map to logical folder

> ðŸ“Œ Shortcuts maintain source permissions and donâ€™t ingest data physically.

---

### 10.2 Customizing Power BI for Healthcare
Power BI supports **theme customization, branding, and advanced visuals** tailored to healthcare.

#### A. Themes
Customize reports with:

*   Color palettes matching hospital branding
*   Font consistency
*   Header/footer templates

#### B. Custom Visuals
Use industry-specific visuals:

*   **Heatmaps** (e.g., ER volume by hour)
*   **Timeline slicers** (for longitudinal care plans)
*   **KPI indicators** (color-coded thresholds)

> ðŸ’¡ Access AppSource for healthcare-certified visuals.

#### C. Embedding in Clinical Workflows
*   Use **iframe integration** to embed Power BI in EHR portals
*   Auto-filter dashboards based on clinician credentials or department

---

### 10.3 API Access and Automation
Fabric exposes APIs to **orchestrate, monitor, and extend** system components.

#### Common APIs:
| API | Purpose |
| :--- | :--- |
| Power BI REST API | Automate report deployment |
| Fabric Pipeline API | Trigger pipeline runs |
| OneLake File API | Programmatically manage data |
| Azure ML SDK | Deploy and monitor models |

#### Sample: Trigger Pipeline from External App
```
POST https://api.fabric.microsoft.com/pipelines/{pipelineId}/run
Headers: Authorization: Bearer <token>
```

> ðŸ”’ Authenticate using service principals or managed identities.

---

### 10.4 Webhooks and Event Integration
Fabric supports **event-driven architectures** using Event Grid or Logic Apps.

#### Use Cases:
*   Notify when patient volume exceeds threshold
*   Trigger model re-training on new data arrival
*   Automate compliance audit after data load

#### Example:
1.  Create Event Grid subscription on OneLake path
2.  Route to Logic App â†’ Send email alert
3.  Log event in audit table

---

### 10.5 Integration with Git and Version Control
Fabric supports GitHub and Azure DevOps integration for:

*   Notebook versioning
*   Power BI report control
*   CI/CD deployment of pipelines

#### Steps:
1.  Link Fabric workspace to Git repository
2.  Commit changes via portal or Git CLI
3.  Create pull requests and code reviews
4.  Deploy via pipelines or release agents

> ðŸ›¡ï¸ Git integration is critical for reproducibility and traceability.

---

### 10.6 Connecting with Dataverse and Power Platform
Dataverse (used in Dynamics 365) can serve as a **source or target** for Fabric data.

#### Use Case:
*   Read Care Coordination tasks from Dataverse
*   Enrich with Fabric analytics
*   Write insights back for use in Power Apps

#### Integration:
*   Use **Dataflow Gen2** with Dataverse connector
*   Apply role filters for PHI protection

---

### 10.7 Leveraging Azure Cognitive Services
Enhance Fabric with AI services:

| Service | Healthcare Use |
| :--- | :--- |
| Text Analytics | Extract keywords from physician notes |
| Translator | Localize patient instructions |
| OCR | Extract values from scanned documents |

> ðŸ“Œ De-identify data before sending to external cognitive APIs.

---

### 10.8 Lab: Customize a Workspace and Integrate External Data via OneLake Shortcuts
**Module Alignment:** Section 10: Advanced Features and Customization

**Objective:**
*   Create a OneLake shortcut to an external data source (simulated as another path within your OneLake, or conceptually an Azure Data Lake Storage Gen2 account).
*   Ingest and transform data accessed via the shortcut into the Silver layer of a Fabric Lakehouse.
*   Build a Power BI report using this integrated data, applying custom branding/themes.
*   Apply a sensitivity label to the dataset and report.
*   (Conceptual) Understand how webhooks could be used to notify upon new data arrival in the external source.

**Scenario:**
Valley General Hospital's oncology department collaborates with an external research institute that stores de-identified clinical trial data in their own Azure Data Lake Storage Gen2 (ADLS Gen2) account. The oncology team needs to analyze this trial data alongside their internal patient data. You are tasked with creating a OneLake shortcut to this external data, integrating it into the `CardiologyLakehouse` (we'll use this existing Lakehouse for simplicity, though in reality, an `OncologyLakehouse` might be more appropriate), and creating a themed Power BI report.

**Prerequisites:**
*   Microsoft Fabric Workspace (e.g., `DEV_CardiologyAnalytics`).
*   Microsoft Fabric Lakehouse (e.g., `CardiologyLakehouse`).
*   Permissions to create shortcuts in your Lakehouse.
*   (For a real ADLS Gen2 shortcut) An actual ADLS Gen2 account with a container and sample data, and appropriate credentials (e.g., account key, SAS token, or service principal with permissions).
    *   **For this lab, we will simulate the "external" source by creating data in a different folder path within your existing OneLake/Lakehouse to avoid needing external Azure resources. The shortcut creation process is similar.**
*   A sample Power BI report or the ability to create one.
*   Sensitivity labels available in Fabric.

**Tools to be Used:**
*   Microsoft Fabric Lakehouse (Shortcuts, SQL Analytics Endpoint)
*   Microsoft Fabric Notebook (PySpark for data preparation and transformation)
*   Microsoft Fabric Power BI (for report creation and theming)

**Estimated Time:** 75 - 90 minutes

**Tasks / Step-by-Step Instructions:**

**Part 1: Simulate and Prepare "External" Data Source in OneLake**

1.  **Open or Create a Notebook:**
    *   In your `DEV_CardiologyAnalytics` workspace, open an existing notebook or create a new one (e.g., `ExternalData_Integration_Oncology`).
    *   Ensure your `CardiologyLakehouse` is attached.

2.  **Create Sample "External" Clinical Trial Data in a Separate Lakehouse Path:**
    *   This simulates data residing in an external ADLS Gen2. We'll write it to a distinct folder within the `Files` section of your `CardiologyLakehouse`.
    ```python
    from pyspark.sql import Row
    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType

    # Define schema for the simulated clinical trial data
    schema_trial_data = StructType([
        StructField("TrialID", StringType(), False),
        StructField("ExternalPatientID", StringType(), False), # De-identified patient ID from research institute
        StructField("EnrollmentDate", DateType(), True),
        StructField("TreatmentArm", StringType(), True), # e.g., 'StandardCare', 'InvestigationalDrugA'
        StructField("AgeAtEnrollment", IntegerType(), True),
        StructField("ResponseMetric", DoubleType(), True), # e.g., tumor size reduction %
        StructField("AdverseEventReported", StringType(), True) # 'Yes' or 'No'
    ])

    # Sample data
    trial_data = [
        Row(TrialID="ONC001", ExternalPatientID="RSRCH_PAT_101", EnrollmentDate="2022-01-15", TreatmentArm="InvestigationalDrugA", AgeAtEnrollment=65, ResponseMetric=0.25, AdverseEventReported="No"),
        Row(TrialID="ONC001", ExternalPatientID="RSRCH_PAT_102", EnrollmentDate="2022-02-01", TreatmentArm="StandardCare", AgeAtEnrollment=70, ResponseMetric=0.10, AdverseEventReported="Yes"),
        Row(TrialID="ONC001", ExternalPatientID="RSRCH_PAT_103", EnrollmentDate="2022-01-20", TreatmentArm="InvestigationalDrugA", AgeAtEnrollment=58, ResponseMetric=0.35, AdverseEventReported="No"),
        Row(TrialID="ONC002", ExternalPatientID="RSRCH_PAT_201", EnrollmentDate="2022-03-10", TreatmentArm="InvestigationalDrugB", AgeAtEnrollment=62, ResponseMetric=0.15, AdverseEventReported="Yes")
    ]

    df_simulated_external_trial_data = spark.createDataFrame(trial_data, schema_trial_data)

    # Define a path within your Lakehouse Files to simulate the external location
    # This path will be the target for our OneLake shortcut later.
    simulated_external_path = "Files/SimulatedExternalResearchData/ClinicalTrials/oncology_trial_data_source" # Using Parquet format

    df_simulated_external_trial_data.write.format("parquet").mode("overwrite").save(simulated_external_path)
    
    print(f"Simulated external clinical trial data saved to: {simulated_external_path}")
    display(df_simulated_external_trial_data)
    ```    *   Run the cell. This creates Parquet files in the specified `Files` path within your `CardiologyLakehouse`. This path (`SimulatedExternalResearchData/ClinicalTrials/oncology_trial_data_source`) will act as our "external ADLS Gen2" source for the shortcut.

**Part 2: Create a OneLake Shortcut to the "External" Data**

1.  **Navigate to your Lakehouse:** Open `CardiologyLakehouse`.
2.  **Create a New Shortcut:**
    *   In the Lakehouse explorer view, under `Tables` or `Files` (location doesn't strictly matter for where you initiate, but it will appear as a folder-like item), click the three dots (**...**) next to `Files` (or the root of the Lakehouse name on the left pane).
    *   Select **New shortcut**.
3.  **Configure the Shortcut:**
    *   In the "New shortcut" dialog:
        *   **Select data source type:** Choose **Microsoft OneLake**.
            *   *(If this were a real external ADLS Gen2, you would select "Azure Data Lake Storage Gen2". The configuration steps would then ask for Account URL, authentication method (Account Key, SAS, Service Principal, Org Account), etc.)*
        *   **Shortcut name:** `External_Oncology_Trials`
        *   **Connection:** Since we chose "Microsoft OneLake", it will ask for the path within OneLake.
            *   You need to navigate to the path where you saved the simulated external data.
            *   Click **Browse**.
            *   Select your current workspace (`DEV_CardiologyAnalytics`).
            *   Select your Lakehouse (`CardiologyLakehouse`).
            *   Navigate into the `Files` directory, then `SimulatedExternalResearchData`, then `ClinicalTrials`.
            *   Select the folder `oncology_trial_data_source` (this is the folder containing the Parquet files).
            *   Click **Select**.
        *   The "Path" field should now be populated (e.g., `DEV_CardiologyAnalytics.CardiologyLakehouse/Files/SimulatedExternalResearchData/ClinicalTrials/oncology_trial_data_source`).
    *   Click **Create**.
    *   You should now see `External_Oncology_Trials` listed in your Lakehouse explorer (likely under `Files` or as a top-level item depending on where you initiated it). It will have a different icon indicating it's a shortcut.
---
**Part 3: Load and Transform Data from the Shortcut into Silver Layer**

1.  **Access Shortcut Data in Notebook:**
    *   Go back to your `ExternalData_Integration_Oncology` notebook (or create a new one).
    *   In a PySpark cell, read the data from the shortcut. The shortcut path in Fabric is typically `[LakehouseName]/[ShortcutName]`.
    ```python
    # Path to the shortcut within the Lakehouse context
    # The shortcut itself points to the 'oncology_trial_data_source' folder which contains parquet files.
    shortcut_path_in_lakehouse = "CardiologyLakehouse.External_Oncology_Trials" 
    # Note: When reading, Spark needs the path to the actual data files, not just the shortcut name if it's a folder.
    # OneLake paths for shortcuts are usually like: /<WorkspaceName>/<LakehouseName>.Lakehouse/<ShortcutName>
    # For direct Spark access, you might need the full OneLake path:
    # For this lab, since the shortcut points to a folder of parquet files, we can read it as such.
    # If the shortcut was to a specific file, the path would be direct to that file.
    # If the shortcut was to a table in another Lakehouse, you'd use spark.read.table("OtherLakehouse.ShortcutToTable")

    # Let's try reading the shortcut as if it's a directory of parquet files
    # The path for Spark will be relative to the Lakehouse root if the shortcut is at the root,
    # or relative to Files/Tables if it's under them.
    # OneLake path for the shortcut (if shortcut is at Lakehouse root):
    # /External_Oncology_Trials (this is the folder of parquet files)
    
    # Correct path for reading data via shortcut (assuming shortcut is at Lakehouse root or under Files)
    # The table access via SQL endpoint would be `CardiologyLakehouse`.`External_Oncology_Trials` if it was a table shortcut.
    # For a file/folder shortcut, we use the path.
    # The shortcut "External_Oncology_Trials" in the Lakehouse explorer points to the folder containing Parquet files.
    # So, we can read this folder.
    
    try:
        # The shortcut 'External_Oncology_Trials' itself represents the folder containing parquet files.
        df_trial_data_from_shortcut = spark.read.format("parquet").load(f"CardiologyLakehouse/External_Oncology_Trials")
        
        df_trial_data_from_shortcut.printSchema()
        display(df_trial_data_from_shortcut.limit(5))

        # Perform transformations (e.g., rename columns, add audit columns)
        df_silver_oncology_trials = df_trial_data_from_shortcut.select(
            col("TrialID").alias("trial_id"),
            col("ExternalPatientID").alias("external_patient_id"),
            col("EnrollmentDate").alias("enrollment_date"),
            col("TreatmentArm").alias("treatment_arm"),
            col("AgeAtEnrollment").alias("age_at_enrollment"),
            col("ResponseMetric").alias("response_metric"),
            col("AdverseEventReported").alias("adverse_event_reported"),
            lit("OncologyResearchShortcut").alias("source_system"),
            current_timestamp().alias("silver_load_timestamp")
        )

        df_silver_oncology_trials.printSchema()
        display(df_silver_oncology_trials.limit(5))

        # Write to a Silver layer table in your CardiologyLakehouse
        df_silver_oncology_trials.write.format("delta").mode("overwrite").saveAsTable("CardiologyLakehouse.silver_oncology_trials")
        print("silver_oncology_trials table created successfully from shortcut data.")

    except Exception as e:
        print(f"Error reading from shortcut or processing data: {e}")
        print("Ensure the shortcut 'External_Oncology_Trials' points to the Parquet files directory.")
        print("The path for spark.read.load() should be like 'LakehouseName/ShortcutName' if the shortcut is a folder of files.")

    ```
    *   Run the cell. This reads the data *through the shortcut* (without copying it into your primary Lakehouse storage for the shortcut itself), transforms it, and saves it as a new Delta table `silver_oncology_trials` in your `CardiologyLakehouse`.

**Part 4: Build a Power BI Report with Custom Theming**

1.  **Create a New Power BI Dataset:**
    *   In `CardiologyLakehouse`, click **New Power BI dataset**.
    *   Select the new `silver_oncology_trials` table. You might also select `dim_patient` if you plan to (conceptually) link internal patients to external trial IDs later. For now, just `silver_oncology_trials` is fine.
    *   Click **Confirm**.
    *   Rename the dataset to `OncologyTrialAnalytics_Dataset`.
2.  **Create a New Report:**
    *   With `OncologyTrialAnalytics_Dataset` open, click **Create report -> Start from scratch**.
3.  **Add Visuals:**
    *   **Bar Chart: Average Response Metric by Treatment Arm:**
        *   X-axis: `silver_oncology_trials[treatment_arm]`
        *   Y-axis: `silver_oncology_trials[response_metric]` (set aggregation to Average)
    *   **Pie Chart: Adverse Event Reported:**
        *   Values: `silver_oncology_trials[external_patient_id]` (Count)
        *   Legend: `silver_oncology_trials[adverse_event_reported]`
    *   **Table: Trial Details:**
        *   Include columns like `trial_id`, `external_patient_id`, `enrollment_date`, `treatment_arm`, `response_metric`.
4.  **Apply Custom Theming:**
    *   In the Power BI report editing view (within Fabric):
        *   Go to the **View** tab on the ribbon.
        *   In the "Themes" group, click the dropdown arrow.
        *   You can select a built-in theme.
        *   **To customize further (or import a theme JSON):**
            *   Click **Browse for themes**. If you have a JSON theme file (often created in Power BI Desktop or downloaded), you can import it.
            *   Alternatively, click **Customize current theme**.
            *   In the "Customize theme" dialog:
                *   **Name and colors:** Change primary colors to match Valley General Hospital's branding (e.g., a specific blue and green).
                *   **Text:** Adjust font families, sizes, and colors for titles, cards, KPIs, and tab headers.
                *   **Visuals:** Customize background, border, header, and tooltip settings for visuals.
                *   **Page:** Set page background.
                *   **Filter pane:** Customize the look of the filter pane.
            *   Click **Apply**.
    *   Observe how your report visuals update with the new theme.
5.  **Save the Report:**
    *   Click **File -> Save**.
    *   Name: `Oncology Clinical Trial Insights`.
    *   Ensure it's saved to your `DEV_CardiologyAnalytics` workspace.

**Part 5: Apply Sensitivity Label**

1.  **Label the Dataset and Report:**
    *   Navigate to `OncologyTrialAnalytics_Dataset` in your workspace, go to **Settings**, and apply an appropriate sensitivity label (e.g., "Confidential - Research Data" or "HIPAA-HIGH" if it contains any re-identifiable linked data, though this scenario implies de-identified external IDs).
    *   Open the `Oncology Clinical Trial Insights` report and ensure the label is applied or apply it via **File -> Sensitivity label**.

**Part 6: (Conceptual) Webhook for New Data Notification**

*   If the external research institute's ADLS Gen2 had an Azure Event Grid subscription, it could publish an event whenever new trial data files are added to their `oncology_trial_data_source` folder.
*   **How it would work:**
    1.  **Event Grid on External ADLS Gen2:** The research institute configures Event Grid on their storage account to monitor for "Blob Created" events in the specific data path.
    2.  **Event Subscription:** They create an event subscription that sends these events to an endpoint you control. This endpoint could be:
        *   An **Azure Function** or **Logic App** in your Azure subscription.
    3.  **Action upon Event:**
        *   The Azure Function/Logic App receives the event (which includes the path to the new file).
        *   It could then:
            *   Trigger your Fabric Data Factory pipeline (via its REST API) that refreshes the `silver_oncology_trials` table by re-reading the shortcut.
            *   Send a notification (e.g., email, Teams message) to the oncology analytics team that new data is available.
            *   Log the event for auditing.
*   This creates an event-driven architecture, automating the refresh process when new external data arrives. The setup of the Event Grid and the consuming Function/Logic App is outside Fabric itself but integrates with Fabric pipelines.

**Expected Outcome / Deliverables:**
*   A OneLake shortcut (`External_Oncology_Trials`) created in `CardiologyLakehouse` pointing to the simulated external data path.
*   A Silver layer table (`silver_oncology_trials`) in `CardiologyLakehouse` populated with data read through the shortcut.
*   A Power BI report (`Oncology Clinical Trial Insights`) built on this data, featuring custom theming to match organizational branding.
*   The Power BI dataset and report are classified with an appropriate sensitivity label.
*   A conceptual understanding of how webhooks and event-driven architecture could be used to automate data refresh from external sources.

**Questions from Manual & Answers: LINK TO HTML?**

*   **Q1: What is the key benefit of using a OneLake shortcut to access data in an external storage system (like Azure Data Lake Gen2 or Amazon S3) compared to directly ingesting and copying all the data into your primary Lakehouse storage?**
    *   **A1:** The key benefit is **accessing data in place without data duplication or movement.**
        *   **No Data Duplication:** The shortcut acts as a symbolic link or pointer to the data in its original location. The data is not copied into your Fabric workspace's primary OneLake storage. This saves storage costs and avoids managing multiple copies of the same data.
        *   **Single Source of Truth:** Analytics are performed on the data residing in the external system, ensuring users are always working with the latest version from the source (unless a refresh/ingestion to a silver table is done, but the shortcut itself points to live external data).
        *   **Simplified Data Governance (for the source):** The source system maintains control and governance over its data. The shortcut respects the permissions set on the source.
        *   **Faster Access for Exploration:** Users can quickly start exploring and analyzing data from external systems via shortcuts without waiting for lengthy ETL processes to copy data.
        *   **Reduced ETL Complexity for Some Scenarios:** For direct querying or ad-hoc analysis on external data, shortcuts can simplify the initial setup. (Note: For performance or complex transformations, you might still ingest from the shortcut into Silver/Gold tables within your Lakehouse).

*   **Q2: Why is Git integration important for managing versions of Fabric items like Notebooks or Power BI report definitions (as PBIX/PBIP files)?**
    *   **A2:** Git integration is important for:
        *   **Version Control:** Tracks changes to code (Notebooks) and report definitions over time. You can see who changed what, when, and why. This is crucial for understanding the evolution of an asset.
        *   **Rollback Capabilities:** If a recent change introduces errors or undesirable behavior, you can easily revert to a previous stable version of the Notebook or report definition.
        *   **Collaboration:** Facilitates teamwork by allowing multiple developers/analysts to work on the same items, merge their changes, and resolve conflicts in a structured way using branches and pull requests.
        *   **Auditing and History:** Provides a complete history of all modifications, which is valuable for compliance, debugging, and understanding the development lifecycle.
        *   **CI/CD (Continuous Integration/Continuous Deployment):** Enables automated testing and deployment of Fabric items. Changes committed to Git can trigger pipelines that deploy Notebooks or Power BI reports to different environments (Dev, Test, Prod).
        *   **Code Reviews:** Pull request mechanisms in Git platforms (like GitHub, Azure DevOps) allow for peer review of code and report changes before they are merged into the main branch, improving quality.
        *   **Reproducibility:** Ensures that you can recreate a specific version of an analytical asset or ML model training script from a particular point in time.

*   **Q3: How can webhooks or event-driven architectures (e.g., using Azure Event Grid with Fabric) support automation and real-time responsiveness in a healthcare data platform?**
    *   **A3:** Webhooks and event-driven architectures can support automation and real-time responsiveness by:
        *   **Automated Pipeline Triggers:** When new data arrives in a source system (e.g., a new HL7 file lands in a storage account, a new FHIR resource is created, an IoT device sends a critical alert), an event can be published. A Fabric Data Factory pipeline can subscribe to this event and automatically trigger an ingestion or processing job. This eliminates the need for polling or fixed schedule-based triggers, making data available faster.
        *   **Real-time Notifications and Alerts:** Critical events (e.g., a patient's lab result exceeding a dangerous threshold, an AI model predicting high risk for a patient, a system failure) can trigger webhooks that send immediate notifications to clinicians, care teams, or IT support via email, SMS, or Teams messages.
        *   **Dynamic Resource Scaling:** Events indicating high load or processing demand could potentially trigger automation to scale up Fabric capacities or other Azure resources.
        *   **Synchronizing Systems:** When data is updated in one system, an event can trigger processes to update related data in other downstream systems or analytical models, ensuring consistency.
        *   **Triggering AI Model Retraining:** The arrival of a significant batch of new data (signaled by an event) could automatically trigger a pipeline to retrain relevant machine learning models.
        *   **Enhanced Monitoring:** Events related to pipeline failures, data quality issues, or security alerts can be routed to monitoring dashboards or incident management systems for immediate attention.
        This shifts the paradigm from batch-oriented processing to a more reactive and near real-time data ecosystem.

---

### 10.9 Quiz
**1. What is a OneLake shortcut used for?**
a) Creating pipeline templates
b) Linking to external data without ingesting
c) Embedding Power BI
**â†’ Answer: b**

**2. Which tool manages notebook versioning?**
a) PowerShell
b) Git
c) Excel
**â†’ Answer: b**

**3. What Azure service enables event-based triggers?**
a) Sentinel
b) Event Grid
c) Fabric Notebook
**â†’ Answer: b**

---

### 10.10 Cheat Sheet (Printable)
*   **OneLake Shortcut** = Connect to external cloud storage
*   **Custom Visuals** = Heatmaps, KPI indicators, slicers
*   **Power BI Themes** = Tailor look-and-feel
*   **Fabric APIs** = Automate tasks and triggers
*   **Webhooks** = Trigger actions on events
*   **Git Integration** = CI/CD and version control
*   **Dataverse** = Bi-directional sync with Dynamics
*   **Cognitive Services** = Text extraction, translation
*   **De-identify** = Always before using external APIs

---

### Interactive Element for Section 10
[Interactive Advanced Features Element](./visualizations-html/Section%2010%20-%20Advanced%20Features%20and%20Customization.html)

---

## ðŸ“˜ Section 11: Case Studies and Real-World Applications {#section-11-case-studies-and-real-world-applications}

---

### 11.0 Overview
Theoretical understanding is essentialâ€”but seeing how Microsoft Fabric functions in **real-world healthcare systems** brings the platformâ€™s potential to life. In this section, we explore practical applications of Fabric within hospitals, health systems, and clinical research institutions.

Each case study reveals how data engineers orchestrated ingestion, modeling, security, and analytics to solve tangible clinical and operational challenges. We'll analyze successes, barriers, and lessons learned to provide transferable strategies for your own deployments.

---

### 11.1 Case Study 1: Reducing Emergency Department (ED) Wait Times
#### Organization: Midwest General Hospital (1,200-bed academic medical center)
**Challenge:**
Excessive wait times in the ED due to delayed patient triage and inefficient bed allocation.

**Solution Using Fabric:**

*   **Data Ingestion:** Connected real-time HL7 ADT messages and staffing schedules using Data Factory.
*   **Modeling:** Created Gold-layer `Fact_Triage` and `Dim_Bed` tables.
*   **Analytics:** Deployed Power BI dashboard visualizing average wait by hour, staff-to-patient ratio.
*   **AI/ML:** Notebook used to predict hourly patient inflow based on seasonality and historical data.

**Outcome:**

*   ED wait times reduced by **27%**
*   Hospital throughput improved by **18%**
*   Enabled **predictive staffing model**

---

### 11.2 Case Study 2: Readmission Risk Prediction
#### Organization: Hopewell Health System (multi-hospital network in 3 states)
**Challenge:**
High rates of 30-day readmissions for CHF patients, costing $5M+ in annual penalties.

**Solution Using Fabric:**

*   **Data Integration:** Combined FHIR `Patient`, `Encounter`, and `Condition` data via REST API and loaded into OneLake.
*   **Model Development:** Logistic regression trained in Notebook using lab results, demographics, and prior visit history.
*   **Deployment:** Predictions written back to `Gold_PatientRisk`, used in daily rounding reports.
*   **Governance:** Access controlled via Purview; model performance tracked via MLflow.

**Outcome:**

*   Readmissions dropped by **19%** within 6 months
*   Achieved **HITRUST AI compliance audit**
*   Clinicians gained confidence via SHAP-based model explainability

---

### 11.3 Case Study 3: Centralizing Data for a Public Health Consortium
#### Organization: State Health Collaborative (10 hospitals, 3 universities)
**Challenge:**
Lack of a unified data platform for COVID surveillance, vaccination metrics, and case trend analysis.

**Solution Using Fabric:**

*   **OneLake Shortcuts:** Linked data across Azure, AWS, and GCP storage.
*   **Dataflows Gen2:** Unified schema across hospital systems, resolving duplicate patients via hashed MPI.
*   **Power BI Dashboards:** Public health dashboards built for state health department use.
*   **Access Review:** Quarterly governance checks using Fabricâ€™s access logs and labels.

**Outcome:**

*   Created **state-wide pandemic dashboard** in 3 weeks
*   Shared insights with **CDC and WHO**
*   Established reusable Fabric architecture for future emergencies

---

### 11.4 Case Study 4: Automating Claims Anomaly Detection
#### Organization: UrbanCare Health (Managed Care Organization)
**Challenge:**
Claims fraud and errors led to millions in unrecouped expenses.

**Solution Using Fabric:**

*   Ingested **claims feeds (X12 837)** and **enrollment files** via Data Factory.
*   Built **anomaly detection model** in PySpark Notebook.
*   Scored claims in near-real time; routed flagged claims to audit queue.
*   Used Power BI dashboard to visualize fraud patterns by NPI and CPT codes.

**Outcome:**

*   Flagged **92%** of known fraud cases
*   Recovered **$1.2M** in claims within 6 months
*   Cut manual review workload by **40%**

---

### 11.5 Themes Across All Cases
| Theme | Impact |
| :--- | :--- |
| Unified Data via OneLake | Simplified architecture, eliminated duplication |
| AI Integration | Enabled forward-looking decision making |
| Governance via Purview | Ensured compliance and visibility |
| Collaboration Across Teams | Analysts, clinicians, and engineers worked in sync |
| Speed to Insight | Built and deployed solutions in weeks, not months |

---

### 11.6 Interview Snapshots: Voices from the Field
**Data Engineer â€“ Midwest General:**
*"Fabric allowed me to prototype and productionalize pipelines in the same place without jumping through admin hoops."*

**Compliance Officer â€“ Hopewell Health:**
*"Purview gave us visibility into every datasetâ€™s lineageâ€”no more spreadsheets to track access."*

**Clinical Operations Lead â€“ UrbanCare Health:**
*"Power BI let us turn claims data into action. Our fraud team can now target exactly where to look."*

---

### 11.7 Transferable Framework: Fabric Solution Blueprint
**Use this adaptable sequence for deploying Fabric in any healthcare problem space:**

1.  **Problem Definition**
    *   Clinical? Operational? Financial?
2.  **Data Discovery**
    *   Sources, sensitivity, accessibility
3.  **Modeling Strategy**
    *   Bronze-Silver-Gold structure, schema plan
4.  **Build & Test**
    *   Pipelines, notebooks, dashboards
5.  **Compliance Review**
    *   DLP, RBAC, audit logs, classification
6.  **Rollout & Feedback**
    *   Stakeholder engagement, iteration cycle
7.  **Monitoring**
    *   Performance tuning, model drift, access reviews

---

### 11.8 Lab: Apply Case Study Architecture to Address Appointment No-Shows
**Module Alignment:** Section 11: Case Studies and Real-World Applications

**Objective:**
*   Analyze a common healthcare problem (appointment no-shows) through the lens of the architectures and solutions presented in the course case studies.
*   Design a high-level Microsoft Fabric solution to ingest relevant data, build a predictive model for no-show risk, and visualize insights for clinic schedulers.
*   Identify key Fabric components (Data Factory, Lakehouse, Notebooks, Power BI, Purview) and their roles in the proposed solution.
*   Consider data governance, collaboration, and potential challenges in implementing such a solution.

**Scenario:**
Valley General Hospital's Cardiology Clinic is experiencing a high rate of patient no-shows for scheduled appointments. This leads to wasted provider time, underutilized resources, and potential delays in patient care. The clinic manager wants to implement a data-driven solution using Microsoft Fabric to predict which appointments are at high risk of being a no-show and to understand the contributing factors.

**Prerequisites:**
*   Completion and understanding of previous labs and sections, especially those covering:
    *   Data ingestion (Lab 3.8)
    *   Data modeling (Lab 4.8)
    *   Machine learning (Lab 7.8)
    *   Power BI reporting (Lab 6.8)
    *   Collaboration and Governance (Labs 5.9, 9.8)
*   Microsoft Fabric Workspace (e.g., `DEV_CardiologyAnalytics`).
*   Microsoft Fabric Lakehouse (e.g., `CardiologyLakehouse`).
*   Ability to create and conceptually design pipelines, notebooks, and reports.

**Tools to be Used (Conceptual Design & Partial Implementation):**
*   Microsoft Fabric Data Factory (for data ingestion design)
*   Microsoft Fabric Lakehouse (for data storage and modeling design)
*   Microsoft Fabric Notebook (for ML model development design and potential feature engineering)
*   Microsoft Fabric Power BI (for dashboard design)
*   Microsoft Purview (for governance considerations)

**Estimated Time:** 90 - 120 minutes (This is a design and partial implementation lab)

**Tasks / Step-by-Step Instructions:**

**Part 1: Problem Definition and Data Discovery (Design Thinking)**

1.  **Understand the Goal:**
    *   Primary Goal: Reduce appointment no-shows in the Cardiology Clinic.
    *   Secondary Goals: Understand factors contributing to no-shows, optimize scheduling, improve resource utilization.
2.  **Identify Key Data Sources and Elements (Brainstorming):**
    *   What data would be relevant to predict no-shows? List them out.
        *   *Example Answer:*
            *   **Appointment System:** `Appointment_ID`, `Patient_ID`, `Scheduled_DateTime`, `Appointment_Type` (New, Follow-up), `Provider_ID`, `Clinic_Location`, `Lead_Time_Days` (days between booking and appointment), `Day_Of_Week`.
            *   **EHR/Patient Master:** `Patient_ID`, `Age`, `Gender`, `Zip_Code` (for distance/socioeconomic factors), `Communication_Preferences` (Email, SMS), `Insurance_Type`.
            *   **Historical Appointment Data:** `Patient_ID`, `Appointment_DateTime`, `Actual_Status` (Attended, No-Show, Cancelled), `Cancellation_Reason` (if available). This is crucial for the target variable and historical features.
            *   **(Optional) External Data:** Weather forecasts for appointment day, public transit information.
3.  **Define the Target Variable for ML:**
    *   What are you trying to predict?
        *   *Example Answer:* A binary variable `Is_NoShow` (True/False or 1/0) for each future scheduled appointment.
4.  **Consider Potential Features for the ML Model:**
    *   From the data elements above, which ones could be good predictors?
        *   *Example Answer:* History of no-shows for the patient, lead time, appointment type, day of the week, patient age, distance to clinic (derived from zip code).

**Part 2: Design the Fabric Solution Architecture (High-Level)**

1.  **Sketch a Medallion Architecture for this problem:**
    *   **Bronze Layer:** Where will raw data from the Appointment System and EHR land? What format?
        *   *Example Answer:* `bronze_appointments_raw` (from scheduling system API/DB), `bronze_patient_demographics_raw` (from EHR DB). Stored as Delta tables.
    *   **Silver Layer:** What conformed, cleansed tables will you create?
        *   *Example Answer:* `silver_appointments` (cleaned, with lead time calculated), `silver_patients` (relevant demographics), `silver_historical_attendance` (derived from past appointments, including no-show flags).
    *   **Gold Layer:** What table will be the input for your ML model and BI dashboard?
        *   *Example Answer:* `gold_appointment_features_for_ml` (aggregated features per patient/appointment), `gold_no_show_predictions` (output from ML model), `dim_provider_clinic_schedule` (for BI).
2.  **Identify Fabric Components and their Roles:**
    *   **Data Ingestion:** How will data get from source systems to Bronze? (Data Factory Pipelines, Notebooks for custom APIs).
    *   **Data Transformation (Bronze -> Silver -> Gold):** (Notebooks with PySpark, Dataflows Gen2).
    *   **Machine Learning:** (Notebooks with PySpark/Python & scikit-learn, MLflow).
    *   **Reporting/Visualization:** (Power BI).
    *   **Governance:** (Purview for lineage/classification, Fabric RBAC).
    *   **Orchestration:** (Data Factory Pipelines).

**Part 3: Partial Implementation - Data Ingestion and Silver Layer (Focus on Appointments)**

1.  **Create Sample "Appointment System" Data (Notebook):**
    *   In a Fabric Notebook (e.g., `NoShow_Prediction_Project`), simulate raw appointment data and save it to a Bronze table.
    ```python
    from pyspark.sql import Row
    from pyspark.sql.functions import col, lit, to_date, current_timestamp, datediff, expr
    from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, BooleanType

    # Schema for simulated raw appointment data
    schema_raw_appointments = StructType([
        StructField("RawAppointmentID", StringType(), False),
        StructField("PatientSystemID", StringType(), False),
        StructField("ScheduledTimestamp", TimestampType(), True),
        StructField("AppointmentTypeRaw", StringType(), True), # e.g., "NewPat", "FollowUpVisit"
        StructField("ProviderCode", StringType(), True),
        StructField("BookingTimestamp", TimestampType(), True),
        StructField("HistoricalStatus", StringType(), True) # 'Attended', 'NoShow', 'Cancelled', 'Scheduled' (for future)
    ])

    # Sample raw data
    raw_app_data = [
        Row(RawAppointmentID="APP_XYZ_001", PatientSystemID="P001", ScheduledTimestamp="2023-07-10T10:00:00", AppointmentTypeRaw="NewPat", ProviderCode="DR_SMITH", BookingTimestamp="2023-06-01T14:00:00", HistoricalStatus="Attended"),
        Row(RawAppointmentID="APP_XYZ_002", PatientSystemID="P002", ScheduledTimestamp="2023-07-10T11:00:00", AppointmentTypeRaw="FollowUpVisit", ProviderCode="DR_JONES", BookingTimestamp="2023-06-15T09:00:00", HistoricalStatus="NoShow"),
        Row(RawAppointmentID="APP_XYZ_003", PatientSystemID="P001", ScheduledTimestamp="2023-07-11T09:30:00", AppointmentTypeRaw="FollowUpVisit", ProviderCode="DR_SMITH", BookingTimestamp="2023-07-01T16:00:00", HistoricalStatus="Attended"),
        Row(RawAppointmentID="APP_XYZ_004", PatientSystemID="P003", ScheduledTimestamp="2023-07-12T14:00:00", AppointmentTypeRaw="NewPat", ProviderCode="DR_BROWN", BookingTimestamp="2023-05-20T10:00:00", HistoricalStatus="Scheduled"), # Future
        Row(RawAppointmentID="APP_XYZ_005", PatientSystemID="P002", ScheduledTimestamp="2023-08-01T15:00:00", AppointmentTypeRaw="FollowUpVisit", ProviderCode="DR_JONES", BookingTimestamp="2023-07-10T11:30:00", HistoricalStatus="Scheduled")  # Future
    ]
    df_bronze_appointments = spark.createDataFrame(raw_app_data, schema_raw_appointments)
    df_bronze_appointments.write.format("delta").mode("overwrite").saveAsTable("CardiologyLakehouse.bronze_appointments_raw")
    print("bronze_appointments_raw table created.")
    display(df_bronze_appointments)
    ```
2.  **Transform to `silver_appointments` (Notebook):**
    *   Clean data, calculate lead time, define the `Is_NoShow` target variable for historical data.
    ```python
    df_bronze = spark.read.table("CardiologyLakehouse.bronze_appointments_raw")

    df_silver_appointments = df_bronze.withColumn("appointment_id", col("RawAppointmentID")) \
        .withColumn("patient_id", col("PatientSystemID")) \
        .withColumn("scheduled_datetime", col("ScheduledTimestamp")) \
        .withColumn("appointment_type", when(col("AppointmentTypeRaw") == "NewPat", "New Patient")
                                     .when(col("AppointmentTypeRaw") == "FollowUpVisit", "Follow-Up")
                                     .otherwise("Other")) \
        .withColumn("provider_id", col("ProviderCode")) \
        .withColumn("booking_datetime", col("BookingTimestamp")) \
        .withColumn("lead_time_days", datediff(to_date(col("ScheduledTimestamp")), to_date(col("BookingTimestamp")))) \
        .withColumn("day_of_week", date_format(col("ScheduledTimestamp"), "EEEE")) \
        .withColumn("is_historical_no_show", when(col("HistoricalStatus") == "NoShow", True).otherwise(False)) \
        .withColumn("is_historical_attended", when(col("HistoricalStatus") == "Attended", True).otherwise(False)) \
        .withColumn("is_future_appointment", when(col("HistoricalStatus") == "Scheduled", True).otherwise(False)) \
        .withColumn("silver_load_timestamp", current_timestamp()) \
        .select("appointment_id", "patient_id", "scheduled_datetime", "appointment_type", 
                "provider_id", "booking_datetime", "lead_time_days", "day_of_week",
                "is_historical_no_show", "is_historical_attended", "is_future_appointment", "silver_load_timestamp")

    df_silver_appointments.write.format("delta").mode("overwrite").option("overwriteSchema","true").saveAsTable("CardiologyLakehouse.silver_appointments")
    print("silver_appointments table created.")
    display(df_silver_appointments)
    ```

**Part 4: Design the ML Model and Prediction Output (Conceptual)**

1.  **Feature Engineering for `gold_appointment_features_for_ml`:**
    *   What features would you create from `silver_appointments` and a (hypothetical) `silver_patients` table?
        *   *Example:* `patient_past_no_show_rate`, `patient_total_appointments`, `is_first_appointment_with_provider`, `avg_lead_time_for_provider_type`.
2.  **Model Choice:**
    *   What type of model? (e.g., Logistic Regression, RandomForest, Gradient Boosting).
3.  **Prediction Output Table (`gold_no_show_predictions`):**
    *   What columns should it have? (`Appointment_ID`, `Patient_ID`, `Scheduled_DateTime`, `NoShow_Risk_Score` (probability), `Predicted_Is_NoShow` (binary flag), `Prediction_Timestamp`).

**Part 5: Design the Power BI Dashboard (Conceptual)**

1.  **Key Visuals and KPIs:**
    *   What information should schedulers see?
        *   *Example:* List of upcoming appointments color-coded by no-show risk score, overall predicted no-show rate for next week, top factors contributing to risk (if model is explainable), trend of no-show rates.
2.  **Interactivity:**
    *   How can users filter or drill down? (By provider, date range, appointment type).
3.  **Actions:**
    *   Could the dashboard link to actions? (e.g., trigger a reminder workflow for high-risk appointments).

**Part 6: Governance and Collaboration Considerations**

1.  **Sensitivity Labels:** What labels for Bronze, Silver, Gold tables and the Power BI report?
    *   *Example:* `bronze_appointments_raw` (Confidential - PHI), `silver_appointments` (Confidential - PHI), `gold_no_show_predictions` (Confidential - PHI), Power BI Report (Confidential - Internal Use).
2.  **RBAC:** Who needs what access to the workspace and specific items?
    *   Data Engineers (Admin/Member), Data Scientists (Contributor for ML notebooks/models), Schedulers/Clinic Managers (Viewer for Power BI report, potentially with RLS if providers are also viewers).
3.  **Data Quality Monitoring:** How would you monitor the quality of incoming appointment data?
4.  **Model Monitoring:** How would you track the no-show prediction model's performance over time? (MLflow metrics, regular re-evaluation).

**Expected Outcome / Deliverables:**
*   A documented high-level solution design for the no-show prediction system using Fabric.
*   Partially implemented Bronze (`bronze_appointments_raw`) and Silver (`silver_appointments`) tables in the Lakehouse.
*   Clear articulation of the features and structure for the Gold ML input table and prediction output table.
*   A conceptual design for the Power BI dashboard, including key visuals.
*   Considerations for governance, collaboration, and operationalizing the solution.

**Questions from Manual & Answers (Adapted for this Lab's Context):  LINK TO HTML?**

*   **Q1: In the context of predicting no-shows, what makes real-world dashboards (like the one you designed) effective for clinic schedulers or managers?**
    *   **A1:** Effective dashboards for clinic schedulers/managers would be:
        *   **Actionable:** Provide clear risk scores or flags for upcoming appointments, enabling staff to take proactive steps (e.g., targeted reminders, overbooking considerations).
        *   **Timely:** Reflect the latest predictions based on up-to-date appointment schedules.
        *   **User-Friendly and Intuitive:** Easy to understand at a glance, with clear KPIs (e.g., overall predicted no-show rate) and visualizations (e.g., lists of high-risk patients).
        *   **Contextual:** Allow filtering by date, provider, clinic, or appointment type to narrow down focus.
        *   **Explainable (if possible):** If the ML model provides reason codes for high risk, displaying these can help staff understand *why* an appointment is flagged.
        *   **Integrated (Ideally):** If possible, insights should be embeddable or accessible within their primary scheduling tools.

*   **Q2: How do OneLake shortcuts and shared workspaces support scaling a solution like no-show prediction if, for example, you needed to incorporate data from a separate hospital department's scheduling system that resides in a different Fabric workspace or even a different ADLS Gen2 account?**
    *   **A2:**
        *   **OneLake Shortcuts:** If another department's scheduling data is in a different ADLS Gen2 or another Fabric Lakehouse, a shortcut can be created in the `ED_Efficiency_Project_Q2` Lakehouse to access that data *in place*. This avoids data duplication and complex ETL to copy it over. The no-show prediction model could then join internal data with this shortcutted external data to build a more comprehensive feature set.
        *   **Shared Workspaces & Cross-Workspace Referencing:** While direct cross-workspace table querying is evolving in Fabric, data sharing can be facilitated. If the other department also uses Fabric, they could share their processed Silver/Gold tables. The no-show project could then potentially create shortcuts to these shared tables within its own Lakehouse. This allows different teams to manage their own data domains but share curated datasets for broader analytics.
        *   **Scalability:** This approach allows the solution to scale by easily incorporating new data sources without fundamentally re-architecting the central project's Lakehouse each time. The core logic for feature engineering and modeling can be adapted to consume data from these new shortcutted sources.

*   **Q3: What ethical risks or biases must be carefully mitigated when developing and deploying an AI model to predict patient appointment no-shows in a healthcare setting?**
    *   **A3:**
        *   **Socioeconomic Bias:** The model might inadvertently learn that patients from certain zip codes, with specific insurance types, or from particular demographic groups (which can be proxies for socioeconomic status or race) have higher no-show rates. If the model then flags these patients more often, it could lead to them receiving excessive (potentially annoying) reminders or even being implicitly deprioritized for appointments, creating inequities in care access.
        *   **Bias Amplification:** If historical data reflects existing biases in how certain patient groups were treated or managed regarding appointments, the model can learn and amplify these biases.
        *   **Lack of Access to Technology:** If reminder systems heavily rely on technology (smartphones, email) that certain patient populations lack access to, a no-show model might unfairly penalize them if "response to digital reminder" becomes a feature.
        *   **Over-reliance and Deskilling:** Staff might become over-reliant on the model's predictions and reduce their own critical thinking or personal outreach efforts that might be more effective for certain patients.
        *   **Transparency and Explainability:** If the model is a "black box," it's hard to identify or address these biases. Patients flagged as high-risk might not understand why, leading to frustration.
        *   **Consequences of Misprediction:**
            *   **False Positives (predicting no-show, but patient attends):** Could lead to unnecessary interventions or overbooking strategies that inconvenience patients.
            *   **False Negatives (predicting attendance, but patient is a no-show):** The original problem of wasted slots persists.
        *   **Mitigation Strategies:** Include fairness assessments during model development (e.g., using tools like Fairlearn), ensuring diverse training data, regularly auditing model predictions for disparate impact across demographic groups, providing model explainability, and implementing policies that ensure equitable treatment regardless of risk score (e.g., using risk scores to offer *more* support rather than to penalize).

---

### 11.9 Quiz
**1. Which hospital unit benefited from predictive staffing models?**
a) Oncology
b) Emergency Department
c) Pediatrics
**â†’ Answer: b**

**2. What tool was used to track model performance in Case Study 2?**
a) GitHub
b) SHAP
c) MLflow
**â†’ Answer: c**

**3. How did Hopewell Health reduce readmissions?**
a) Manual chart review
b) Predictive modeling + rounding dashboards
c) EHR upgrade
**â†’ Answer: b**

---

### 11.10 Cheat Sheet (Printable)
*   **Use Cases** = Readmission prediction, fraud detection, ED flow, public health reporting
*   **Tools** = Notebooks, Power BI, MLflow, Purview, Dataflows Gen2
*   **Results** = â†“ Readmissions, â†“ fraud, â†‘ ED efficiency
*   **Framework** = Problem â†’ Data â†’ Build â†’ Govern â†’ Monitor
*   **Key to Success** = Interdisciplinary collaboration + governance discipline

---

### Interactive Element for Section 11
[Interactive Case Studies Element](./visualizations-html/Section%2011%20-%20Case%20Studies%20and%20Real-World%20Applications.html)

---

## ðŸ“˜ Section 12: Labs and Exercises {#section-12-labs-and-exercises}

---

### 12.0 Overview
Knowledge without practice is fragile. This section offers comprehensive **hands-on labs and exercises** aligned with each core competency explored in previous sections. These tasks simulate real-world projects in a healthcare setting and support self-guided learning, team collaboration, and practical certification preparation.

Each lab is structured with:

*   Scenario and objective
*   Task steps
*   Challenge questions
*   Role-specific assignments
*   Model answers for self-assessment

---

### 12.1 Lab Categories
| Category | Description |
| :--- | :--- |
| **Environment Setup** | Provisioning workspaces, roles, and Lakehouse |
| **Data Ingestion** | Ingesting HL7, FHIR, streaming, batch |
| **Modeling** | Silver-Gold data modeling, normalization |
| **Governance & Security** | RBAC, Purview classification, audit logs |
| **Analytics** | Creating clinical dashboards with Power BI |
| **AI/ML** | Training and deploying predictive models |
| **Performance Optimization** | Query tuning, Spark tuning, report load |
| **Integration** | Git, Azure ML, webhooks, APIs |
| **Use Case Application** | Build end-to-end solution from real scenario |

---

### 12.2 Capstone Challenge: Building a Unified Sepsis Surveillance and Prediction System
**Module Alignment:** Section 12: Labs and Exercises (Capstone Project integrating concepts from Sections 1-11)

**Objective:**
*   Apply knowledge and skills gained throughout the Microsoft Fabric for Healthcare course to design and partially implement a comprehensive solution for a critical healthcare problem: sepsis surveillance and early prediction.
*   Integrate data from multiple simulated sources (vitals, labs, patient demographics, encounter history).
*   Develop a data model (Bronze, Silver, Gold) suitable for both real-time surveillance and predictive analytics.
*   Build a proof-of-concept machine learning model to predict sepsis onset risk.
*   Design an interactive Power BI dashboard for clinicians to monitor at-risk patients and sepsis-related KPIs.
*   Incorporate considerations for data governance, security (including RLS and sensitivity labels), performance, and collaboration.

**Scenario:**
Valley General Hospital is launching an initiative to improve early detection and management of sepsis, a life-threatening condition. They want to leverage Microsoft Fabric to create a unified system that:
1.  Ingests and integrates relevant patient data in near real-time.
2.  Provides clinicians with a dashboard to monitor patients for early warning signs based on established criteria (e.g., SIRS, SOFA scores - simplified for this lab).
3.  Employs a machine learning model to predict the likelihood of sepsis onset for ICU patients.

**Prerequisites:**
*   Successful completion and thorough understanding of all previous labs and course content (Labs 1.5 through 11.8).
*   Proficiency in using Fabric Lakehouse, Notebooks (PySpark/Python), Data Factory (conceptual design), Power BI, and understanding of MLflow and Purview concepts.
*   Ability to work with less prescriptive instructions and make informed design choices.
*   Microsoft Fabric Workspace and necessary permissions.

**Tools to be Used (Design and Partial Implementation):**
*   Microsoft Fabric Lakehouse
*   Microsoft Fabric Notebooks (PySpark, Python, scikit-learn)
*   MLflow
*   Microsoft Fabric Power BI
*   (Conceptual) Data Factory, Real-Time Analytics (KQL for streaming if extending), Microsoft Purview

**Estimated Time:** 3-4 hours (or longer, depending on depth of implementation)

**High-Level Tasks & Design Considerations:**

**Phase 1: Data Source Identification and Ingestion Strategy (Design & Simulate)**

1.  **Identify Key Data Sources:**
    *   **Patient Vitals:** Heart Rate, Respiratory Rate, Temperature, Blood Pressure (simulated as streaming or frequent batch).
    *   **Lab Results:** White Blood Cell count (WBC), Lactate levels, Creatinine (simulated batch).
    *   **Patient Demographics & History:** Age, existing comorbidities, recent surgeries (from `dim_patient` or a new `silver_patient_history` table).
    *   **Encounter Data:** ICU admission/discharge times, current location (from `fact_encounter` or a new `silver_icu_stays` table).
2.  **Design Ingestion Pipelines (Conceptual for Data Factory/Real-Time Analytics):**
    *   How would you ingest streaming vitals? (e.g., Event Hub -> Real-Time Analytics KQL -> Lakehouse Bronze table).
    *   How would you ingest batch lab results? (e.g., Data Factory pipeline from LIS DB/files -> Lakehouse Bronze table).
3.  **Simulate Bronze Layer Data (Notebook):**
    *   Create PySpark DataFrames representing raw data from these sources and save them as Delta tables in a `SepsisBronze` schema/folder in your Lakehouse (e.g., `bronze_patient_vitals_stream`, `bronze_lab_results_batch`). Include timestamps.

**Phase 2: Data Modeling (Silver & Gold Layers - Implement in Notebook)**

1.  **Design and Create Silver Layer Tables:**
    *   `silver_patient_vitals_processed`: Cleaned, validated vitals with patient and encounter context.
    *   `silver_lab_results_processed`: Cleaned, validated labs with patient and encounter context, potentially flagging abnormal results.
    *   `silver_icu_patient_episodes`: Consolidates ICU stay information, linking demographics, vitals, and labs for a given patient during an ICU stay.
2.  **Design and Create Gold Layer Tables:**
    *   `gold_sepsis_surveillance_hourly` (or other appropriate frequency): Aggregated table per patient per hour (or shift) including latest vitals, key lab results, and calculated simplified Sepsis Indicators (e.g., if 2 out of 3 SIRS criteria are met â€“ Temp >38C or <36C, HR >90, RR >20. *This is a simplification; real SOFA/qSOFA is more complex*).
    *   `gold_sepsis_ml_features`: Feature-engineered table specifically for training the sepsis prediction model (may include trends, deltas in vitals/labs over time, historical data).
    *   `gold_sepsis_predictions`: Table to store predictions from the ML model.

**Phase 3: Machine Learning Model Development (Implement in Notebook)**

1.  **Feature Engineering:** From `gold_sepsis_ml_features`, create relevant features.
2.  **Define Target Variable:** `Sepsis_Onset_Next_6_Hours` (boolean â€“ this would require careful labeling of historical data, which you'll need to simulate or define based on criteria).
3.  **Model Selection, Training, and Evaluation:**
    *   Choose a suitable classification model (e.g., Logistic Regression, Random Forest, XGBoost).
    *   Perform train-test split.
    *   Train the model.
    *   Evaluate using appropriate metrics (AUC, Precision, Recall, F1-score, Specificity).
4.  **MLflow Tracking:** Log parameters, metrics, and the model artifact.
5.  **Prediction:** Generate predictions on a test set or new data and store them in `gold_sepsis_predictions`.

**Phase 4: Power BI Dashboard Design and Implementation**

1.  **Create Power BI Dataset (DirectLake):** Connect to your Gold layer tables.
2.  **Design "Sepsis Surveillance Dashboard":**
    *   **Patient List View:** Table showing current ICU patients, their latest vitals, key labs, calculated Sepsis Indicators, and the ML prediction score/risk level. Color-code high-risk patients.
    *   **Individual Patient Drill-Through:** Allow clicking on a patient to see trends of their vitals and labs over the last 24-48 hours.
    *   **KPIs:** Overall number of patients at high risk (by indicator), overall number of patients at high risk (by ML model), average time to intervention (if this data were available).
    *   **Filters:** By ICU unit, shift, risk level.
3.  **Implement RLS:**
    *   If different ICU units should only see their patients, design and implement RLS.
4.  **Apply Sensitivity Labels:** Label the dataset and report appropriately (e.g., "HIPAA-HIGH - Clinical Decision Support").

**Phase 5: Governance, Performance, and Collaboration Considerations (Discussion/Documentation)**

1.  **Data Governance:**
    *   How will Microsoft Purview be used for lineage and classification?
    *   What are the key audit requirements?
2.  **Security:**
    *   Detail RBAC for the workspace and specific sensitive items.
    *   How will data be protected at rest and in transit?
3.  **Performance:**
    *   What are potential performance bottlenecks for the streaming ingestion, ML scoring, and Power BI dashboard?
    *   What optimization techniques would you consider (e.g., table partitioning, `OPTIMIZE`, DAX optimization)?
4.  **Collaboration:**
    *   How will data engineers, data scientists, and clinicians collaborate within this Fabric workspace?
    *   What are the communication and hand-off points?
5.  **Operationalization:**
    *   How would the ML model be retrained and deployed?
    *   How would the "real-time" aspect of the surveillance dashboard be maintained?

**Deliverables for this Capstone Lab:**

1.  **Fabric Notebook(s):**
    *   Code for simulating Bronze data.
    *   Code for transforming data into Silver and Gold layer tables.
    *   Code for ML model training, evaluation, MLflow tracking, and prediction.
2.  **Fabric Lakehouse:**
    *   Bronze, Silver, and Gold layer Delta tables as designed.
3.  **Power BI Report:**
    *   A functional Power BI dashboard connected to the Gold tables, implementing key visuals and RLS (if applicable).
    *   Sensitivity label applied.
4.  **Short Design Document (e.g., Markdown in Notebook or separate document):**
    *   Briefly outlining the architecture, data flow, ML model approach, dashboard design, and considerations for governance, performance, and collaboration.

**Guidance for Learners:**

*   This is a challenge lab. You are expected to draw upon all previous learnings.
*   Focus on a feasible scope for the implementation parts. The design document can cover more aspirational aspects.
*   Make reasonable assumptions where specific data or business rules are not provided (and document them).
*   Simplicity in ML model choice and feature engineering is acceptable; the focus is on the end-to-end Fabric workflow.
*   Prioritize creating a functional, albeit simplified, version of each component.

---

### 12.3 Challenge Lab: Streaming Device Data Ingestion
**Objective:** Ingest real-time vitals data from a wearable and store in Lakehouse with timestamp filtering.

**Tools:**

*   Event Hub (simulated)
*   Eventstream
*   Real-Time Analytics
*   Lakehouse (Gold_PatientVitals)

---

### 12.4 Mini-Scenario Exercises
| Scenario | Task |
| :--- | :--- |
| Appointments No-Show | Build a predictive model using patient demographics and prior attendance |
| Claims Fraud | Design an anomaly detection system for claim lines |
| Patient Education | Extract keywords from free-text notes using Cognitive Services |
| ED Throughput | Analyze patient flow by time, disposition, and clinician |

---

### 12.5 Self-Assessment Quiz: Labs and Exercises
**1. What table layer should you use for model training?**
a) Bronze
b) Silver
c) Gold
**â†’ Answer: c**

**2. Which Fabric tool enables visual ETL?**
a) Notebook
b) Dataflow Gen2
c) Spark UI
**â†’ Answer: b**

**3. What should be logged with MLflow?**
a) SQL queries
b) Metadata schemas
c) Metrics, parameters, artifacts
**â†’ Answer: c**

---

### 12.6 Role-Specific Lab Checklist
| Role | Required Labs |
| :--- | :--- |
| Data Engineer | All ingestion, modeling, governance, ML |
| Analyst | Power BI dashboards, RLS, DirectLake |
| Compliance Officer | Purview classification, audit trails, RBAC setup |

---

### 12.7 Printable Job Aids and Instructions
Included in Appendix:

*   Power BI Publishing Steps (with RLS)
*   Notebook Starter Template (with MLflow)
*   HIPAA-Compliant Data Logging Checklist
*   Ingestion Pipeline Design Template
*   Security Configuration Worksheet

---

### Interactive Element for Section 12
[Interactive Labs & Exercises Element](./visualizations-html/Section%2012%20-%20Labs%20and%20Exercises.html)

---

## ðŸ“˜ Section 13: Assessment and Certification {#section-13-assessment-and-certification}

---

### 13.0 Overview
To ensure meaningful skill acquisition and industry readiness, this section presents a **comprehensive certification assessment** covering the entire Fabric for Healthcare training program. The exam is structured to test conceptual understanding, practical application, and governance awareness.

Successful completion not only demonstrates mastery of Microsoft Fabric but also the ability to apply it responsibly in regulated healthcare environments.

---

### 13.1 Exam Structure
| Format | Number of Questions | Weight |
| :--- | :--- | :--- |
| Multiple Choice | 20 | 40% |
| Scenario-Based | 5 | 30% |
| Short Answer | 5 | 30% |

---

### 13.2 Multiple Choice Questions (Sample Set)
**1. What format does OneLake use to store data?**
a) JSON
b) Delta Parquet
c) XML
d) Avro
â†’ **Answer: b**

**2. Which of the following is used to apply row-level permissions in Power BI?**
a) Audit Policy
b) Sensitivity Labels
c) RLS
d) Power Query
â†’ **Answer: c**

**3. What Fabric tool is used to create pipelines?**
a) Notebooks
b) Dataflows Gen2
c) Azure Monitor
d) Data Factory
â†’ **Answer: d**

**4. Which Azure service enables real-time alerts from data activity?**
a) Azure Event Grid
b) Azure Data Lake
c) Azure DevOps
d) Azure Files
â†’ **Answer: a**

**5. What should you always do before using PHI in external AI services?**
a) Encrypt it
b) Tokenize or de-identify it
c) Email it to IT
d) None of the above
â†’ **Answer: b**

---

### 13.3 Scenario-Based Questions
**Scenario 1:**
You're tasked with reducing readmissions in a rural hospital using Microsoft Fabric.

**Questions:**

*   What Fabric tools would you use to ingest, model, and predict?
*   How would you ensure HIPAA compliance during modeling?
*   What visualizations would be valuable for nurse managers?

**Scenario 2:**
A provider dashboard is loading slowly during morning rounds.

**Questions:**

*   What optimization strategies can improve performance?
*   How would you use DAX efficiently?

**Scenario 3:**
An external research partner needs access to de-identified data.

**Questions:**

*   Which tools support secure external collaboration?
*   What role-based configurations are needed?

**Scenario 4:**
The compliance officer requests a full lineage of patient medication data from ingestion to reporting.

**Questions:**

*   What Fabric governance features will you use?
*   How will you verify access history?

**Scenario 5:**
A COVID dashboard must be built and shared with the state health department within 72 hours.

**Questions:**

*   How would you architect this solution using Fabric?
*   What shortcuts and pipelines will help accelerate deployment?

---

### 13.4 Short Answer Prompts
1.  Describe the medallion architecture (Bronze-Silver-Gold) and its purpose.
2.  How does Microsoft Purview assist with HIPAA compliance?
3.  Explain the difference between sensitivity labels and RLS.
4.  How do notebooks in Fabric support model versioning and collaboration?
5.  Why is explainability important in healthcare AI?

---

### 13.5 Grading and Evaluation Criteria
| Section | Max Points | Criteria |
| :--- | :--- | :--- |
| Multiple Choice | 40 | 2 pts/question |
| Scenario Responses | 30 | Depth, tools used, governance |
| Short Answers | 30 | Clarity, terminology, best practices |

> **Passing Score:** 75% (75 points or above)

---

### 13.6 Certificate of Completion
Upon successful completion, learners will receive a personalized digital certificate containing:

*   Learnerâ€™s Name
*   Course Title: *Microsoft Fabric for Healthcare â€“ Data Engineer Track*
*   Completion Date
*   Signature line (Instructor/Program Lead)
*   Certificate ID

> PDF template is available in the Appendix or on the learning portal.

---

### 13.7 Feedback and Next Steps
After assessment:

*   Complete feedback survey (link optional)
*   Optional: Submit case project for instructor review
*   Request badge for LinkedIn profile

---

### 13.8 Printable: Exam Preparation Checklist
*   [ ] Reviewed all 12 sections
*   [ ] Completed all labs
*   [ ] Can explain HIPAA controls in Fabric
*   [ ] Know how to create pipelines and dashboards
*   [ ] Built and deployed a simple ML model
*   [ ] Practiced role-based access configuration
*   [ ] Understand OneLake architecture and shortcuts
*   [ ] Used Purview for classification and audit

---

### Interactive Elements for Section 13
*   [Assessment and Certification Overview](./visualizations-html/Section%2013%20-%20Assessment%20and%20Certification.html)
*   [Student Assessment Test 1](./visualizations-html/Section%2013%20-%20Student%20Assessment%20Test.html)
*   [Student Assessment Test 2](./visualizations-html/Section%2013%20-%20Student%20Assessment%20Test%202.html)
*   [Microsoft Fabric Interactive Quiz](./visualizations-html/microsoft-fabric-interactive-quiz.html)

---

## ðŸ“˜ Section 14: Job Aids and Cheat Sheets {#section-14-job-aids-and-cheat-sheets}

---

### 14.0 Overview
This section is designed to serve as your **at-a-glance companion** for key Microsoft Fabric tasks. Whether you're deploying pipelines, configuring governance, or building analytical models, these job aids provide **condensed, actionable references**.

Each cheat sheet or visual guide in this section is printable, and many are structured as one-page summaries for team sharing and field use.

---

### 14.1 Data Lakehouse Architecture Cheat Sheet
#### ðŸ” Medallion Layering
| Layer | Contents | Purpose | Example Table |
| :--- | :--- | :--- | :--- |
| Bronze | Raw, unvalidated data | Ingestion, traceability | Bronze_FHIRPatient |
| Silver | Cleaned, standardized data | Analytics model foundation | Silver_Encounter |
| Gold | Aggregated, curated data | Dashboards, ML input | Gold_ReadmissionRisk |

**Best Practice:**

*   Use `OPTIMIZE` on Silver and Gold
*   Use `VACUUM` on Bronze with retention
*   Preserve schema evolution in Silver

---

### 14.2 Fabric Governance Visual Guide
#### ðŸ” Governance Toolkit
| Tool | Function |
| :--- | :--- |
| Microsoft Purview | Lineage, classification, sensitivity tagging |
| Sensitivity Labels | HIPAA-HIGH, Confidential, Export Restricted |
| Audit Logs | Access records and data interactions |
| Access Reviews | Quarterly role review via Entra ID |
| DLP Policies | Prevent sharing or downloading PHI |

**Visual Prompt (included in .docx):**
Governance Workflow Diagram â†’ Ingest â†’ Classify â†’ Label â†’ Review â†’ Monitor

---

### 14.3 Power BI RLS + DLP Setup Instructions
#### RLS Configuration
1.  Create Security Table:
    ```
    SecurityMatrix(UserEmail, Department)
    ```
2.  In Power BI:
    *   Go to Modeling â†’ Manage Roles
    *   Apply DAX Filter:
        ```
        [Department] = LOOKUPVALUE(SecurityMatrix[Department], SecurityMatrix[UserEmail], USERPRINCIPALNAME())
        ```
3.  Publish â†’ Assign Users to Roles

#### DLP Actions
*   Enable in Microsoft Purview â†’ Policy Management
*   Assign policy to Fabric item
*   Restrict download/export
*   Enable alerts for external sharing

---

### 14.4 Notebook & MLflow Commands
#### Start MLflow Tracking
```python
import mlflow
mlflow.start_run()
mlflow.log_param("model", "logistic")
mlflow.log_metric("AUC", 0.89)
mlflow.end_run()
```

#### Save Scored Predictions
```python
# predictions.write.format("delta").mode("overwrite").save("Tables/Gold_PatientRisk")
```

---

### 14.5 Pipeline Build Command Sequence (via UI or API)
#### ETL Pipeline Template
*   Step 1: Source (SQL, API, S3)
*   Step 2: Transformation (Dataflow Gen2 / Notebook)
*   Step 3: Sink (Lakehouse / Warehouse)

```json
{
  "source": "FHIR Server",
  "transform": "Standardize Encounter",
  "destination": "Silver_Encounter"
}
```

> Use tags: `Project=EDAnalytics`, `Sensitivity=HIPAA-HIGH`

---

### 14.6 Quick Commands Reference Sheet
| Task | Command/Action |
| :--- | :--- |
| Partition Lakehouse Table | `.repartition("field")` |
| Optimize Delta Table | `OPTIMIZE TableName` |
| Clean Old Files | `VACUUM TableName RETAIN 168 HOURS` |
| Add Sensitivity Label | Use Purview â†’ "Classify and Label" |
| Trigger Pipeline (API) | `POST /pipelines/{id}/run` |

---

### 14.7 Troubleshooting Tips Table
| Symptom | Resolution |
| :--- | :--- |
| Slow Power BI load | Reduce visuals, use DirectLake |
| High Lakehouse scan latency | Add partitioning + OPTIMIZE |
| Spark job OOM | Increase executor memory |
| Duplicated patient records | Enable MPI-based deduplication |
| External share blocked | Check DLP + label configuration |

---

### 14.8 SOP Snapshot: Data Engineer
#### Daily Tasks:
*   Monitor pipeline execution
*   Review Purview audit logs
*   Confirm RLS accuracy on active dashboards

#### Weekly Tasks:
*   Run OPTIMIZE and VACUUM on Silver and Bronze
*   Validate ML model drift metrics
*   Update data dictionary and schemas

#### Monthly Tasks:
*   Review RBAC roles
*   Coordinate with compliance officer for Purview review
*   Update Git repository with changes

---

### 14.9 Printables Index (Appendix Reference)
| Aid Name | File or Page Ref |
| :--- | :--- |
| RLS and DLP Setup Instructions | Appendix A |
| Data Engineer SOP Sheet | Appendix B |
| Pipeline Builder Template | Appendix C |
| Notebook MLflow Logging Template | Appendix D |
| Governance Workflow Diagram | Appendix E |

---

### Interactive Element for Section 14
[Interactive Job Aids & Cheat Sheets](./visualizations-html/Section%2014%20-%20Job%20Aids%20and%20Cheat%20Sheets.html)

---

## ðŸ“˜ Section 15: Final Notes and Resources {#section-15-final-notes-and-resources}

---

### 15.0 Overview
Congratulations on reaching the final section of this comprehensive technical training journey. As you move forward, Microsoft Fabric will continue to evolveâ€”bringing new features, integrations, and standards to the healthcare landscape.

This section provides a wrap-up of the programâ€™s central themes, shares future certification and learning opportunities, and compiles curated resources to support your growth as a healthcare-focused data engineer.

---

### 15.1 Recap: Core Themes & Capabilities
| Capability Area | Key Takeaway |
| :--- | :--- |
| Data Ingestion | Use pipelines and notebooks for structured, semi-structured, and streaming sources. |
| Medallion Modeling | Employ the Bronze-Silver-Gold pattern for scalable, traceable data flow. |
| Security & Compliance | HIPAA and HITRUST require labeling, logging, encryption, and access control. |
| Analytics | DirectLake and Power BI deliver real-time insights with fine-grained security. |
| AI/ML | Train explainable models using notebooks, log experiments, and deploy responsibly. |
| Governance | Use Microsoft Purview for end-to-end data oversight and auditability. |
| Collaboration | Secure role-based workspaces drive interdisciplinary productivity. |
| Customization | APIs, Git integration, and OneLake shortcuts enable deep system extensibility. |

---

### 15.2 Continued Learning and Certification Paths
To remain current and elevate your profile, consider these Microsoft and industry certifications:

| Certification | Provider | Relevance |
| :--- | :--- | :--- |
| Azure Data Engineer Associate | Microsoft | Core data services |
| Microsoft Certified: Fabric Specialist (in development) | Microsoft | Role-specific Fabric use |
| Certified Health Data Analyst (CHDA) | AHIMA | Healthcare analytics standards |
| HITRUST Implementer | HITRUST Alliance | Security framework application |
| Power BI Data Analyst Associate | Microsoft | Advanced reporting and RLS mastery |

---

### 15.3 Microsoft Fabric Ecosystem Tools
| Tool/Portal | Description |
| :--- | :--- |
| Microsoft Learn | Free modular Fabric content |
| Microsoft Docs | Technical documentation for APIs and features |
| Fabric Feedback Portal | Submit feature requests |
| Azure Architecture Center | Best practices for cloud-native solutions |
| Compliance Manager | Track HIPAA/HITRUST adherence per workload |

---

### 15.4 Community and Collaboration Hubs
Get involved, learn from others, and share what you build:

*   **Microsoft Tech Community â€“ Fabric Forum**
    [https://techcommunity.microsoft.com](https://techcommunity.microsoft.com)
*   **Power BI Community**
    [https://community.powerbi.com](https://community.powerbi.com)
*   **Fabric GitHub Repositories**
    Access public templates and notebook libraries
*   **LinkedIn Learning + YouTube**
    Follow the â€œMicrosoft Fabricâ€ tag for tutorial series

---

### 15.5 Regulatory and Standards References
For those working in healthcare analytics and IT compliance:

| Source | Link |
| :--- | :--- |
| HIPAA Security Rule | [https://www.hhs.gov/hipaa](https://www.hhs.gov/hipaa) |
| HITRUST CSF | [https://hitrustalliance.net](https://hitrustalliance.net) |
| HL7 and FHIR | [https://www.hl7.org/fhir](https://www.hl7.org/fhir) |
| FDA AI/ML Policy | [https://www.fda.gov/medical-devices](https://www.fda.gov/medical-devices) |

---

### 15.6 Final Notes for Data Engineers
*   Automate responsiblyâ€”build pipelines that are observable, secure, and versioned.
*   Never compromise on governance. Purview, audit logs, and access reviews are non-negotiable.
*   Collaborate deeply with clinical and compliance teams to keep models relevant and ethical.
*   Focus on explainabilityâ€”every prediction should be interpretable and defensible.
*   Stay currentâ€”Fabric is advancing rapidly. Whatâ€™s beta today may be essential tomorrow.

---

### 15.7 Program Closure and Reflection
Youâ€™ve now completed an intensive, in-depth training designed to prepare you for complex real-world healthcare analytics challenges. With Microsoft Fabric, you are equipped to lead transformational projects that improve patient care, streamline operations, and deliver measurable outcomes.

---

### 15.8 Certificate Claim (Optional Step)
To finalize your training and obtain a digital certificate:

1.  Complete the final assessment in Section 13.
2.  Submit your results to your program administrator or instructor.
3.  Request a signed certificate using your learner ID or course instance.

> Templates are available in the Appendix and can be digitally personalized.

---

### Interactive Element for Section 15
[Interactive Final Notes & Resources](./visualizations-html/Section%2015%20-%20Final%20Notes%20and%20Resources.html)
